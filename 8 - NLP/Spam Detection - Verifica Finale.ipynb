{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a33e3bc",
   "metadata": {},
   "source": [
    "## SPAM DETECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecee1657",
   "metadata": {},
   "source": [
    "## PROGETTO SPAM DETECTION\n",
    "### L'azienda ProfessionAI vuole realizzare una libreria capace di fare analisi delle email ricevute. \n",
    "### Nello specifico, il CEO ha richiesto di identificare le email di tipo SPAM sulle quali fare analisi contenutistiche.\n",
    "### Il CTO nello specifico ti fornisce un dataset e ti chiede di:\n",
    "### 1) Addestrare un classificatore per identificare SPAM\n",
    "### 2) Individuare i Topic principali tra le email SPAM presenti nel dataset\n",
    "### 3) Calcolare la distanza semantica tra i topics ottenuti, per dedurne l'eterogeneit√†.\n",
    "### 4) Estrarre dalle mail NON SPAM le Organizzazioni presenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa987af",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from scipy import spatial\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,  confusion_matrix, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_learning_curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34fcb190",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('datasets/Verifica Finale - Spam Detection/spam_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3864817f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "english_stopwords = stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def data_cleaner(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    for c in string.punctuation:\n",
    "        sentence = sentence.replace(c, \" \")\n",
    "    document = nlp(sentence)\n",
    "    sentence = ' '.join(token.lemma_ for token in document)\n",
    "    sentence = ' '.join(word for word in sentence.split() if word not in english_stopwords)\n",
    "    sentence = re.sub('\\d', '', sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "273fd572",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dataset['cleaned_text'] = dataset['text'].apply(data_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e08cb7",
   "metadata": {},
   "source": [
    "1) classificatore per l'identificazione dello spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1c5b76",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset['text'], dataset['label_num'], test_size=0.2, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f19159f3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.923671497584541, 0.9953703703703703, 0.7337883959044369, 0.8447937131630648)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "nb_y_pred = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "nb_accuracy = accuracy_score(y_test, nb_y_pred)\n",
    "nb_precision = precision_score(y_test, nb_y_pred)\n",
    "nb_recall = recall_score(y_test, nb_y_pred)\n",
    "nb_f1 = f1_score(y_test, nb_y_pred)\n",
    "\n",
    "nb_accuracy, nb_precision, nb_recall, nb_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70ac3e3a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_conf_matrix = confusion_matrix(y_test, nb_y_pred)\n",
    "sns.heatmap(nb_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - Naive Bayes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e076d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = MLPClassifier(activation='logistic', # per attivit√† di classificazione testuale √® consigliato usare la funzione logistica\n",
    "#                     hidden_layer_sizes=(100,),\n",
    "#                     max_iter=100,\n",
    "#                     solver='adam', # per questo tipo di task √® consigliato usare l'ottimizzatore adam\n",
    "#                     tol=0.005,\n",
    "#                     verbose=True)\n",
    "\n",
    "# clf.fit(X_train_tfidf,y_train)\n",
    "# mlp_y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# mlp_accuracy = accuracy_score(y_test, mlp_y_pred)\n",
    "# mlp_precision = precision_score(y_test, mlp_y_pred)\n",
    "# mlp_recall = recall_score(y_test, mlp_y_pred)\n",
    "# mlp_f1 = f1_score(y_test, mlp_y_pred)\n",
    "\n",
    "# mlp_accuracy, mlp_precision, mlp_recall, mlp_f1\n",
    "\n",
    "# mlp_conf_matrix = confusion_matrix(y_test, mlp_y_pred)\n",
    "# sns.heatmap(mlp_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"])\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"True\")\n",
    "# plt.title(\"Confusion Matrix - MLPClassifier\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e4eed2f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Iteration 1, loss = 0.61245220\n",
      "Iteration 1, loss = 0.65481817\n",
      "Iteration 1, loss = 0.74249431\n",
      "Iteration 1, loss = 0.61814244\n",
      "Iteration 1, loss = 0.62438265\n",
      "Iteration 1, loss = 0.72531819\n",
      "Iteration 1, loss = 0.61595171\n",
      "Iteration 1, loss = 0.73305899\n",
      "Iteration 1, loss = 0.61079154\n",
      "Iteration 1, loss = 0.70224608\n",
      "Iteration 2, loss = 0.60731092\n",
      "Iteration 2, loss = 0.65404753\n",
      "Iteration 2, loss = 0.61140022\n",
      "Iteration 2, loss = 0.62300981\n",
      "Iteration 2, loss = 0.60921066\n",
      "Iteration 2, loss = 0.61878844\n",
      "Iteration 2, loss = 0.57388696\n",
      "Iteration 1, loss = 0.72900303\n",
      "Iteration 2, loss = 0.60207280\n",
      "Iteration 2, loss = 0.57597005\n",
      "Iteration 2, loss = 0.62311773\n",
      "Iteration 3, loss = 0.61671726\n",
      "Iteration 3, loss = 0.60458410\n",
      "Iteration 1, loss = 0.63047055\n",
      "Iteration 3, loss = 0.60823417\n",
      "Iteration 3, loss = 0.60551578\n",
      "Iteration 1, loss = 0.59348593\n",
      "Iteration 3, loss = 0.60525529\n",
      "Iteration 1, loss = 0.61210861\n",
      "Iteration 1, loss = 0.59932764\n",
      "Iteration 1, loss = 0.59661865\n",
      "Iteration 4, loss = 0.60388046\n",
      "Iteration 3, loss = 0.55533034\n",
      "Iteration 4, loss = 0.60593678\n",
      "Iteration 3, loss = 0.56857605\n",
      "Iteration 3, loss = 0.55560824\n",
      "Iteration 3, loss = 0.55223025\n",
      "Iteration 3, loss = 0.56645099\n",
      "Iteration 4, loss = 0.60478949\n",
      "Iteration 4, loss = 0.60382387\n",
      "Iteration 4, loss = 0.60402057\n",
      "Iteration 2, loss = 0.61851336\n",
      "Iteration 5, loss = 0.60418109\n",
      "Iteration 5, loss = 0.60367172\n",
      "Iteration 5, loss = 0.60361174\n",
      "Iteration 5, loss = 0.60388530\n",
      "Iteration 5, loss = 0.60353844\n",
      "Iteration 4, loss = 0.52902847\n",
      "Iteration 4, loss = 0.54160355\n",
      "Iteration 4, loss = 0.53369477\n",
      "Iteration 4, loss = 0.54001391\n",
      "Iteration 4, loss = 0.52897978\n",
      "Iteration 2, loss = 0.56807867\n",
      "Iteration 6, loss = 0.60361667\n",
      "Iteration 6, loss = 0.60341409\n",
      "Iteration 6, loss = 0.60361121\n",
      "Iteration 6, loss = 0.60365896\n",
      "Iteration 6, loss = 0.60356344\n",
      "Iteration 5, loss = 0.50847243\n",
      "Iteration 2, loss = 0.56092048\n",
      "Iteration 5, loss = 0.50682781\n",
      "Iteration 2, loss = 0.56388016\n",
      "Iteration 5, loss = 0.52105465\n",
      "Iteration 5, loss = 0.50399125\n",
      "Iteration 2, loss = 0.56415688\n",
      "Iteration 7, loss = 0.60339132\n",
      "Iteration 7, loss = 0.60356355\n",
      "Iteration 7, loss = 0.60360661\n",
      "Iteration 5, loss = 0.51983699\n",
      "Iteration 7, loss = 0.60359492\n",
      "Iteration 6, loss = 0.48017484\n",
      "Iteration 6, loss = 0.48399687\n",
      "Iteration 2, loss = 0.56878181\n",
      "Iteration 7, loss = 0.60350159\n",
      "Iteration 3, loss = 0.60350278\n",
      "Iteration 8, loss = 0.60354146\n",
      "Iteration 6, loss = 0.47512463\n",
      "Iteration 8, loss = 0.60355326\n",
      "Iteration 6, loss = 0.50095395\n",
      "Iteration 8, loss = 0.60358305\n",
      "Iteration 7, loss = 0.44811373\n",
      "Iteration 8, loss = 0.60336564\n",
      "Iteration 9, loss = 0.60357613\n",
      "Iteration 6, loss = 0.49964045\n",
      "Iteration 8, loss = 0.60346505\n",
      "Iteration 9, loss = 0.60356373\n",
      "Iteration 7, loss = 0.46031719\n",
      "Iteration 7, loss = 0.44310989\n",
      "Iteration 9, loss = 0.60348412\n",
      "Iteration 7, loss = 0.47826976\n",
      "Iteration 9, loss = 0.60335049\n",
      "Iteration 10, loss = 0.60358674\n",
      "Iteration 8, loss = 0.43549315\n",
      "Iteration 3, loss = 0.53195584\n",
      "Iteration 10, loss = 0.60353127\n",
      "Iteration 7, loss = 0.47980417\n",
      "Iteration 4, loss = 0.60366475\n",
      "Iteration 9, loss = 0.60357366\n",
      "Iteration 10, loss = 0.60343948\n",
      "Iteration 3, loss = 0.54176069\n",
      "Iteration 8, loss = 0.41303323\n",
      "Iteration 8, loss = 0.45536599\n",
      "Iteration 11, loss = 0.60353799\n",
      "Iteration 8, loss = 0.40843239\n",
      "Iteration 10, loss = 0.60356892\n",
      "Iteration 11, loss = 0.60343113\n",
      "Iteration 9, loss = 0.40946836\n",
      "Iteration 11, loss = 0.60351220\n",
      "Iteration 12, loss = 0.60352781\n",
      "Iteration 9, loss = 0.43120724\n",
      "Iteration 10, loss = 0.60335850\n",
      "Iteration 9, loss = 0.37538054\n",
      "Iteration 11, loss = 0.60355778\n",
      "Iteration 3, loss = 0.52866172\n",
      "Iteration 5, loss = 0.60345595\n",
      "Iteration 8, loss = 0.45737640\n",
      "Iteration 12, loss = 0.60342841\n",
      "Iteration 4, loss = 0.49784788\n",
      "Iteration 12, loss = 0.60348419\n",
      "Iteration 3, loss = 0.53156349\n",
      "Iteration 12, loss = 0.60354896\n",
      "Iteration 3, loss = 0.54007134\n",
      "Iteration 13, loss = 0.60353015\n",
      "Iteration 13, loss = 0.60342437\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), solver=sgd; total time=  32.9s\n",
      "Iteration 10, loss = 0.38259354\n",
      "Iteration 13, loss = 0.60347025\n",
      "Iteration 11, loss = 0.60333753\n",
      "Iteration 9, loss = 0.43364197\n",
      "Iteration 10, loss = 0.33758922\n",
      "Iteration 10, loss = 0.40550610\n",
      "Iteration 9, loss = 0.37186289\n",
      "Iteration 14, loss = 0.60345593\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), solver=sgd; total time=  35.7s\n",
      "Iteration 14, loss = 0.60350208\n",
      "Iteration 11, loss = 0.35508991\n",
      "Iteration 13, loss = 0.60354278\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), solver=sgd; total time=  37.3s\n",
      "Iteration 12, loss = 0.60333715\n",
      "Iteration 10, loss = 0.33508645\n",
      "Iteration 11, loss = 0.37872107\n",
      "Iteration 4, loss = 0.51059373\n",
      "Iteration 6, loss = 0.60344252\n",
      "Iteration 4, loss = 0.49085172\n",
      "Iteration 10, loss = 0.40859031\n",
      "Iteration 4, loss = 0.51319011\n",
      "Iteration 12, loss = 0.32771475\n",
      "Iteration 12, loss = 0.35131633\n",
      "Iteration 13, loss = 0.60329980\n",
      "Iteration 11, loss = 0.30110352\n",
      "Iteration 15, loss = 0.60349483\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), solver=sgd; total time=  41.2s\n",
      "Iteration 5, loss = 0.45882140\n",
      "Iteration 13, loss = 0.30067908\n",
      "Iteration 1, loss = 0.68257288\n",
      "Iteration 11, loss = 0.38233733\n",
      "Iteration 4, loss = 0.49207005\n",
      "Iteration 11, loss = 0.29960999\n",
      "Iteration 13, loss = 0.32414944\n",
      "Iteration 14, loss = 0.60337308\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), solver=sgd; total time=  44.9s\n",
      "Iteration 12, loss = 0.26657176\n",
      "Iteration 1, loss = 0.72829812\n",
      "Iteration 7, loss = 0.60348847\n",
      "Iteration 14, loss = 0.27487751\n",
      "Iteration 12, loss = 0.26629597\n",
      "Iteration 12, loss = 0.35552230\n",
      "Iteration 5, loss = 0.47605911\n",
      "Iteration 1, loss = 0.66742912\n",
      "Iteration 14, loss = 0.29755610\n",
      "Iteration 13, loss = 0.32858721\n",
      "Iteration 2, loss = 0.61395984\n",
      "Iteration 1, loss = 0.83505235\n",
      "Iteration 13, loss = 0.23622225\n",
      "Iteration 5, loss = 0.44550964\n",
      "Iteration 14, loss = 0.30220065\n",
      "Iteration 15, loss = 0.27187306\n",
      "Iteration 8, loss = 0.60338268\n",
      "Iteration 5, loss = 0.44683800\n",
      "Iteration 2, loss = 0.61769868\n",
      "Iteration 15, loss = 0.25051690\n",
      "Iteration 5, loss = 0.48247176\n",
      "Iteration 16, loss = 0.24788446\n",
      "Iteration 14, loss = 0.20961880\n",
      "Iteration 15, loss = 0.27676473\n",
      "Iteration 13, loss = 0.23540030\n",
      "Iteration 2, loss = 0.61048499\n",
      "Iteration 6, loss = 0.43567172\n",
      "Iteration 2, loss = 0.63009124\n",
      "Iteration 6, loss = 0.39487740\n",
      "Iteration 16, loss = 0.25280368\n",
      "Iteration 17, loss = 0.22550939\n",
      "Iteration 15, loss = 0.18638325\n",
      "Iteration 6, loss = 0.41381656\n",
      "Iteration 14, loss = 0.20790333\n",
      "Iteration 18, loss = 0.20522265\n",
      "Iteration 3, loss = 0.60378112\n",
      "Iteration 6, loss = 0.39660461\n",
      "Iteration 3, loss = 0.60376390\n",
      "Iteration 17, loss = 0.23049306\n",
      "Iteration 16, loss = 0.16608951\n",
      "Iteration 16, loss = 0.22783529\n",
      "Iteration 15, loss = 0.18403019\n",
      "Iteration 9, loss = 0.60347911\n",
      "Iteration 6, loss = 0.44670936\n",
      "Iteration 19, loss = 0.18682899\n",
      "Iteration 16, loss = 0.16328953\n",
      "Iteration 17, loss = 0.14862236\n",
      "Iteration 17, loss = 0.20714735\n",
      "Iteration 4, loss = 0.60382834\n",
      "Iteration 4, loss = 0.60388948\n",
      "Iteration 3, loss = 0.60359518\n",
      "Iteration 18, loss = 0.20999120\n",
      "Iteration 17, loss = 0.14555802\n",
      "Iteration 18, loss = 0.13351693\n",
      "Iteration 20, loss = 0.17029839\n",
      "Iteration 7, loss = 0.34379341\n",
      "Iteration 18, loss = 0.18840283\n",
      "Iteration 1, loss = 0.60669188\n",
      "Iteration 10, loss = 0.60333407\n",
      "Iteration 7, loss = 0.40771231\n",
      "Iteration 18, loss = 0.13030966\n",
      "Iteration 3, loss = 0.60441126\n",
      "Iteration 19, loss = 0.12047873\n",
      "Iteration 19, loss = 0.19141604\n",
      "Iteration 7, loss = 0.36565148\n",
      "Iteration 21, loss = 0.15546648\n",
      "Iteration 19, loss = 0.11725598\n",
      "Iteration 5, loss = 0.60361210\n",
      "Iteration 20, loss = 0.10919654\n",
      "Iteration 5, loss = 0.60365441\n",
      "Iteration 19, loss = 0.17140249\n",
      "Iteration 4, loss = 0.60389263\n",
      "Iteration 7, loss = 0.39121529\n",
      "Iteration 20, loss = 0.10599049\n",
      "Iteration 7, loss = 0.34150897\n",
      "Iteration 20, loss = 0.17464680\n",
      "Iteration 4, loss = 0.60434091\n",
      "Iteration 21, loss = 0.09943563\n",
      "Iteration 11, loss = 0.60334651\n",
      "Iteration 22, loss = 0.14228425\n",
      "Iteration 21, loss = 0.15974165\n",
      "Iteration 20, loss = 0.15629161\n",
      "Iteration 22, loss = 0.09084513\n",
      "Iteration 5, loss = 0.60366731\n",
      "Iteration 21, loss = 0.09622925\n",
      "Iteration 8, loss = 0.31707294\n",
      "Iteration 8, loss = 0.29287769\n",
      "Iteration 23, loss = 0.13057634\n",
      "Iteration 21, loss = 0.14282697\n",
      "Iteration 6, loss = 0.60360286\n",
      "Iteration 23, loss = 0.08336590\n",
      "Iteration 8, loss = 0.36570249\n",
      "Iteration 6, loss = 0.60357891\n",
      "Iteration 22, loss = 0.14636368\n",
      "Iteration 12, loss = 0.60331750\n",
      "Iteration 24, loss = 0.12008471\n",
      "Iteration 5, loss = 0.60377249\n",
      "Iteration 22, loss = 0.13073368\n",
      "Iteration 24, loss = 0.07678306\n",
      "Iteration 22, loss = 0.08775203\n",
      "Iteration 2, loss = 0.55377613\n",
      "Iteration 23, loss = 0.12002223\n",
      "Iteration 25, loss = 0.07105454\n",
      "Iteration 9, loss = 0.27139379\n",
      "Iteration 23, loss = 0.08043330\n",
      "Iteration 9, loss = 0.24649656\n",
      "Iteration 23, loss = 0.13434266\n",
      "Iteration 25, loss = 0.11077570\n",
      "Iteration 24, loss = 0.11052686\n",
      "Iteration 26, loss = 0.06593220\n",
      "Iteration 8, loss = 0.29123601\n",
      "Iteration 26, loss = 0.10246607\n",
      "Iteration 13, loss = 0.60334451\n",
      "Iteration 25, loss = 0.10204908\n",
      "Iteration 8, loss = 0.34355702\n",
      "Iteration 7, loss = 0.60354441\n",
      "Iteration 24, loss = 0.07400031\n",
      "Iteration 27, loss = 0.09506016\n",
      "Iteration 10, loss = 0.23078739\n",
      "Iteration 6, loss = 0.60358242\n",
      "Iteration 7, loss = 0.60357453\n",
      "Iteration 6, loss = 0.60368539\n",
      "Iteration 26, loss = 0.09447961\n",
      "Iteration 27, loss = 0.06135127\n",
      "Iteration 25, loss = 0.06831073\n",
      "Iteration 14, loss = 0.60330943\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), solver=sgd; total time= 1.5min\n",
      "Iteration 24, loss = 0.12371554\n",
      "Iteration 10, loss = 0.20713006\n",
      "Iteration 9, loss = 0.32274875\n",
      "Iteration 25, loss = 0.11424305\n",
      "Iteration 28, loss = 0.05733986\n",
      "Iteration 28, loss = 0.08843272\n",
      "Iteration 8, loss = 0.60359034\n",
      "Iteration 7, loss = 0.60356893\n",
      "Iteration 3, loss = 0.51558906\n",
      "Iteration 29, loss = 0.05369521\n",
      "Iteration 29, loss = 0.08245870\n",
      "Iteration 26, loss = 0.06333085\n",
      "Iteration 27, loss = 0.08771646\n",
      "Iteration 26, loss = 0.10575292\n",
      "Iteration 8, loss = 0.60353370\n",
      "Iteration 11, loss = 0.17487644\n",
      "Iteration 30, loss = 0.05041516\n",
      "Iteration 11, loss = 0.19596469\n",
      "Iteration 28, loss = 0.08162980\n",
      "Iteration 9, loss = 0.24623704\n",
      "Iteration 7, loss = 0.60360120\n",
      "Iteration 27, loss = 0.09819089\n",
      "Iteration 30, loss = 0.07711733\n",
      "Iteration 9, loss = 0.29734427\n",
      "Iteration 27, loss = 0.05886337\n",
      "Iteration 9, loss = 0.60354103\n",
      "Iteration 29, loss = 0.07617873\n",
      "Iteration 28, loss = 0.05491439\n",
      "Iteration 12, loss = 0.14855611\n",
      "Iteration 9, loss = 0.60357012\n",
      "Iteration 31, loss = 0.07229464\n",
      "Iteration 4, loss = 0.47518176\n",
      "Iteration 8, loss = 0.60358688\n",
      "Iteration 29, loss = 0.05138899\n",
      "Iteration 30, loss = 0.07129657\n",
      "Iteration 10, loss = 0.28149489\n",
      "Iteration 28, loss = 0.09136925\n",
      "Iteration 10, loss = 0.20785596\n",
      "Iteration 31, loss = 0.04748997\n",
      "Iteration 32, loss = 0.06791319\n",
      "Iteration 8, loss = 0.60353236\n",
      "Iteration 10, loss = 0.60359095\n",
      "Iteration 30, loss = 0.04822617\n",
      "Iteration 1, loss = 0.60664056\n",
      "Iteration 33, loss = 0.06396308\n",
      "Iteration 9, loss = 0.60362138\n",
      "Iteration 34, loss = 0.06034867\n",
      "Iteration 31, loss = 0.04536303\n",
      "Iteration 10, loss = 0.60359188\n",
      "Iteration 9, loss = 0.60356595\n",
      "Iteration 11, loss = 0.24406583\n",
      "Iteration 29, loss = 0.08521754\n",
      "Iteration 31, loss = 0.06684695\n",
      "Iteration 10, loss = 0.25403362\n",
      "Iteration 32, loss = 0.04483068\n",
      "Iteration 10, loss = 0.60356223\n",
      "Iteration 30, loss = 0.07976726\n",
      "Iteration 32, loss = 0.04280191\n",
      "Iteration 35, loss = 0.05708658\n",
      "Iteration 11, loss = 0.60352817\n",
      "Iteration 32, loss = 0.06286714\n",
      "Iteration 12, loss = 0.16724841\n",
      "Iteration 36, loss = 0.05411844\n",
      "Iteration 10, loss = 0.60351196\n",
      "Iteration 33, loss = 0.04048082\n",
      "Iteration 31, loss = 0.07477239\n",
      "Iteration 2, loss = 0.55326043\n",
      "Iteration 11, loss = 0.60355078\n",
      "Iteration 11, loss = 0.17596474\n",
      "Iteration 11, loss = 0.60356622\n",
      "Iteration 37, loss = 0.05140182\n",
      "Iteration 11, loss = 0.21636671\n",
      "Iteration 32, loss = 0.07030448\n",
      "Iteration 12, loss = 0.21060662\n",
      "Iteration 34, loss = 0.03838083\n",
      "Iteration 33, loss = 0.05925753\n",
      "Iteration 33, loss = 0.06623434\n",
      "Iteration 12, loss = 0.60352693\n",
      "Iteration 33, loss = 0.04242506\n",
      "Iteration 13, loss = 0.12740237\n",
      "Iteration 38, loss = 0.04890242\n",
      "Iteration 35, loss = 0.03647220\n",
      "Iteration 34, loss = 0.06253780\n",
      "Iteration 39, loss = 0.04661173\n",
      "Iteration 13, loss = 0.14378830\n",
      "Iteration 11, loss = 0.60352205\n",
      "Iteration 12, loss = 0.60352961\n",
      "Iteration 40, loss = 0.04451193\n",
      "Iteration 36, loss = 0.03470146\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), solver=adam; total time= 2.1min\n",
      "Iteration 34, loss = 0.05595279\n",
      "Iteration 12, loss = 0.18466615\n",
      "Iteration 13, loss = 0.18211460\n",
      "Iteration 35, loss = 0.05914354\n",
      "Iteration 12, loss = 0.15007162\n",
      "Iteration 13, loss = 0.60349206\n",
      "Iteration 35, loss = 0.05296923\n",
      "Iteration 3, loss = 0.51245333\n",
      "Iteration 12, loss = 0.60355256\n",
      "Iteration 34, loss = 0.04024875\n",
      "Iteration 12, loss = 0.60352440\n",
      "Iteration 36, loss = 0.05609059\n",
      "Iteration 36, loss = 0.05023632\n",
      "Iteration 41, loss = 0.04257360\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), solver=adam; total time= 2.2min\n",
      "Iteration 5, loss = 0.43139745\n",
      "Iteration 14, loss = 0.11012149\n",
      "Iteration 14, loss = 0.60347605\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), solver=sgd; total time= 1.7min\n",
      "Iteration 13, loss = 0.15832242\n",
      "Iteration 13, loss = 0.60351512\n",
      "Iteration 13, loss = 0.60352059\n",
      "Iteration 35, loss = 0.03823877\n",
      "Iteration 13, loss = 0.60370557\n",
      "Iteration 37, loss = 0.04775160\n",
      "Iteration 36, loss = 0.03642825\n",
      "Iteration 4, loss = 0.47110374\n",
      "Iteration 37, loss = 0.05328803\n",
      "Iteration 37, loss = 0.03475028\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), solver=adam; total time= 2.4min\n",
      "Iteration 38, loss = 0.05069691\n",
      "Iteration 14, loss = 0.12450507\n",
      "Iteration 14, loss = 0.60367074\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.60346824\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), solver=sgd; total time= 1.8min\n",
      "Iteration 1, loss = 0.61811174\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), solver=sgd; total time= 1.8min\n",
      "Iteration 14, loss = 0.15792057\n",
      "Iteration 38, loss = 0.04545232\n",
      "Iteration 1, loss = 0.59345110\n",
      "Iteration 39, loss = 0.04832605\n",
      "Iteration 14, loss = 0.60348696\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), solver=sgd; total time= 1.8min\n",
      "Iteration 39, loss = 0.04336766\n",
      "Iteration 1, loss = 0.66980110\n",
      "Iteration 40, loss = 0.04615608\n",
      "Iteration 14, loss = 0.13682641\n",
      "Iteration 40, loss = 0.04142782\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), solver=adam; total time= 2.5min\n",
      "Iteration 13, loss = 0.12911162\n",
      "Iteration 6, loss = 0.38393260\n",
      "Iteration 1, loss = 0.62315530\n",
      "Iteration 2, loss = 0.56083871\n",
      "Iteration 15, loss = 0.09630186\n",
      "Iteration 2, loss = 0.55432966\n",
      "Iteration 15, loss = 0.13772890\n",
      "Iteration 1, loss = 0.72011354\n",
      "Iteration 15, loss = 0.10891591\n",
      "Iteration 15, loss = 0.11916155\n",
      "Iteration 1, loss = 0.64495097\n",
      "Iteration 41, loss = 0.04412863\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(50,), solver=adam; total time= 2.6min\n",
      "Iteration 1, loss = 0.69483934\n",
      "Iteration 5, loss = 0.42549202\n",
      "Iteration 2, loss = 0.60506227\n",
      "Iteration 2, loss = 0.55882050\n",
      "Iteration 1, loss = 0.62856008\n",
      "Iteration 7, loss = 0.33355500\n",
      "Iteration 14, loss = 0.11190452\n",
      "Iteration 16, loss = 0.10461532\n",
      "Iteration 2, loss = 0.60629762\n",
      "Iteration 16, loss = 0.08487646\n",
      "Iteration 16, loss = 0.09588931\n",
      "Iteration 16, loss = 0.12110239\n",
      "Iteration 2, loss = 0.60593959\n",
      "Iteration 3, loss = 0.52687806\n",
      "Iteration 15, loss = 0.09792838\n",
      "Iteration 3, loss = 0.60380314\n",
      "Iteration 2, loss = 0.60556075\n",
      "Iteration 17, loss = 0.08517865\n",
      "Iteration 3, loss = 0.52293826\n",
      "Iteration 3, loss = 0.60520716\n",
      "Iteration 3, loss = 0.51549207\n",
      "Iteration 2, loss = 0.60621028\n",
      "Iteration 17, loss = 0.09261111\n",
      "Iteration 6, loss = 0.37689215\n",
      "Iteration 17, loss = 0.10706764\n",
      "Iteration 1, loss = 0.64322074\n",
      "Iteration 16, loss = 0.08643329\n",
      "Iteration 8, loss = 0.28510138\n",
      "Iteration 3, loss = 0.60388935\n",
      "Iteration 17, loss = 0.07545583\n",
      "Iteration 18, loss = 0.08266863\n",
      "Iteration 18, loss = 0.09547147\n",
      "Iteration 4, loss = 0.49187039\n",
      "Iteration 3, loss = 0.60414870\n",
      "Iteration 4, loss = 0.60351795\n",
      "Iteration 4, loss = 0.60416257\n",
      "Iteration 4, loss = 0.60391000\n",
      "Iteration 18, loss = 0.07625211\n",
      "Iteration 3, loss = 0.60482634\n",
      "Iteration 19, loss = 0.07426527\n",
      "Iteration 19, loss = 0.08552220\n",
      "Iteration 4, loss = 0.48651829\n",
      "Iteration 7, loss = 0.32651305\n",
      "Iteration 18, loss = 0.06758304\n",
      "Iteration 5, loss = 0.60366442\n",
      "Iteration 17, loss = 0.07681269\n",
      "Iteration 4, loss = 0.47202033\n",
      "Iteration 4, loss = 0.60367341\n",
      "Iteration 9, loss = 0.24066109\n",
      "Iteration 19, loss = 0.06879936\n",
      "Iteration 20, loss = 0.07716551\n",
      "Iteration 5, loss = 0.60370694\n",
      "Iteration 4, loss = 0.60368929\n",
      "Iteration 2, loss = 0.55462434\n",
      "Iteration 6, loss = 0.60367016\n",
      "Iteration 5, loss = 0.44554930\n",
      "Iteration 5, loss = 0.60351901\n",
      "Iteration 19, loss = 0.06097875\n",
      "Iteration 21, loss = 0.07013486\n",
      "Iteration 5, loss = 0.60369167\n",
      "Iteration 20, loss = 0.06229813\n",
      "Iteration 6, loss = 0.60361542\n",
      "Iteration 5, loss = 0.45667802\n",
      "Iteration 5, loss = 0.41988932\n",
      "Iteration 18, loss = 0.06884415\n",
      "Iteration 20, loss = 0.06714133\n",
      "Iteration 5, loss = 0.60365709\n",
      "Iteration 7, loss = 0.60369369\n",
      "Iteration 20, loss = 0.05533435\n",
      "Iteration 10, loss = 0.20244205\n",
      "Iteration 22, loss = 0.06402441\n",
      "Iteration 8, loss = 0.27782403\n",
      "Iteration 7, loss = 0.60359934\n",
      "Iteration 19, loss = 0.06208998\n",
      "Iteration 6, loss = 0.60369404\n",
      "Iteration 6, loss = 0.60356807\n",
      "Iteration 21, loss = 0.05685565\n",
      "Iteration 3, loss = 0.51401831\n",
      "Iteration 8, loss = 0.60361940\n",
      "Iteration 11, loss = 0.17053879\n",
      "Iteration 21, loss = 0.06117949\n",
      "Iteration 23, loss = 0.05872190\n",
      "Iteration 6, loss = 0.41889210\n",
      "Iteration 6, loss = 0.36382650\n",
      "Iteration 6, loss = 0.39981314\n",
      "Iteration 7, loss = 0.60341989\n",
      "Iteration 21, loss = 0.05059034\n",
      "Iteration 6, loss = 0.60358207\n",
      "Iteration 9, loss = 0.23425596\n",
      "Iteration 20, loss = 0.05635490\n",
      "Iteration 8, loss = 0.60360444\n",
      "Iteration 22, loss = 0.05595642\n",
      "Iteration 22, loss = 0.05216049\n",
      "Iteration 22, loss = 0.04641878\n",
      "Iteration 24, loss = 0.05423948\n",
      "Iteration 9, loss = 0.60360298\n",
      "Iteration 21, loss = 0.05146095\n",
      "Iteration 8, loss = 0.60344789\n",
      "Iteration 9, loss = 0.60361199\n",
      "Iteration 7, loss = 0.60370347\n",
      "Iteration 7, loss = 0.60360048\n",
      "Iteration 12, loss = 0.14471416\n",
      "Iteration 23, loss = 0.04280099\n",
      "Iteration 25, loss = 0.05025009\n",
      "Iteration 10, loss = 0.19705948\n",
      "Iteration 7, loss = 0.35077474\n",
      "Iteration 10, loss = 0.60358674\n",
      "Iteration 22, loss = 0.04722139\n",
      "Iteration 7, loss = 0.30750797\n",
      "Iteration 23, loss = 0.05149997\n",
      "Iteration 7, loss = 0.37870053\n",
      "Iteration 23, loss = 0.04805650\n",
      "Iteration 8, loss = 0.60394866\n",
      "Iteration 9, loss = 0.60341309\n",
      "Iteration 8, loss = 0.60359934\n",
      "Iteration 11, loss = 0.60358419\n",
      "Iteration 10, loss = 0.60359305\n",
      "Iteration 23, loss = 0.04357232\n",
      "Iteration 13, loss = 0.12371977\n",
      "Iteration 26, loss = 0.04679301\n",
      "Iteration 24, loss = 0.03971130\n",
      "Iteration 24, loss = 0.04453908\n",
      "Iteration 9, loss = 0.60353840\n",
      "Iteration 9, loss = 0.60350300\n",
      "Iteration 4, loss = 0.47265667\n",
      "Iteration 8, loss = 0.30210447\n",
      "Iteration 8, loss = 0.33761774\n",
      "Iteration 24, loss = 0.04762288\n",
      "Iteration 25, loss = 0.03696044\n",
      "Iteration 11, loss = 0.60355561\n",
      "Iteration 12, loss = 0.60356010\n",
      "Iteration 27, loss = 0.04374575\n",
      "Iteration 10, loss = 0.60358116\n",
      "Iteration 10, loss = 0.60337127\n",
      "Iteration 14, loss = 0.10690689\n",
      "Iteration 8, loss = 0.25544861\n",
      "Iteration 11, loss = 0.16597527\n",
      "Iteration 25, loss = 0.04142881\n",
      "Iteration 10, loss = 0.60354709\n",
      "Iteration 24, loss = 0.04039723\n",
      "Iteration 28, loss = 0.04102703\n",
      "Iteration 25, loss = 0.04421755\n",
      "Iteration 9, loss = 0.25611443\n",
      "Iteration 11, loss = 0.60340445\n",
      "Iteration 13, loss = 0.60362251\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), solver=sgd; total time= 1.9min\n",
      "Iteration 11, loss = 0.60356272\n",
      "Iteration 26, loss = 0.03456140\n",
      "Iteration 15, loss = 0.09332433\n",
      "Iteration 9, loss = 0.29678193\n",
      "Iteration 11, loss = 0.60355247\n",
      "Iteration 9, loss = 0.21064222\n",
      "Iteration 29, loss = 0.03861639\n",
      "Iteration 12, loss = 0.60355992\n",
      "Iteration 5, loss = 0.43079809\n",
      "Iteration 26, loss = 0.03871115\n",
      "Iteration 26, loss = 0.04124508\n",
      "Iteration 12, loss = 0.60355788\n",
      "Iteration 12, loss = 0.14082475\n",
      "Iteration 12, loss = 0.60374431\n",
      "Iteration 25, loss = 0.03760312\n",
      "Iteration 30, loss = 0.03648722\n",
      "Iteration 12, loss = 0.60349835\n",
      "Iteration 13, loss = 0.60358881\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), solver=sgd; total time= 2.2min\n",
      "Iteration 10, loss = 0.21552610\n",
      "Iteration 16, loss = 0.08208305\n",
      "Iteration 26, loss = 0.03516058\n",
      "Iteration 10, loss = 0.17433625\n",
      "Iteration 13, loss = 0.60359259\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), solver=sgd; total time= 2.2min\n",
      "Iteration 27, loss = 0.03864004\n",
      "Iteration 27, loss = 0.03240182\n",
      "Iteration 27, loss = 0.03630375\n",
      "Iteration 31, loss = 0.03455625\n",
      "Iteration 10, loss = 0.25851582\n",
      "Iteration 13, loss = 0.60358337\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), solver=sgd; total time= 2.3min\n",
      "Iteration 13, loss = 0.60353250\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), solver=sgd; total time= 2.4min\n",
      "Iteration 13, loss = 0.12042053\n",
      "Iteration 1, loss = 0.61188909\n",
      "Iteration 6, loss = 0.38621392\n",
      "Iteration 28, loss = 0.03415389\n",
      "Iteration 17, loss = 0.07290909\n",
      "Iteration 27, loss = 0.03298535\n",
      "Iteration 28, loss = 0.03056214\n",
      "Iteration 32, loss = 0.03282666\n",
      "Iteration 29, loss = 0.03225541\n",
      "Iteration 11, loss = 0.14535821\n",
      "Iteration 29, loss = 0.02885062\n",
      "Iteration 11, loss = 0.18140486Iteration 1, loss = 0.61833505\n",
      "\n",
      "Iteration 11, loss = 0.22377832\n",
      "Iteration 1, loss = 0.62405906\n",
      "Iteration 28, loss = 0.03631943\n",
      "Iteration 1, loss = 0.58834579\n",
      "Iteration 7, loss = 0.34035304\n",
      "Iteration 28, loss = 0.03108064\n",
      "Iteration 1, loss = 0.65390893\n",
      "Iteration 2, loss = 0.54967105\n",
      "Iteration 14, loss = 0.10398821\n",
      "Iteration 33, loss = 0.03126630\n",
      "Iteration 30, loss = 0.03055048\n",
      "Iteration 18, loss = 0.06526352\n",
      "Iteration 29, loss = 0.02933530\n",
      "Iteration 29, loss = 0.03422983\n",
      "Iteration 12, loss = 0.19358857\n",
      "Iteration 12, loss = 0.12261361\n",
      "Iteration 31, loss = 0.02900377\n",
      "Iteration 34, loss = 0.02987099\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), solver=adam; total time= 5.2min\n",
      "Iteration 12, loss = 0.15363823\n",
      "Iteration 30, loss = 0.02734690\n",
      "Iteration 2, loss = 0.55413298\n",
      "Iteration 30, loss = 0.03239953\n",
      "Iteration 19, loss = 0.05889173\n",
      "Iteration 2, loss = 0.55717757\n",
      "Iteration 15, loss = 0.09061545\n",
      "Iteration 2, loss = 0.60621337\n",
      "Iteration 3, loss = 0.50256799\n",
      "Iteration 2, loss = 0.54250498\n",
      "Iteration 31, loss = 0.02595324\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), solver=adam; total time= 5.4min\n",
      "Iteration 32, loss = 0.02759812\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), solver=adam; total time= 5.4min\n",
      "Iteration 13, loss = 0.16756518\n",
      "Iteration 31, loss = 0.03074937\n",
      "Iteration 8, loss = 0.29474538\n",
      "Iteration 13, loss = 0.13081389\n",
      "Iteration 30, loss = 0.02779660\n",
      "Iteration 13, loss = 0.10457542\n",
      "Iteration 16, loss = 0.07977432\n",
      "Iteration 1, loss = 0.69870456\n",
      "Iteration 31, loss = 0.02641153\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), solver=adam; total time= 5.5min\n",
      "Iteration 4, loss = 0.45701599\n",
      "Iteration 14, loss = 0.14580955\n",
      "Iteration 1, loss = 0.62951501\n",
      "Iteration 3, loss = 0.49367414\n",
      "Iteration 3, loss = 0.60482638\n",
      "Iteration 20, loss = 0.05357402\n",
      "Iteration 14, loss = 0.11260466\n",
      "Iteration 17, loss = 0.07081824\n",
      "Iteration 3, loss = 0.51306454\n",
      "Iteration 32, loss = 0.02920815\n",
      "Iteration 14, loss = 0.09050523\n",
      "Iteration 3, loss = 0.51043934\n",
      "Iteration 9, loss = 0.25213006\n",
      "Iteration 1, loss = 0.62129421\n",
      "Iteration 15, loss = 0.12756785\n",
      "Iteration 1, loss = 0.66209594\n",
      "Iteration 33, loss = 0.02788059\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(100,), solver=adam; total time= 5.8min\n",
      "Iteration 15, loss = 0.09790198\n",
      "Iteration 2, loss = 0.60637429\n",
      "Iteration 21, loss = 0.04891847\n",
      "Iteration 5, loss = 0.40933734\n",
      "Iteration 2, loss = 0.60429514\n",
      "Iteration 18, loss = 0.06342256\n",
      "Iteration 1, loss = 0.64137259\n",
      "Iteration 10, loss = 0.21425934\n",
      "Iteration 4, loss = 0.43691135\n",
      "Iteration 2, loss = 0.60599912\n",
      "Iteration 2, loss = 0.57716468\n",
      "Iteration 4, loss = 0.60360667\n",
      "Iteration 4, loss = 0.47178268\n",
      "Iteration 3, loss = 0.54834388\n",
      "Iteration 4, loss = 0.46861901\n",
      "Iteration 16, loss = 0.11249420\n",
      "Iteration 15, loss = 0.07885720\n",
      "Iteration 4, loss = 0.52615656\n",
      "Iteration 22, loss = 0.04506332\n",
      "Iteration 5, loss = 0.50294242\n",
      "Iteration 2, loss = 0.60462926\n",
      "Iteration 16, loss = 0.08596496\n",
      "Iteration 6, loss = 0.47868499\n",
      "Iteration 17, loss = 0.09965216\n",
      "Iteration 3, loss = 0.60526840\n",
      "Iteration 6, loss = 0.35830531\n",
      "Iteration 3, loss = 0.60414300\n",
      "Iteration 3, loss = 0.60516302\n",
      "Iteration 5, loss = 0.60364508\n",
      "Iteration 23, loss = 0.04165934\n",
      "Iteration 5, loss = 0.42223217\n",
      "Iteration 19, loss = 0.05717885\n",
      "Iteration 11, loss = 0.18220814\n",
      "Iteration 5, loss = 0.37531242\n",
      "Iteration 7, loss = 0.45268216\n",
      "Iteration 3, loss = 0.60446527\n",
      "Iteration 18, loss = 0.08906803\n",
      "Iteration 17, loss = 0.07603890\n",
      "Iteration 5, loss = 0.42764878\n",
      "Iteration 16, loss = 0.06955548\n",
      "Iteration 24, loss = 0.03874035\n",
      "Iteration 6, loss = 0.60345338\n",
      "Iteration 4, loss = 0.60372471\n",
      "Iteration 8, loss = 0.42520511\n",
      "Iteration 18, loss = 0.06784196\n",
      "Iteration 19, loss = 0.08005802\n",
      "Iteration 9, loss = 0.39652847\n",
      "Iteration 4, loss = 0.60371384\n",
      "Iteration 6, loss = 0.37223434\n",
      "Iteration 7, loss = 0.30790198\n",
      "Iteration 4, loss = 0.60365977\n",
      "Iteration 6, loss = 0.31132080\n",
      "Iteration 6, loss = 0.37939014\n",
      "Iteration 20, loss = 0.05194300\n",
      "Iteration 10, loss = 0.36721732\n",
      "Iteration 12, loss = 0.15517475\n",
      "Iteration 11, loss = 0.33817476\n",
      "Iteration 5, loss = 0.60395679\n",
      "Iteration 4, loss = 0.60379210\n",
      "Iteration 7, loss = 0.60359488\n",
      "Iteration 19, loss = 0.06110432\n",
      "Iteration 12, loss = 0.31001853\n",
      "Iteration 25, loss = 0.03619023\n",
      "Iteration 17, loss = 0.06191448\n",
      "Iteration 5, loss = 0.60365920\n",
      "Iteration 20, loss = 0.07240638\n",
      "Iteration 13, loss = 0.28352886\n",
      "Iteration 14, loss = 0.25900488\n",
      "Iteration 21, loss = 0.04748606\n",
      "Iteration 7, loss = 0.32038474\n",
      "Iteration 5, loss = 0.60376337\n",
      "Iteration 7, loss = 0.32979645\n",
      "Iteration 15, loss = 0.23670941\n",
      "Iteration 8, loss = 0.26025560\n",
      "Iteration 7, loss = 0.25298220\n",
      "Iteration 20, loss = 0.05535972\n",
      "Iteration 13, loss = 0.13340857\n",
      "Iteration 6, loss = 0.60368808\n",
      "Iteration 16, loss = 0.21672477\n",
      "Iteration 21, loss = 0.06592956\n",
      "Iteration 26, loss = 0.03393684\n",
      "Iteration 18, loss = 0.05561699\n",
      "Iteration 6, loss = 0.60362487\n",
      "Iteration 17, loss = 0.19903502\n",
      "Iteration 8, loss = 0.60349568\n",
      "Iteration 6, loss = 0.60375270\n",
      "Iteration 18, loss = 0.18332050\n",
      "Iteration 8, loss = 0.28160207\n",
      "Iteration 19, loss = 0.16949654\n",
      "Iteration 22, loss = 0.04365768\n",
      "Iteration 5, loss = 0.60365863\n",
      "Iteration 22, loss = 0.06033109\n",
      "Iteration 20, loss = 0.15737153\n",
      "Iteration 7, loss = 0.60375976\n",
      "Iteration 9, loss = 0.21764962\n",
      "Iteration 7, loss = 0.60371946\n",
      "Iteration 14, loss = 0.11531400\n",
      "Iteration 21, loss = 0.05061037\n",
      "Iteration 21, loss = 0.14669979\n",
      "Iteration 8, loss = 0.20374584\n",
      "Iteration 9, loss = 0.60345881\n",
      "Iteration 22, loss = 0.13729415\n",
      "Iteration 19, loss = 0.05044617\n",
      "Iteration 27, loss = 0.03198679\n",
      "Iteration 7, loss = 0.60378350\n",
      "Iteration 8, loss = 0.27078340\n",
      "Iteration 23, loss = 0.12896822\n",
      "Iteration 24, loss = 0.12155243\n",
      "Iteration 23, loss = 0.04035217\n",
      "Iteration 8, loss = 0.60362281\n",
      "Iteration 23, loss = 0.05560289\n",
      "Iteration 8, loss = 0.60358932\n",
      "Iteration 20, loss = 0.04595295\n",
      "Iteration 10, loss = 0.60346623\n",
      "Iteration 15, loss = 0.10066711\n",
      "Iteration 9, loss = 0.23699847\n",
      "Iteration 22, loss = 0.04642444\n",
      "Iteration 25, loss = 0.11495054\n",
      "Iteration 9, loss = 0.16489265\n",
      "Iteration 9, loss = 0.60387067\n",
      "Iteration 10, loss = 0.18205171\n",
      "Iteration 8, loss = 0.60387347\n",
      "Iteration 26, loss = 0.10913405\n",
      "Iteration 6, loss = 0.60365977\n",
      "Iteration 23, loss = 0.04283186\n",
      "Iteration 9, loss = 0.22574239\n",
      "Iteration 27, loss = 0.10380620\n",
      "Iteration 21, loss = 0.04208597\n",
      "Iteration 24, loss = 0.03750146\n",
      "Iteration 11, loss = 0.60345674\n",
      "Iteration 28, loss = 0.09911568\n",
      "Iteration 9, loss = 0.60373476\n",
      "Iteration 24, loss = 0.05141230\n",
      "Iteration 28, loss = 0.03021715\n",
      "Iteration 10, loss = 0.60384333\n",
      "Iteration 29, loss = 0.09481420\n",
      "Iteration 10, loss = 0.13499110\n",
      "Iteration 10, loss = 0.19897861\n",
      "Iteration 30, loss = 0.09099245\n",
      "Iteration 24, loss = 0.03974283\n",
      "Iteration 10, loss = 0.60364406\n",
      "Iteration 7, loss = 0.60365178\n",
      "Iteration 25, loss = 0.04779043\n",
      "Iteration 16, loss = 0.08868318\n",
      "Iteration 29, loss = 0.02867344\n",
      "Iteration 11, loss = 0.60357198\n",
      "Iteration 9, loss = 0.60362788\n",
      "Iteration 11, loss = 0.15269994\n",
      "Iteration 31, loss = 0.08746604\n",
      "Iteration 25, loss = 0.03705672\n",
      "Iteration 12, loss = 0.60349360\n",
      "Iteration 25, loss = 0.03501908\n",
      "Iteration 10, loss = 0.18775209\n",
      "Iteration 32, loss = 0.08429841\n",
      "Iteration 22, loss = 0.03883496\n",
      "Iteration 33, loss = 0.08136271\n",
      "Iteration 10, loss = 0.60359027\n",
      "Iteration 11, loss = 0.60385604\n",
      "Iteration 8, loss = 0.60373184\n",
      "Iteration 11, loss = 0.16726243\n",
      "Iteration 30, loss = 0.02724233\n",
      "Iteration 12, loss = 0.60361585\n",
      "Iteration 26, loss = 0.03470688\n",
      "Iteration 34, loss = 0.07866305\n",
      "Iteration 11, loss = 0.11194920\n",
      "Iteration 17, loss = 0.07884558\n",
      "Iteration 13, loss = 0.60352802\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(200,), solver=sgd; total time= 3.4min\n",
      "Iteration 26, loss = 0.04462087\n",
      "Iteration 12, loss = 0.12906786\n",
      "Iteration 11, loss = 0.60377819\n",
      "Iteration 35, loss = 0.07618289\n",
      "Iteration 11, loss = 0.15666640\n",
      "Iteration 26, loss = 0.03284023\n",
      "Iteration 1, loss = 0.62573703\n",
      "Iteration 31, loss = 0.02599171\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), solver=adam; total time= 7.4min\n",
      "Iteration 2, loss = 0.57498823\n",
      "Iteration 1, loss = 0.60311709\n",
      "Iteration 36, loss = 0.07390025\n",
      "Iteration 9, loss = 0.60353773\n",
      "Iteration 12, loss = 0.14172685\n",
      "Iteration 12, loss = 0.60358592\n",
      "Iteration 3, loss = 0.55337331\n",
      "Iteration 27, loss = 0.03263441\n",
      "Iteration 13, loss = 0.60354135\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.03603766\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(200,), solver=sgd; total time= 2.9min\n",
      "Iteration 12, loss = 0.60368284\n",
      "Iteration 37, loss = 0.07178267\n",
      "Iteration 2, loss = 0.57280541\n",
      "Iteration 12, loss = 0.09421005\n",
      "Iteration 27, loss = 0.04181301\n",
      "Iteration 4, loss = 0.53221501\n",
      "Iteration 1, loss = 0.79028190\n",
      "Iteration 38, loss = 0.06982746\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), solver=adam; total time= 2.6min\n",
      "Iteration 12, loss = 0.13181613\n",
      "Iteration 13, loss = 0.11027365\n",
      "Iteration 2, loss = 0.65788408\n",
      "Iteration 3, loss = 0.55043691\n",
      "Iteration 13, loss = 0.60361565\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(200,), solver=sgd; total time= 2.9min\n",
      "Iteration 3, loss = 0.58814389\n",
      "Iteration 5, loss = 0.51000577\n",
      "Iteration 1, loss = 0.65052952\n",
      "Iteration 27, loss = 0.03090644\n",
      "Iteration 4, loss = 0.52600841\n",
      "Iteration 2, loss = 0.62151246\n",
      "Iteration 24, loss = 0.03356675\n",
      "Iteration 1, loss = 0.81761294\n",
      "Iteration 6, loss = 0.48449946\n",
      "Iteration 10, loss = 0.60359473\n",
      "Iteration 5, loss = 0.49943816\n",
      "Iteration 2, loss = 0.67416636\n",
      "Iteration 7, loss = 0.45705258\n",
      "Iteration 3, loss = 0.60839332\n",
      "Iteration 28, loss = 0.03935082\n",
      "Iteration 28, loss = 0.03079450\n",
      "Iteration 13, loss = 0.60363922\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(200,), solver=sgd; total time= 3.2min\n",
      "Iteration 3, loss = 0.59579822\n",
      "Iteration 6, loss = 0.46967904\n",
      "Iteration 4, loss = 0.60457511\n",
      "Iteration 4, loss = 0.55408693\n",
      "Iteration 13, loss = 0.11203687\n",
      "Iteration 8, loss = 0.42765226\n",
      "Iteration 5, loss = 0.60379872\n",
      "Iteration 13, loss = 0.08058521\n",
      "Iteration 4, loss = 0.55646950\n",
      "Iteration 7, loss = 0.43710802\n",
      "Iteration 13, loss = 0.12090098\n",
      "Iteration 6, loss = 0.60351760\n",
      "Iteration 8, loss = 0.40220274\n",
      "Iteration 1, loss = 0.63390942\n",
      "Iteration 9, loss = 0.39578012\n",
      "Iteration 18, loss = 0.07063334\n",
      "Iteration 5, loss = 0.53375914\n",
      "Iteration 2, loss = 0.61510814\n",
      "Iteration 5, loss = 0.53359370\n",
      "Iteration 11, loss = 0.60365174\n",
      "Iteration 7, loss = 0.60350943\n",
      "Iteration 6, loss = 0.51547457\n",
      "Iteration 28, loss = 0.02918973\n",
      "Iteration 25, loss = 0.03144059\n",
      "Iteration 3, loss = 0.60707447\n",
      "Iteration 6, loss = 0.51636296\n",
      "Iteration 9, loss = 0.36672911\n",
      "Iteration 14, loss = 0.09523479\n",
      "Iteration 8, loss = 0.60346029\n",
      "Iteration 4, loss = 0.60441025\n",
      "Iteration 10, loss = 0.36352564\n",
      "Iteration 7, loss = 0.49917887\n",
      "Iteration 9, loss = 0.60343547\n",
      "Iteration 5, loss = 0.60377375\n",
      "Iteration 29, loss = 0.03711910\n",
      "Iteration 10, loss = 0.33207907\n",
      "Iteration 7, loss = 0.49820887\n",
      "Iteration 10, loss = 0.60341916\n",
      "Iteration 14, loss = 0.06988938\n",
      "Iteration 8, loss = 0.48128622\n",
      "Iteration 14, loss = 0.09633381\n",
      "Iteration 11, loss = 0.60342674\n",
      "Iteration 6, loss = 0.60370855\n",
      "Iteration 9, loss = 0.46264656\n",
      "Iteration 19, loss = 0.06378410\n",
      "Iteration 11, loss = 0.29947716\n",
      "Iteration 12, loss = 0.60341184\n",
      "Iteration 29, loss = 0.02916943\n",
      "Iteration 29, loss = 0.02767360\n",
      "Iteration 13, loss = 0.60340221\n",
      "Iteration 8, loss = 0.48044212\n",
      "Iteration 11, loss = 0.33178631\n",
      "Iteration 12, loss = 0.60354363\n",
      "Iteration 7, loss = 0.60362109\n",
      "Iteration 10, loss = 0.44292395\n",
      "Iteration 14, loss = 0.60337878\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), solver=sgd; total time=  32.6s\n",
      "Iteration 26, loss = 0.02952395\n",
      "Iteration 8, loss = 0.60361113\n",
      "Iteration 12, loss = 0.26950907\n",
      "Iteration 9, loss = 0.60361763\n",
      "Iteration 13, loss = 0.24266943\n",
      "Iteration 1, loss = 0.68706558\n",
      "Iteration 30, loss = 0.03516626\n",
      "Iteration 14, loss = 0.10427109\n",
      "Iteration 14, loss = 0.21915658\n",
      "Iteration 30, loss = 0.02771170\n",
      "Iteration 11, loss = 0.42220599\n",
      "Iteration 9, loss = 0.46196282\n",
      "Iteration 12, loss = 0.30132712\n",
      "Iteration 10, loss = 0.60351986\n",
      "Iteration 15, loss = 0.08370798\n",
      "Iteration 2, loss = 0.63424767\n",
      "Iteration 11, loss = 0.60353515\n",
      "Iteration 13, loss = 0.60358963\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(200,), solver=sgd; total time= 3.9min\n",
      "Iteration 20, loss = 0.05798849\n",
      "Iteration 3, loss = 0.61134930\n",
      "Iteration 15, loss = 0.08296778\n",
      "Iteration 12, loss = 0.40054418\n",
      "Iteration 12, loss = 0.60351096\n",
      "Iteration 30, loss = 0.02632165\n",
      "Iteration 15, loss = 0.19853035\n",
      "Iteration 4, loss = 0.60502908\n",
      "Iteration 10, loss = 0.44279139\n",
      "Iteration 13, loss = 0.60348702\n",
      "Iteration 15, loss = 0.06124726\n",
      "Iteration 13, loss = 0.27306158\n",
      "Iteration 16, loss = 0.18079642\n",
      "Iteration 11, loss = 0.42267808\n",
      "Iteration 27, loss = 0.02788181\n",
      "Iteration 5, loss = 0.60385831\n",
      "Iteration 1, loss = 0.60585322\n",
      "Iteration 13, loss = 0.37853319\n",
      "Iteration 14, loss = 0.60349790\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), solver=sgd; total time=  42.3s\n",
      "Iteration 12, loss = 0.40195607\n",
      "Iteration 17, loss = 0.16550463\n",
      "Iteration 15, loss = 0.09086759\n",
      "Iteration 31, loss = 0.03339418\n",
      "Iteration 2, loss = 0.60455922\n",
      "Iteration 6, loss = 0.60367039\n",
      "Iteration 31, loss = 0.02639151\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.38084407\n",
      "Iteration 14, loss = 0.35612814\n",
      "Iteration 31, loss = 0.02510132\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), solver=adam; total time= 7.2min\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), solver=adam; total time= 7.8min\n",
      "Iteration 3, loss = 0.60406089\n",
      "Iteration 18, loss = 0.15221538\n",
      "Iteration 1, loss = 0.63029911\n",
      "Iteration 14, loss = 0.24721663\n",
      "Iteration 21, loss = 0.05309093\n",
      "Iteration 7, loss = 0.60361089\n",
      "Iteration 4, loss = 0.60368968\n",
      "Iteration 14, loss = 0.35948658\n",
      "Iteration 28, loss = 0.02638040\n",
      "Iteration 5, loss = 0.60360892\n",
      "Iteration 19, loss = 0.14077425\n",
      "Iteration 15, loss = 0.33820109\n",
      "Iteration 2, loss = 0.61397032\n",
      "Iteration 15, loss = 0.22437005Iteration 8, loss = 0.60357934\n",
      "\n",
      "Iteration 6, loss = 0.60358299\n",
      "Iteration 16, loss = 0.07356197\n",
      "Iteration 20, loss = 0.13082197\n",
      "Iteration 16, loss = 0.31762168\n",
      "Iteration 15, loss = 0.33393205\n",
      "Iteration 7, loss = 0.60362029\n",
      "Iteration 16, loss = 0.05426556\n",
      "Iteration 16, loss = 0.20430348\n",
      "Iteration 17, loss = 0.29735395\n",
      "Iteration 16, loss = 0.31212906\n",
      "Iteration 9, loss = 0.60361270\n",
      "Iteration 3, loss = 0.60665475\n",
      "Iteration 32, loss = 0.03182065\n",
      "Iteration 17, loss = 0.18673578\n",
      "Iteration 8, loss = 0.60355146\n",
      "Iteration 1, loss = 0.69660700\n",
      "Iteration 10, loss = 0.60358875\n",
      "Iteration 18, loss = 0.27823096\n",
      "Iteration 17, loss = 0.29122832\n",
      "Iteration 16, loss = 0.07321129\n",
      "Iteration 21, loss = 0.12215657\n",
      "Iteration 1, loss = 0.66103764\n",
      "Iteration 11, loss = 0.60360041\n",
      "Iteration 16, loss = 0.07997007\n",
      "Iteration 19, loss = 0.26027017\n",
      "Iteration 18, loss = 0.27132480\n",
      "Iteration 18, loss = 0.17148342\n",
      "Iteration 4, loss = 0.60435849\n",
      "Iteration 9, loss = 0.60354967\n",
      "Iteration 29, loss = 0.02505678\n",
      "Iteration 2, loss = 0.57147281\n",
      "Iteration 10, loss = 0.60352695\n",
      "Iteration 19, loss = 0.15813335\n",
      "Iteration 22, loss = 0.04888680\n",
      "Iteration 12, loss = 0.60355853\n",
      "Iteration 5, loss = 0.60385610\n",
      "Iteration 17, loss = 0.06526356\n",
      "Iteration 11, loss = 0.60352503\n",
      "Iteration 19, loss = 0.25279644\n",
      "Iteration 20, loss = 0.24352764\n",
      "Iteration 22, loss = 0.11457200\n",
      "Iteration 6, loss = 0.60374079\n",
      "Iteration 12, loss = 0.60351184\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), solver=sgd; total time=  36.0s\n",
      "Iteration 20, loss = 0.14655890\n",
      "Iteration 7, loss = 0.60369318\n",
      "Iteration 2, loss = 0.56445628\n",
      "Iteration 23, loss = 0.10786022\n",
      "Iteration 3, loss = 0.54330454\n",
      "Iteration 13, loss = 0.60353701\n",
      "Iteration 20, loss = 0.23569707\n",
      "Iteration 8, loss = 0.60367810\n",
      "Iteration 33, loss = 0.03038183\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), solver=adam; total time= 7.5min\n",
      "Iteration 24, loss = 0.10194253\n",
      "Iteration 21, loss = 0.22822304\n",
      "Iteration 17, loss = 0.04859831\n",
      "Iteration 9, loss = 0.60368180\n",
      "Iteration 30, loss = 0.02389134\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(150,), solver=adam; total time= 7.6min\n",
      "Iteration 17, loss = 0.06498202\n",
      "Iteration 25, loss = 0.09667746\n",
      "Iteration 21, loss = 0.13643937\n",
      "Iteration 22, loss = 0.21383151\n",
      "Iteration 10, loss = 0.60366546\n",
      "Iteration 4, loss = 0.51832702\n",
      "Iteration 14, loss = 0.60353181\n",
      "Iteration 17, loss = 0.07095602\n",
      "Iteration 26, loss = 0.09197025\n",
      "Iteration 21, loss = 0.21993827\n",
      "Iteration 1, loss = 0.60736335\n",
      "Iteration 23, loss = 0.20083834\n",
      "Iteration 11, loss = 0.60363258\n",
      "Iteration 22, loss = 0.20567057\n",
      "Iteration 24, loss = 0.18901290\n",
      "Iteration 23, loss = 0.04529157\n",
      "Iteration 1, loss = 0.62470126\n",
      "Iteration 15, loss = 0.60353877\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), solver=sgd; total time= 1.0min\n",
      "Iteration 1, loss = 0.72518647\n",
      "Iteration 25, loss = 0.17824790\n",
      "Iteration 23, loss = 0.19264567\n",
      "Iteration 22, loss = 0.12760817\n",
      "Iteration 5, loss = 0.49206179\n",
      "Iteration 12, loss = 0.60366071\n",
      "Iteration 27, loss = 0.08776415\n",
      "Iteration 3, loss = 0.53829867\n",
      "Iteration 26, loss = 0.16854276\n",
      "Iteration 2, loss = 0.56306192\n",
      "Iteration 23, loss = 0.11981202\n",
      "Iteration 13, loss = 0.60363557\n",
      "Iteration 18, loss = 0.05831340\n",
      "Iteration 2, loss = 0.56626626\n",
      "Iteration 24, loss = 0.18095739\n",
      "Iteration 14, loss = 0.60362434\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), solver=sgd; total time=  48.5sIteration 27, loss = 0.15962930\n",
      "\n",
      "Iteration 28, loss = 0.08398882\n",
      "Iteration 24, loss = 0.11298607\n",
      "Iteration 28, loss = 0.15157079\n",
      "Iteration 18, loss = 0.05843636\n",
      "Iteration 6, loss = 0.46521328\n",
      "Iteration 4, loss = 0.50964290\n",
      "Iteration 25, loss = 0.10690130\n",
      "Iteration 29, loss = 0.14426635\n",
      "Iteration 18, loss = 0.04385367\n",
      "Iteration 29, loss = 0.08063550\n",
      "Iteration 25, loss = 0.17036428\n",
      "Iteration 3, loss = 0.54044606\n",
      "Iteration 2, loss = 0.58075246\n",
      "Iteration 26, loss = 0.10154090\n",
      "Iteration 30, loss = 0.07747481\n",
      "Iteration 1, loss = 0.75190357\n",
      "Iteration 3, loss = 0.53222617\n",
      "Iteration 5, loss = 0.48172296\n",
      "Iteration 27, loss = 0.09670539\n",
      "Iteration 31, loss = 0.07467655\n",
      "Iteration 30, loss = 0.13763426\n",
      "Iteration 18, loss = 0.06359059\n",
      "Iteration 28, loss = 0.09239386\n",
      "Iteration 32, loss = 0.07210596\n",
      "Iteration 24, loss = 0.04215022\n",
      "Iteration 31, loss = 0.13157340\n",
      "Iteration 29, loss = 0.08849751\n",
      "Iteration 1, loss = 0.60563129\n",
      "Iteration 33, loss = 0.06974358\n",
      "Iteration 26, loss = 0.16083312\n",
      "Iteration 3, loss = 0.54804691\n",
      "Iteration 2, loss = 0.61991711\n",
      "Iteration 6, loss = 0.45137406\n",
      "Iteration 4, loss = 0.51288317\n",
      "Iteration 27, loss = 0.15222793\n",
      "Iteration 34, loss = 0.06757844\n",
      "Iteration 7, loss = 0.43640165\n",
      "Iteration 32, loss = 0.12603255\n",
      "Iteration 4, loss = 0.50053714\n",
      "Iteration 19, loss = 0.05274389\n",
      "Iteration 28, loss = 0.14442737\n",
      "Iteration 30, loss = 0.08496436\n",
      "Iteration 33, loss = 0.12096517\n",
      "Iteration 3, loss = 0.60348819\n",
      "Iteration 35, loss = 0.06557605\n",
      "Iteration 19, loss = 0.05273149\n",
      "Iteration 19, loss = 0.03993835\n",
      "Iteration 29, loss = 0.13744787\n",
      "Iteration 34, loss = 0.11635274\n",
      "Iteration 31, loss = 0.08177598\n",
      "Iteration 5, loss = 0.46533004\n",
      "Iteration 35, loss = 0.11205879\n",
      "Iteration 32, loss = 0.07885748\n",
      "Iteration 4, loss = 0.52551794\n",
      "Iteration 4, loss = 0.60412803\n",
      "Iteration 36, loss = 0.10814397\n",
      "Iteration 8, loss = 0.40597199\n",
      "Iteration 33, loss = 0.07618791\n",
      "Iteration 2, loss = 0.60426093\n",
      "Iteration 36, loss = 0.06370007\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), solver=adam; total time= 2.4min\n",
      "Iteration 37, loss = 0.10451318\n",
      "Iteration 7, loss = 0.41928194\n",
      "Iteration 34, loss = 0.07376970\n",
      "Iteration 30, loss = 0.13108559\n",
      "Iteration 38, loss = 0.10114461\n",
      "Iteration 6, loss = 0.42657163\n",
      "Iteration 5, loss = 0.50188358\n",
      "Iteration 20, loss = 0.03662174\n",
      "Iteration 5, loss = 0.60365036\n",
      "Iteration 3, loss = 0.60383388\n",
      "Iteration 1, loss = 0.61448984\n",
      "Iteration 35, loss = 0.07150733\n",
      "Iteration 39, loss = 0.09804243\n",
      "Iteration 5, loss = 0.48205728\n",
      "Iteration 31, loss = 0.12531204\n",
      "Iteration 7, loss = 0.38465765\n",
      "Iteration 25, loss = 0.03942094\n",
      "Iteration 20, loss = 0.04800970\n",
      "Iteration 4, loss = 0.60391640\n",
      "Iteration 19, loss = 0.05741430\n",
      "Iteration 40, loss = 0.09513422\n",
      "Iteration 36, loss = 0.06946276\n",
      "Iteration 32, loss = 0.12004109\n",
      "Iteration 6, loss = 0.60364260\n",
      "Iteration 20, loss = 0.04798986\n",
      "Iteration 6, loss = 0.47705683\n",
      "Iteration 37, loss = 0.06752262\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), solver=adam; total time= 2.7min\n",
      "Iteration 2, loss = 0.60516744\n",
      "Iteration 41, loss = 0.09245273\n",
      "Iteration 8, loss = 0.38542659\n",
      "Iteration 9, loss = 0.37415266\n",
      "Iteration 42, loss = 0.08994659\n",
      "Iteration 5, loss = 0.60378488\n",
      "Iteration 33, loss = 0.11524621\n",
      "Iteration 7, loss = 0.60357428\n",
      "Iteration 8, loss = 0.34217871\n",
      "Iteration 43, loss = 0.08761136\n",
      "Iteration 7, loss = 0.45038653\n",
      "Iteration 34, loss = 0.11085737\n",
      "Iteration 44, loss = 0.08541549\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), solver=adam; total time= 2.5minIteration 3, loss = 0.60404787\n",
      "\n",
      "Iteration 1, loss = 0.67453542\n",
      "Iteration 21, loss = 0.03375541\n",
      "Iteration 35, loss = 0.10685955\n",
      "Iteration 10, loss = 0.34232770\n",
      "Iteration 6, loss = 0.60376564\n",
      "Iteration 26, loss = 0.03703330\n",
      "Iteration 21, loss = 0.04402618\n",
      "Iteration 4, loss = 0.60371863\n",
      "Iteration 36, loss = 0.10316239\n",
      "Iteration 2, loss = 0.61088473\n",
      "Iteration 8, loss = 0.60353910\n",
      "Iteration 6, loss = 0.44875145\n",
      "Iteration 9, loss = 0.35115207\n",
      "Iteration 8, loss = 0.42212679\n",
      "Iteration 21, loss = 0.04397488\n",
      "Iteration 1, loss = 0.79983679\n",
      "Iteration 7, loss = 0.60374447\n",
      "Iteration 11, loss = 0.31104297\n",
      "Iteration 3, loss = 0.60350306\n",
      "Iteration 37, loss = 0.09980568\n",
      "Iteration 5, loss = 0.60369830\n",
      "Iteration 9, loss = 0.30149878\n",
      "Iteration 2, loss = 0.62360768\n",
      "Iteration 22, loss = 0.03134170\n",
      "Iteration 38, loss = 0.09661519\n",
      "Iteration 4, loss = 0.60416388\n",
      "Iteration 20, loss = 0.05226135\n",
      "Iteration 9, loss = 0.60354899\n",
      "Iteration 8, loss = 0.60372508\n",
      "Iteration 6, loss = 0.60374369\n",
      "Iteration 7, loss = 0.41143715\n",
      "Iteration 10, loss = 0.26398036\n",
      "Iteration 9, loss = 0.39221872\n",
      "Iteration 39, loss = 0.09371610\n",
      "Iteration 10, loss = 0.31721379\n",
      "Iteration 27, loss = 0.03489260\n",
      "Iteration 3, loss = 0.60487756\n",
      "Iteration 9, loss = 0.60383001\n",
      "Iteration 7, loss = 0.60367649\n",
      "Iteration 11, loss = 0.23151041\n",
      "Iteration 22, loss = 0.04059786\n",
      "Iteration 5, loss = 0.60382007\n",
      "Iteration 12, loss = 0.28158523\n",
      "Iteration 23, loss = 0.02923456\n",
      "Iteration 40, loss = 0.09100956\n",
      "Iteration 10, loss = 0.36163791\n",
      "Iteration 11, loss = 0.28512320\n",
      "Iteration 4, loss = 0.60431676\n",
      "Iteration 12, loss = 0.20367127\n",
      "Iteration 10, loss = 0.60363706\n",
      "Iteration 41, loss = 0.08849990\n",
      "Iteration 6, loss = 0.60388429\n",
      "Iteration 10, loss = 0.60375148\n",
      "Iteration 22, loss = 0.04054493\n",
      "Iteration 8, loss = 0.60376240\n",
      "Iteration 13, loss = 0.25453218\n",
      "Iteration 5, loss = 0.60388473\n",
      "Iteration 42, loss = 0.08616685\n",
      "Iteration 11, loss = 0.33083967\n",
      "Iteration 13, loss = 0.18079023\n",
      "Iteration 8, loss = 0.37216948\n",
      "Iteration 11, loss = 0.60353259\n",
      "Iteration 9, loss = 0.60366543\n",
      "Iteration 43, loss = 0.08395733\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50,), solver=adam; total time= 3.2min\n",
      "Iteration 28, loss = 0.03302784\n",
      "Iteration 12, loss = 0.25601019\n",
      "Iteration 11, loss = 0.60379225\n",
      "Iteration 7, loss = 0.60379853\n",
      "Iteration 21, loss = 0.04784802\n",
      "Iteration 12, loss = 0.60351038\n",
      "Iteration 14, loss = 0.16168879\n",
      "Iteration 9, loss = 0.33304623\n",
      "Iteration 6, loss = 0.60371080\n",
      "Iteration 13, loss = 0.22993091\n",
      "Iteration 8, loss = 0.60374024\n",
      "Iteration 10, loss = 0.60369696\n",
      "Iteration 13, loss = 0.60350290\n",
      "Iteration 14, loss = 0.23040837\n",
      "Iteration 24, loss = 0.02740587\n",
      "Iteration 12, loss = 0.60368037\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), solver=sgd; total time= 1.6min\n",
      "Iteration 15, loss = 0.14605748\n",
      "Iteration 12, loss = 0.30131921\n",
      "Iteration 10, loss = 0.29558975\n",
      "Iteration 23, loss = 0.03759112\n",
      "Iteration 7, loss = 0.60369838\n",
      "Iteration 15, loss = 0.20911244\n",
      "Iteration 14, loss = 0.60349337\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), solver=sgd; total time= 1.8min\n",
      "Iteration 14, loss = 0.20725775\n",
      "Iteration 11, loss = 0.60366638\n",
      "Iteration 1, loss = 0.59525314\n",
      "Iteration 23, loss = 0.03771971\n",
      "Iteration 9, loss = 0.60373086\n",
      "Iteration 22, loss = 0.04413299\n",
      "Iteration 13, loss = 0.27368527\n",
      "Iteration 16, loss = 0.19071795\n",
      "Iteration 29, loss = 0.03137713\n",
      "Iteration 15, loss = 0.18781358\n",
      "Iteration 16, loss = 0.13298567\n",
      "Iteration 8, loss = 0.60372219\n",
      "Iteration 25, loss = 0.02576203\n",
      "Iteration 1, loss = 0.63954852\n",
      "Iteration 11, loss = 0.26140731\n",
      "Iteration 24, loss = 0.03503137\n",
      "Iteration 17, loss = 0.17467952\n",
      "Iteration 12, loss = 0.60362168\n",
      "Iteration 16, loss = 0.17094669\n",
      "Iteration 10, loss = 0.60380920\n",
      "Iteration 14, loss = 0.24845450\n",
      "Iteration 1, loss = 0.58998070\n",
      "Iteration 12, loss = 0.23186155\n",
      "Iteration 18, loss = 0.16100229\n",
      "Iteration 2, loss = 0.55416335\n",
      "Iteration 15, loss = 0.22593430\n",
      "Iteration 2, loss = 0.56028722\n",
      "Iteration 11, loss = 0.60374497\n",
      "Iteration 17, loss = 0.15670412\n",
      "Iteration 9, loss = 0.60370550\n",
      "Iteration 17, loss = 0.12205469\n",
      "Iteration 13, loss = 0.60364559\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), solver=sgd; total time= 1.6min\n",
      "Iteration 30, loss = 0.02988610\n",
      "Iteration 12, loss = 0.60369647\n",
      "Iteration 13, loss = 0.20593998\n",
      "Iteration 19, loss = 0.14911234\n",
      "Iteration 25, loss = 0.03280888\n",
      "Iteration 26, loss = 0.02435043\n",
      "Iteration 23, loss = 0.04093378\n",
      "Iteration 24, loss = 0.03514776\n",
      "Iteration 16, loss = 0.20607504\n",
      "Iteration 10, loss = 0.60365663\n",
      "Iteration 18, loss = 0.14447259\n",
      "Iteration 1, loss = 0.61132265\n",
      "Iteration 18, loss = 0.11287974\n",
      "Iteration 20, loss = 0.13888210\n",
      "Iteration 13, loss = 0.60377121\n",
      "Iteration 3, loss = 0.52646929\n",
      "Iteration 2, loss = 0.55078409\n",
      "Iteration 26, loss = 0.03088923\n",
      "Iteration 11, loss = 0.60364932\n",
      "Iteration 19, loss = 0.13401928\n",
      "Iteration 2, loss = 0.56053733\n",
      "Iteration 3, loss = 0.51656701\n",
      "Iteration 21, loss = 0.12999240\n",
      "Iteration 14, loss = 0.60366688\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), solver=sgd; total time= 1.7min\n",
      "Iteration 17, loss = 0.18870472\n",
      "Iteration 19, loss = 0.10502024\n",
      "Iteration 12, loss = 0.60366065\n",
      "Iteration 14, loss = 0.18435890\n",
      "Iteration 24, loss = 0.03816403\n",
      "Iteration 31, loss = 0.02854315\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(200,), solver=adam; total time=10.0min\n",
      "Iteration 25, loss = 0.03295148\n",
      "Iteration 27, loss = 0.02309267\n",
      "Iteration 13, loss = 0.60366651\n",
      "Iteration 4, loss = 0.47378306\n",
      "Iteration 4, loss = 0.49340111\n",
      "Iteration 27, loss = 0.02918875\n",
      "Iteration 18, loss = 0.17375411\n",
      "Iteration 1, loss = 0.59955163\n",
      "Iteration 20, loss = 0.12501539\n",
      "Iteration 3, loss = 0.52560930\n",
      "Iteration 22, loss = 0.12219046\n",
      "Iteration 15, loss = 0.16617755\n",
      "Iteration 20, loss = 0.09841266\n",
      "Iteration 1, loss = 0.67466472\n",
      "Iteration 14, loss = 0.60380260\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n",
      "Iteration 3, loss = 0.50994782\n",
      "Iteration 23, loss = 0.11542893\n",
      "Iteration 16, loss = 0.15109401\n",
      "Iteration 25, loss = 0.03572609\n",
      "Iteration 19, loss = 0.16076594\n",
      "Iteration 21, loss = 0.09262583\n",
      "Iteration 24, loss = 0.10946469\n",
      "Iteration 21, loss = 0.11717878\n",
      "Iteration 4, loss = 0.49097072\n",
      "Iteration 26, loss = 0.03102917\n",
      "Iteration 2, loss = 0.55995831\n",
      "Iteration 4, loss = 0.46374917\n",
      "Iteration 28, loss = 0.02199923\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(200,), solver=adam; total time= 8.3min\n",
      "Iteration 5, loss = 0.45879414\n",
      "Iteration 28, loss = 0.02768085\n",
      "Iteration 25, loss = 0.10421268\n",
      "Iteration 1, loss = 0.70152591\n",
      "Iteration 20, loss = 0.14953299\n",
      "Iteration 5, loss = 0.41145067\n",
      "Iteration 17, loss = 0.13839861\n",
      "Iteration 22, loss = 0.11044741\n",
      "Iteration 22, loss = 0.08762868\n",
      "Iteration 26, loss = 0.09946405\n",
      "Iteration 6, loss = 0.42225934\n",
      "Iteration 3, loss = 0.52585126\n",
      "Iteration 1, loss = 0.74408063\n",
      "Iteration 2, loss = 0.60637529\n",
      "Iteration 2, loss = 0.60726804\n",
      "Iteration 21, loss = 0.13980419\n",
      "Iteration 23, loss = 0.10454938\n",
      "Iteration 5, loss = 0.42463537\n",
      "Iteration 26, loss = 0.03358723\n",
      "Iteration 27, loss = 0.09525842\n",
      "Iteration 6, loss = 0.35665959\n",
      "Iteration 27, loss = 0.02930868\n",
      "Iteration 2, loss = 0.60648054\n",
      "Iteration 23, loss = 0.08328471\n",
      "Iteration 24, loss = 0.09933388\n",
      "Iteration 22, loss = 0.13130179\n",
      "Iteration 18, loss = 0.12774508\n",
      "Iteration 7, loss = 0.38259329\n",
      "Iteration 3, loss = 0.60522614\n",
      "Iteration 5, loss = 0.45151513\n",
      "Iteration 4, loss = 0.48820517\n",
      "Iteration 6, loss = 0.37077629\n",
      "Iteration 28, loss = 0.09150756\n",
      "Iteration 3, loss = 0.60601369\n",
      "Iteration 27, loss = 0.03171103\n",
      "Iteration 29, loss = 0.02633205\n",
      "Iteration 3, loss = 0.60484632\n",
      "Iteration 23, loss = 0.12390537\n",
      "Iteration 19, loss = 0.11869186\n",
      "Iteration 24, loss = 0.07947645\n",
      "Iteration 25, loss = 0.09464736\n",
      "Iteration 4, loss = 0.60406197\n",
      "Iteration 4, loss = 0.60386442\n",
      "Iteration 7, loss = 0.30379752\n",
      "Iteration 4, loss = 0.60424697\n",
      "Iteration 28, loss = 0.02779402\n",
      "Iteration 8, loss = 0.34327898\n",
      "Iteration 25, loss = 0.07599402\n",
      "Iteration 30, loss = 0.02514013\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(200,), solver=adam; total time= 9.0min\n",
      "Iteration 24, loss = 0.11731713\n",
      "Iteration 5, loss = 0.60391395\n",
      "Iteration 5, loss = 0.44473911\n",
      "Iteration 5, loss = 0.60376921\n",
      "Iteration 29, loss = 0.08806564\n",
      "Iteration 20, loss = 0.11093672\n",
      "Iteration 7, loss = 0.31757552\n",
      "Iteration 26, loss = 0.09057893\n",
      "Iteration 26, loss = 0.07294146\n",
      "Iteration 28, loss = 0.03005795\n",
      "Iteration 8, loss = 0.25733345\n",
      "Iteration 6, loss = 0.60381331\n",
      "Iteration 1, loss = 0.63124548\n",
      "Iteration 6, loss = 0.40864342\n",
      "Iteration 30, loss = 0.08497702\n",
      "Iteration 25, loss = 0.11152909\n",
      "Iteration 5, loss = 0.60379562\n",
      "Iteration 6, loss = 0.60371154\n",
      "Iteration 21, loss = 0.10426512\n",
      "Iteration 6, loss = 0.39611705\n",
      "Iteration 8, loss = 0.26933290\n",
      "Iteration 27, loss = 0.07023863\n",
      "Iteration 9, loss = 0.30418240\n",
      "Iteration 29, loss = 0.02644303\n",
      "Iteration 27, loss = 0.08686676\n",
      "Iteration 2, loss = 0.60406036\n",
      "Iteration 26, loss = 0.10639859\n",
      "Iteration 7, loss = 0.60373545\n",
      "Iteration 7, loss = 0.60376984\n",
      "Iteration 6, loss = 0.60398605\n",
      "Iteration 31, loss = 0.08219148\n",
      "Iteration 22, loss = 0.09846152\n",
      "Iteration 9, loss = 0.21850487\n",
      "Iteration 7, loss = 0.36295294\n",
      "Iteration 27, loss = 0.10174339\n",
      "Iteration 28, loss = 0.06772112\n",
      "Iteration 3, loss = 0.60443729\n",
      "Iteration 29, loss = 0.02857183\n",
      "Iteration 28, loss = 0.08354325\n",
      "Iteration 8, loss = 0.60381873\n",
      "Iteration 7, loss = 0.60402252\n",
      "Iteration 9, loss = 0.22875366\n",
      "Iteration 23, loss = 0.09336575\n",
      "Iteration 30, loss = 0.02524067\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(200,), solver=adam; total time= 9.8min\n",
      "Iteration 29, loss = 0.06549080\n",
      "Iteration 32, loss = 0.07959866\n",
      "Iteration 10, loss = 0.26895269\n",
      "Iteration 28, loss = 0.09764218\n",
      "Iteration 7, loss = 0.34573043\n",
      "Iteration 8, loss = 0.60377264\n",
      "Iteration 29, loss = 0.08055040\n",
      "Iteration 9, loss = 0.60371736\n",
      "Iteration 30, loss = 0.06342476\n",
      "Iteration 33, loss = 0.07726077\n",
      "Iteration 29, loss = 0.09388673\n",
      "Iteration 8, loss = 0.60393775\n",
      "Iteration 10, loss = 0.18729036\n",
      "Iteration 1, loss = 0.61657876\n",
      "Iteration 4, loss = 0.60426477\n",
      "Iteration 8, loss = 0.29790891\n",
      "Iteration 9, loss = 0.60374941\n",
      "Iteration 10, loss = 0.19530517\n",
      "Iteration 24, loss = 0.08892971\n",
      "Iteration 11, loss = 0.23726888\n",
      "Iteration 10, loss = 0.60362034\n",
      "Iteration 34, loss = 0.07508755\n",
      "Iteration 8, loss = 0.31750461\n",
      "Iteration 31, loss = 0.06154788\n",
      "Iteration 30, loss = 0.02723998\n",
      "Iteration 30, loss = 0.07788525\n",
      "Iteration 30, loss = 0.09053632\n",
      "Iteration 9, loss = 0.25480975\n",
      "Iteration 2, loss = 0.60401987\n",
      "Iteration 25, loss = 0.08495100\n",
      "Iteration 12, loss = 0.21017689\n",
      "Iteration 11, loss = 0.60362366\n",
      "Iteration 35, loss = 0.07305652\n",
      "Iteration 5, loss = 0.60389518\n",
      "Iteration 9, loss = 0.60391889\n",
      "Iteration 31, loss = 0.08745394\n",
      "Iteration 10, loss = 0.60372530\n",
      "Iteration 26, loss = 0.08144231\n",
      "Iteration 31, loss = 0.07537115\n",
      "Iteration 11, loss = 0.16275437\n",
      "Iteration 32, loss = 0.05981604\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), solver=adam; total time= 4.9min\n",
      "Iteration 36, loss = 0.07119176\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), solver=adam; total time= 5.3min\n",
      "Iteration 11, loss = 0.16921908\n",
      "Iteration 12, loss = 0.60367738\n",
      "Iteration 3, loss = 0.60407228\n",
      "Iteration 6, loss = 0.60399429\n",
      "Iteration 31, loss = 0.02604608\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.0001, hidden_layer_sizes=(200,), solver=adam; total time=10.0min\n",
      "Iteration 32, loss = 0.07308402\n",
      "Iteration 9, loss = 0.27584181\n",
      "Iteration 32, loss = 0.08462873\n",
      "Iteration 13, loss = 0.18730468\n",
      "Iteration 27, loss = 0.07825930\n",
      "Iteration 12, loss = 0.14321246\n",
      "Iteration 11, loss = 0.60378477\n",
      "Iteration 10, loss = 0.60398055\n",
      "Iteration 7, loss = 0.60388996\n",
      "Iteration 4, loss = 0.60423326\n",
      "Iteration 28, loss = 0.07540415\n",
      "Iteration 12, loss = 0.14868882\n",
      "Iteration 13, loss = 0.60369690\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), solver=sgd; total time= 2.3min\n",
      "Iteration 10, loss = 0.23887724\n",
      "Iteration 33, loss = 0.07097256\n",
      "Iteration 10, loss = 0.21869590\n",
      "Iteration 33, loss = 0.08204743\n",
      "Iteration 1, loss = 0.61558827\n",
      "Iteration 1, loss = 0.60531849\n",
      "Iteration 1, loss = 0.65278115\n",
      "Iteration 29, loss = 0.07280652\n",
      "Iteration 8, loss = 0.60383401\n",
      "Iteration 34, loss = 0.07970146\n",
      "Iteration 13, loss = 0.12777763\n",
      "Iteration 12, loss = 0.60378913\n",
      "Iteration 11, loss = 0.20841531\n",
      "Iteration 5, loss = 0.60395220\n",
      "Iteration 30, loss = 0.07047230\n",
      "Iteration 34, loss = 0.06903514\n",
      "Iteration 11, loss = 0.60396325\n",
      "Iteration 13, loss = 0.13264727\n",
      "Iteration 14, loss = 0.16828302\n",
      "Iteration 2, loss = 0.55518667\n",
      "Iteration 35, loss = 0.07746816\n",
      "Iteration 9, loss = 0.60382569\n",
      "Iteration 1, loss = 0.59816443\n",
      "Iteration 6, loss = 0.60401284\n",
      "Iteration 13, loss = 0.60372839\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), solver=sgd; total time= 2.4min\n",
      "Iteration 12, loss = 0.60386696\n",
      "Iteration 12, loss = 0.18290213\n",
      "Iteration 11, loss = 0.18950677\n",
      "Iteration 36, loss = 0.07547229\n",
      "Iteration 2, loss = 0.54881417\n",
      "Iteration 31, loss = 0.06828508\n",
      "Iteration 35, loss = 0.06723801\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), solver=adam; total time= 5.9min\n",
      "Iteration 14, loss = 0.11968301\n",
      "Iteration 7, loss = 0.60408339\n",
      "Iteration 14, loss = 0.11539310\n",
      "Iteration 2, loss = 0.55851163\n",
      "Iteration 10, loss = 0.60383621\n",
      "Iteration 13, loss = 0.60384152\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), solver=sgd; total time= 2.5min\n",
      "Iteration 37, loss = 0.07356525\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), solver=adam; total time= 5.6min\n",
      "Iteration 32, loss = 0.06630847\n",
      "Iteration 13, loss = 0.16262251\n",
      "Iteration 12, loss = 0.16620119\n",
      "Iteration 3, loss = 0.50585008\n",
      "Iteration 15, loss = 0.15252304\n",
      "Iteration 1, loss = 0.63368370\n",
      "Iteration 1, loss = 0.63207697\n",
      "Iteration 11, loss = 0.60381440\n",
      "Iteration 3, loss = 0.51436627\n",
      "Iteration 15, loss = 0.10548708\n",
      "Iteration 15, loss = 0.10925728\n",
      "Iteration 8, loss = 0.60401026\n",
      "Iteration 3, loss = 0.51535435\n",
      "Iteration 2, loss = 0.54800198\n",
      "Iteration 2, loss = 0.60423127\n",
      "Iteration 1, loss = 0.73664086\n",
      "Iteration 13, loss = 0.14748615\n",
      "Iteration 33, loss = 0.06444897\n",
      "Iteration 4, loss = 0.46306281\n",
      "Iteration 12, loss = 0.60377710\n",
      "Iteration 16, loss = 0.10064208\n",
      "Iteration 14, loss = 0.14602722\n",
      "Iteration 4, loss = 0.47586871\n",
      "Iteration 3, loss = 0.60457411\n",
      "Iteration 1, loss = 0.61800547\n",
      "Iteration 16, loss = 0.13934768\n",
      "Iteration 3, loss = 0.50516326\n",
      "Iteration 16, loss = 0.09717377\n",
      "Iteration 9, loss = 0.60390542\n",
      "Iteration 2, loss = 0.60679730\n",
      "Iteration 4, loss = 0.47844817\n",
      "Iteration 5, loss = 0.41586942\n",
      "Iteration 4, loss = 0.60381289\n",
      "Iteration 2, loss = 0.60405839\n",
      "Iteration 14, loss = 0.13271975\n",
      "Iteration 2, loss = 0.55793039\n",
      "Iteration 10, loss = 0.60399894\n",
      "Iteration 17, loss = 0.12836356\n",
      "Iteration 17, loss = 0.09041310\n",
      "Iteration 34, loss = 0.06278487\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100,), solver=adam; total time= 6.2min\n",
      "Iteration 17, loss = 0.09371745\n",
      "Iteration 13, loss = 0.60376510\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), solver=sgd; total time= 2.5min\n",
      "Iteration 4, loss = 0.45970693\n",
      "Iteration 5, loss = 0.43336904\n",
      "Iteration 15, loss = 0.13271521\n",
      "Iteration 3, loss = 0.60687051\n",
      "Iteration 11, loss = 0.60396253\n",
      "Iteration 1, loss = 0.68435501\n",
      "Iteration 3, loss = 0.60459219\n",
      "Iteration 15, loss = 0.12068094\n",
      "Iteration 5, loss = 0.60388905\n",
      "Iteration 3, loss = 0.51772975\n",
      "Iteration 16, loss = 0.12159083\n",
      "Iteration 12, loss = 0.60383010\n",
      "Iteration 6, loss = 0.36655145\n",
      "Iteration 18, loss = 0.08462327\n",
      "Iteration 5, loss = 0.44116429\n",
      "Iteration 6, loss = 0.38844899\n",
      "Iteration 5, loss = 0.41001300\n",
      "Iteration 2, loss = 0.60621524\n",
      "Iteration 1, loss = 0.67644008\n",
      "Iteration 18, loss = 0.08756122\n",
      "Iteration 4, loss = 0.60450328\n",
      "Iteration 4, loss = 0.60405116\n",
      "Iteration 18, loss = 0.11895297\n",
      "Iteration 19, loss = 0.07979801\n",
      "Iteration 4, loss = 0.48108938\n",
      "Iteration 16, loss = 0.11087553\n",
      "Iteration 13, loss = 0.60382582\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), solver=sgd; total time= 2.4min\n",
      "Iteration 6, loss = 0.40135317\n",
      "Iteration 3, loss = 0.60560747\n",
      "Iteration 1, loss = 0.69221954\n",
      "Iteration 6, loss = 0.35786039\n",
      "Iteration 2, loss = 0.60506859\n",
      "Iteration 19, loss = 0.08246502\n",
      "Iteration 19, loss = 0.11106433\n",
      "Iteration 20, loss = 0.07552889\n",
      "Iteration 2, loss = 0.60464891\n",
      "Iteration 6, loss = 0.60367957\n",
      "Iteration 17, loss = 0.11249588\n",
      "Iteration 5, loss = 0.60416451\n",
      "Iteration 3, loss = 0.56740135\n",
      "Iteration 5, loss = 0.60409011\n",
      "Iteration 7, loss = 0.31714312\n",
      "Iteration 20, loss = 0.10429574\n",
      "Iteration 4, loss = 0.54949053\n",
      "Iteration 7, loss = 0.34227078\n",
      "Iteration 7, loss = 0.36076268\n",
      "Iteration 5, loss = 0.53620923\n",
      "Iteration 6, loss = 0.60387905\n",
      "Iteration 7, loss = 0.60402323\n",
      "Iteration 18, loss = 0.10476032\n",
      "Iteration 6, loss = 0.60413557\n",
      "Iteration 6, loss = 0.52308853\n",
      "Iteration 20, loss = 0.07810440\n",
      "Iteration 21, loss = 0.07188317\n",
      "Iteration 21, loss = 0.09842923\n",
      "Iteration 17, loss = 0.10269028\n",
      "Iteration 4, loss = 0.60404561\n",
      "Iteration 5, loss = 0.44231164\n",
      "Iteration 7, loss = 0.60388285\n",
      "Iteration 3, loss = 0.60543188\n",
      "Iteration 7, loss = 0.30604506\n",
      "Iteration 7, loss = 0.60408995\n",
      "Iteration 7, loss = 0.50974013\n",
      "Iteration 8, loss = 0.29783963\n",
      "Iteration 8, loss = 0.49599950\n",
      "Iteration 22, loss = 0.06868057\n",
      "Iteration 19, loss = 0.09822412\n",
      "Iteration 8, loss = 0.27199757\n",
      "Iteration 8, loss = 0.60387670\n",
      "Iteration 4, loss = 0.60399800\n",
      "Iteration 9, loss = 0.48160964\n",
      "Iteration 8, loss = 0.32078942\n",
      "Iteration 21, loss = 0.07423663\n",
      "Iteration 10, loss = 0.46665369\n",
      "Iteration 8, loss = 0.60386246\n",
      "Iteration 22, loss = 0.09336385\n",
      "Iteration 8, loss = 0.60413764\n",
      "Iteration 5, loss = 0.60425722\n",
      "Iteration 9, loss = 0.60386939\n",
      "Iteration 5, loss = 0.60398523\n",
      "Iteration 11, loss = 0.45100356\n",
      "Iteration 20, loss = 0.09264494\n",
      "Iteration 12, loss = 0.43482628\n",
      "Iteration 8, loss = 0.25941550\n",
      "Iteration 23, loss = 0.06584564\n",
      "Iteration 9, loss = 0.25771182\n",
      "Iteration 18, loss = 0.09583159\n",
      "Iteration 13, loss = 0.41840405\n",
      "Iteration 22, loss = 0.07096915\n",
      "Iteration 14, loss = 0.40202427\n",
      "Iteration 9, loss = 0.23279268\n",
      "Iteration 6, loss = 0.40093629\n",
      "Iteration 10, loss = 0.60384599\n",
      "Iteration 9, loss = 0.60404794\n",
      "Iteration 23, loss = 0.08893196\n",
      "Iteration 15, loss = 0.38577690\n",
      "Iteration 21, loss = 0.08784129\n",
      "Iteration 9, loss = 0.60372334\n",
      "Iteration 9, loss = 0.28292207\n",
      "Iteration 6, loss = 0.60391395\n",
      "Iteration 16, loss = 0.36990113\n",
      "Iteration 24, loss = 0.06330379\n",
      "Iteration 19, loss = 0.09009154\n",
      "Iteration 17, loss = 0.35453057\n",
      "Iteration 6, loss = 0.60401015\n",
      "Iteration 10, loss = 0.22341794\n",
      "Iteration 9, loss = 0.22048777\n",
      "Iteration 10, loss = 0.60394324\n",
      "Iteration 23, loss = 0.06799964\n",
      "Iteration 22, loss = 0.08356871\n",
      "Iteration 24, loss = 0.08497237\n",
      "Iteration 10, loss = 0.60377574\n",
      "Iteration 10, loss = 0.24886071\n",
      "Iteration 18, loss = 0.33991153\n",
      "Iteration 20, loss = 0.08508518\n",
      "Iteration 7, loss = 0.60389924\n",
      "Iteration 7, loss = 0.60409904\n",
      "Iteration 10, loss = 0.20049342\n",
      "Iteration 23, loss = 0.07987516\n",
      "Iteration 25, loss = 0.06103614\n",
      "Iteration 7, loss = 0.35817854\n",
      "Iteration 11, loss = 0.60381938\n",
      "Iteration 25, loss = 0.08152019\n",
      "Iteration 19, loss = 0.32616269\n",
      "Iteration 11, loss = 0.60397693\n",
      "Iteration 20, loss = 0.31324716\n",
      "Iteration 26, loss = 0.05902876\n",
      "Iteration 11, loss = 0.19522685\n",
      "Iteration 24, loss = 0.06535967\n",
      "Iteration 21, loss = 0.30117634\n",
      "Iteration 24, loss = 0.07659412\n",
      "Iteration 12, loss = 0.60416518\n",
      "Iteration 8, loss = 0.60393652\n",
      "Iteration 11, loss = 0.17482191\n",
      "Iteration 22, loss = 0.29011170\n",
      "Iteration 8, loss = 0.60401627\n",
      "Iteration 23, loss = 0.27964782\n",
      "Iteration 27, loss = 0.05715760\n",
      "Iteration 11, loss = 0.60368828\n",
      "Iteration 10, loss = 0.18910579\n",
      "Iteration 25, loss = 0.06300674\n",
      "Iteration 12, loss = 0.60391192\n",
      "Iteration 26, loss = 0.07836851\n",
      "Iteration 21, loss = 0.08079107\n",
      "Iteration 24, loss = 0.27019577\n",
      "Iteration 25, loss = 0.07366167\n",
      "Iteration 13, loss = 0.60386643\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(200,), solver=sgd; total time= 2.7min\n",
      "Iteration 8, loss = 0.31513904\n",
      "Iteration 11, loss = 0.21937357\n",
      "Iteration 28, loss = 0.05545825\n",
      "Iteration 9, loss = 0.60387085\n",
      "Iteration 1, loss = 0.59678976\n",
      "Iteration 25, loss = 0.26118175\n",
      "Iteration 2, loss = 0.57498683\n",
      "Iteration 13, loss = 0.60396943\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(200,), solver=sgd; total time= 2.8min\n",
      "Iteration 12, loss = 0.15418246\n",
      "Iteration 26, loss = 0.25304529\n",
      "Iteration 26, loss = 0.06087376\n",
      "Iteration 3, loss = 0.55610930\n",
      "Iteration 29, loss = 0.05393013\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), solver=adam; total time= 6.6min\n",
      "Iteration 1, loss = 0.69893238\n",
      "Iteration 4, loss = 0.53645363\n",
      "Iteration 12, loss = 0.60376012\n",
      "Iteration 10, loss = 0.60399448\n",
      "Iteration 9, loss = 0.60399133\n",
      "Iteration 12, loss = 0.17242691\n",
      "Iteration 27, loss = 0.07556164\n",
      "Iteration 1, loss = 0.61522142\n",
      "Iteration 27, loss = 0.24548312\n",
      "Iteration 11, loss = 0.16444120\n",
      "Iteration 22, loss = 0.07701857\n",
      "Iteration 2, loss = 0.60600199\n",
      "Iteration 2, loss = 0.57882936\n",
      "Iteration 5, loss = 0.51563826\n",
      "Iteration 27, loss = 0.05891092\n",
      "Iteration 9, loss = 0.27557560\n",
      "Iteration 28, loss = 0.23849810\n",
      "Iteration 3, loss = 0.56417430\n",
      "Iteration 6, loss = 0.49288539\n",
      "Iteration 3, loss = 0.56741321\n",
      "Iteration 12, loss = 0.19451991\n",
      "Iteration 4, loss = 0.54867412\n",
      "Iteration 7, loss = 0.46869906\n",
      "Iteration 10, loss = 0.60412866\n",
      "Iteration 13, loss = 0.13787022\n",
      "Iteration 26, loss = 0.07101315\n",
      "Iteration 13, loss = 0.60375326\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(200,), solver=sgd; total time= 3.2min\n",
      "Iteration 29, loss = 0.23188277\n",
      "Iteration 8, loss = 0.44392177\n",
      "Iteration 28, loss = 0.05715648\n",
      "Iteration 9, loss = 0.41811652\n",
      "Iteration 4, loss = 0.54941574\n",
      "Iteration 1, loss = 0.75976927\n",
      "Iteration 23, loss = 0.07372203\n",
      "Iteration 5, loss = 0.53301064\n",
      "Iteration 28, loss = 0.07300685\n",
      "Iteration 30, loss = 0.22586206\n",
      "Iteration 11, loss = 0.60398642\n",
      "Iteration 11, loss = 0.60396886\n",
      "Iteration 10, loss = 0.24083910\n",
      "Iteration 6, loss = 0.51625870\n",
      "Iteration 5, loss = 0.53594366\n",
      "Iteration 2, loss = 0.64272101\n",
      "Iteration 10, loss = 0.39360549\n",
      "Iteration 7, loss = 0.49769616\n",
      "Iteration 31, loss = 0.22016311\n",
      "Iteration 27, loss = 0.06861136\n",
      "Iteration 6, loss = 0.52270860\n",
      "Iteration 3, loss = 0.58371918\n",
      "Iteration 8, loss = 0.47763933\n",
      "Iteration 12, loss = 0.14531911\n",
      "Iteration 32, loss = 0.21486274\n",
      "Iteration 11, loss = 0.36982591\n",
      "Iteration 29, loss = 0.07069659\n",
      "Iteration 12, loss = 0.34757980\n",
      "Iteration 13, loss = 0.17382987\n",
      "Iteration 33, loss = 0.20993941\n",
      "Iteration 24, loss = 0.07075001\n",
      "Iteration 9, loss = 0.45630729\n",
      "Iteration 7, loss = 0.50930713\n",
      "Iteration 12, loss = 0.60401437\n",
      "Iteration 29, loss = 0.05550982\n",
      "Iteration 4, loss = 0.55840132\n",
      "Iteration 13, loss = 0.32732763\n",
      "Iteration 12, loss = 0.60387692\n",
      "Iteration 13, loss = 0.15384900\n",
      "Iteration 10, loss = 0.43376314\n",
      "Iteration 34, loss = 0.20524982\n",
      "Iteration 8, loss = 0.49551190\n",
      "Iteration 5, loss = 0.54351918\n",
      "Iteration 14, loss = 0.30890147\n",
      "Iteration 11, loss = 0.41131259\n",
      "Iteration 35, loss = 0.20085337\n",
      "Iteration 9, loss = 0.48111077\n",
      "Iteration 11, loss = 0.21097600\n",
      "Iteration 6, loss = 0.53154682\n",
      "Iteration 36, loss = 0.19679946\n",
      "Iteration 15, loss = 0.29225882\n",
      "Iteration 12, loss = 0.38869138\n",
      "Iteration 10, loss = 0.46602837\n",
      "Iteration 14, loss = 0.12466280\n",
      "Iteration 16, loss = 0.27734809\n",
      "Iteration 7, loss = 0.52000999\n",
      "Iteration 13, loss = 0.36694567\n",
      "Iteration 11, loss = 0.45035432\n",
      "Iteration 28, loss = 0.06644989\n",
      "Iteration 13, loss = 0.60398693\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(200,), solver=sgd; total time= 3.1min\n",
      "Iteration 17, loss = 0.26406490\n",
      "Iteration 30, loss = 0.06856869\n",
      "Iteration 13, loss = 0.13001436\n",
      "Iteration 37, loss = 0.19282798\n",
      "Iteration 1, loss = 0.69345293\n",
      "Iteration 12, loss = 0.43421308\n",
      "Iteration 25, loss = 0.06811445\n",
      "Iteration 13, loss = 0.60382959\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(200,), solver=sgd; total time= 3.1min\n",
      "Iteration 18, loss = 0.25223997\n",
      "Iteration 38, loss = 0.18913626\n",
      "Iteration 14, loss = 0.34689505\n",
      "Iteration 13, loss = 0.41763597\n",
      "Iteration 30, loss = 0.05401886\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), solver=adam; total time= 7.6min\n",
      "Iteration 8, loss = 0.50822832\n",
      "Iteration 39, loss = 0.18566033\n",
      "Iteration 14, loss = 0.15674862\n",
      "Iteration 2, loss = 0.63668755\n",
      "Iteration 12, loss = 0.18635300\n",
      "Iteration 1, loss = 0.67945795\n",
      "Iteration 1, loss = 0.67550015\n",
      "Iteration 9, loss = 0.49623582\n",
      "Iteration 14, loss = 0.40116817\n",
      "Iteration 40, loss = 0.18232355\n",
      "Iteration 15, loss = 0.32774725\n",
      "Iteration 19, loss = 0.24157824\n",
      "Iteration 2, loss = 0.63084819\n",
      "Iteration 3, loss = 0.61312582\n",
      "Iteration 2, loss = 0.63301347\n",
      "Iteration 31, loss = 0.06659136\n",
      "Iteration 15, loss = 0.38489469\n",
      "Iteration 14, loss = 0.13895415\n",
      "Iteration 4, loss = 0.60588228\n",
      "Iteration 3, loss = 0.61135097\n",
      "Iteration 41, loss = 0.17912087\n",
      "Iteration 10, loss = 0.48360773\n",
      "Iteration 20, loss = 0.23198344\n",
      "Iteration 16, loss = 0.31060800\n",
      "Iteration 4, loss = 0.60600393\n",
      "Iteration 5, loss = 0.60450582\n",
      "Iteration 16, loss = 0.36889609\n",
      "Iteration 3, loss = 0.61075891\n",
      "Iteration 42, loss = 0.17618548\n",
      "Iteration 21, loss = 0.22331807\n",
      "Iteration 5, loss = 0.60477158\n",
      "Iteration 11, loss = 0.47071281\n",
      "Iteration 29, loss = 0.06444271\n",
      "Iteration 4, loss = 0.60578898\n",
      "Iteration 6, loss = 0.60431894\n",
      "Iteration 22, loss = 0.21551824\n",
      "Iteration 43, loss = 0.17330992\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), solver=adam; total time= 2.9min\n",
      "Iteration 6, loss = 0.60451528\n",
      "Iteration 14, loss = 0.11777566\n",
      "Iteration 12, loss = 0.45730235\n",
      "Iteration 17, loss = 0.29512917\n",
      "Iteration 26, loss = 0.06576895\n",
      "Iteration 15, loss = 0.11401697\n",
      "Iteration 7, loss = 0.60420837\n",
      "Iteration 7, loss = 0.60440451\n",
      "Iteration 17, loss = 0.35344012\n",
      "Iteration 1, loss = 0.71374361\n",
      "Iteration 5, loss = 0.60475322\n",
      "Iteration 8, loss = 0.60426569\n",
      "Iteration 13, loss = 0.44325036\n",
      "Iteration 8, loss = 0.60439757\n",
      "Iteration 6, loss = 0.60453747\n",
      "Iteration 23, loss = 0.20854935\n",
      "Iteration 32, loss = 0.06479184\n",
      "Iteration 9, loss = 0.60418853\n",
      "Iteration 9, loss = 0.60440948\n",
      "Iteration 18, loss = 0.28109818\n",
      "Iteration 14, loss = 0.42911079\n",
      "Iteration 2, loss = 0.64404639\n",
      "Iteration 7, loss = 0.60442874\n",
      "Iteration 10, loss = 0.60417621\n",
      "Iteration 18, loss = 0.33878092\n",
      "Iteration 10, loss = 0.60435360\n",
      "Iteration 24, loss = 0.20194466\n",
      "Iteration 30, loss = 0.06262468\n",
      "Iteration 11, loss = 0.60418093\n",
      "Iteration 3, loss = 0.61415916\n",
      "Iteration 8, loss = 0.60443238\n",
      "Iteration 11, loss = 0.60434535\n",
      "Iteration 25, loss = 0.19603773\n",
      "Iteration 15, loss = 0.41472805\n",
      "Iteration 19, loss = 0.26855880\n",
      "Iteration 12, loss = 0.60415576\n",
      "Iteration 12, loss = 0.60431979\n",
      "Iteration 9, loss = 0.60441877\n",
      "Iteration 13, loss = 0.60413771\n",
      "Iteration 20, loss = 0.25733355\n",
      "Iteration 19, loss = 0.32489507\n",
      "Iteration 13, loss = 0.60431288\n",
      "Iteration 26, loss = 0.19053728\n",
      "Iteration 14, loss = 0.60414589\n",
      "Iteration 4, loss = 0.60651987\n",
      "Iteration 16, loss = 0.40036346Iteration 10, loss = 0.60447626\n",
      "\n",
      "Iteration 13, loss = 0.16635703\n",
      "Iteration 33, loss = 0.06311462\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 14, loss = 0.60431189\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), solver=adam; total time= 7.9min\n",
      "Iteration 15, loss = 0.60410748\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), solver=sgd; total time=  35.7s\n",
      "Iteration 20, loss = 0.31192467\n",
      "Iteration 15, loss = 0.12697371\n",
      "Iteration 21, loss = 0.24707163\n",
      "Iteration 15, loss = 0.10796211\n",
      "Iteration 1, loss = 0.83951811\n",
      "Iteration 31, loss = 0.06094330\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.29975692\n",
      "Iteration 17, loss = 0.38632680[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), solver=adam; total time= 7.5min\n",
      "\n",
      "Iteration 5, loss = 0.60484168\n",
      "Iteration 15, loss = 0.60426468\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), solver=sgd; total time=  33.6s\n",
      "Iteration 2, loss = 0.68857019\n",
      "Iteration 11, loss = 0.60437439\n",
      "Iteration 27, loss = 0.18544851\n",
      "Iteration 15, loss = 0.14279861\n",
      "Iteration 18, loss = 0.37243612\n",
      "Iteration 22, loss = 0.28859390\n",
      "Iteration 22, loss = 0.23786782\n",
      "Iteration 16, loss = 0.10531282\n",
      "Iteration 3, loss = 0.62408558\n",
      "Iteration 6, loss = 0.60460899\n",
      "Iteration 1, loss = 0.59275729\n",
      "Iteration 28, loss = 0.18081242\n",
      "Iteration 23, loss = 0.27811916\n",
      "Iteration 12, loss = 0.60435829\n",
      "Iteration 27, loss = 0.06356385\n",
      "Iteration 7, loss = 0.60459374\n",
      "Iteration 4, loss = 0.60815653\n",
      "Iteration 23, loss = 0.22962125\n",
      "Iteration 19, loss = 0.35904836\n",
      "Iteration 13, loss = 0.60437821\n",
      "Iteration 24, loss = 0.26850593\n",
      "Iteration 29, loss = 0.17648598\n",
      "Iteration 5, loss = 0.60495889\n",
      "Iteration 14, loss = 0.60436277\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), solver=sgd; total time=  43.9s\n",
      "Iteration 24, loss = 0.22203830\n",
      "Iteration 8, loss = 0.60453130\n",
      "Iteration 20, loss = 0.34612627\n",
      "Iteration 25, loss = 0.25962085\n",
      "Iteration 6, loss = 0.60452941\n",
      "Iteration 30, loss = 0.17238491\n",
      "Iteration 25, loss = 0.21515296\n",
      "Iteration 21, loss = 0.33389401\n",
      "Iteration 9, loss = 0.60442458\n",
      "Iteration 26, loss = 0.25143113\n",
      "Iteration 16, loss = 0.13115390\n",
      "Iteration 7, loss = 0.60442873\n",
      "Iteration 31, loss = 0.16857942\n",
      "Iteration 26, loss = 0.20879472\n",
      "Iteration 16, loss = 0.11686751\n",
      "Iteration 8, loss = 0.60439045\n",
      "Iteration 1, loss = 0.68719159\n",
      "Iteration 27, loss = 0.24374754\n",
      "Iteration 10, loss = 0.60442803\n",
      "Iteration 22, loss = 0.32228613\n",
      "Iteration 27, loss = 0.20289569\n",
      "Iteration 32, loss = 0.16505964\n",
      "Iteration 9, loss = 0.60436714\n",
      "Iteration 2, loss = 0.56409036\n",
      "Iteration 11, loss = 0.60436452\n",
      "Iteration 28, loss = 0.23672997\n",
      "Iteration 33, loss = 0.16167009\n",
      "Iteration 10, loss = 0.60437021\n",
      "Iteration 16, loss = 0.09992145\n",
      "Iteration 28, loss = 0.19757851\n",
      "Iteration 12, loss = 0.60435513\n",
      "Iteration 11, loss = 0.60434999\n",
      "Iteration 14, loss = 0.14991441\n",
      "Iteration 34, loss = 0.15854244\n",
      "Iteration 29, loss = 0.23017078\n",
      "Iteration 17, loss = 0.09804690\n",
      "Iteration 12, loss = 0.60433858\n",
      "Iteration 1, loss = 0.62176124\n",
      "Iteration 29, loss = 0.19254506\n",
      "Iteration 35, loss = 0.15555453\n",
      "Iteration 30, loss = 0.22396191\n",
      "Iteration 23, loss = 0.31131215\n",
      "Iteration 1, loss = 0.59980904\n",
      "Iteration 13, loss = 0.60435138\n",
      "Iteration 13, loss = 0.60435462\n",
      "Iteration 30, loss = 0.18798321\n",
      "Iteration 36, loss = 0.15274809\n",
      "Iteration 14, loss = 0.60439933\n",
      "Iteration 28, loss = 0.06164071\n",
      "Iteration 31, loss = 0.21828656\n",
      "Iteration 37, loss = 0.15005154\n",
      "Iteration 15, loss = 0.60431734\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), solver=sgd; total time=  31.5s\n",
      "Iteration 2, loss = 0.57729527\n",
      "Iteration 31, loss = 0.18356710\n",
      "Iteration 14, loss = 0.60435740\n",
      "Iteration 17, loss = 0.12141148\n",
      "Iteration 17, loss = 0.10863671\n",
      "Iteration 32, loss = 0.17949334\n",
      "Iteration 2, loss = 0.57056746\n",
      "Iteration 15, loss = 0.60435353\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), solver=sgd; total time=  52.1s\n",
      "Iteration 3, loss = 0.53706928\n",
      "Iteration 32, loss = 0.21302470\n",
      "Iteration 24, loss = 0.30108032\n",
      "Iteration 33, loss = 0.17568773\n",
      "Iteration 38, loss = 0.14752894\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), solver=adam; total time= 2.2min\n",
      "Iteration 15, loss = 0.13652470\n",
      "Iteration 33, loss = 0.20801878\n",
      "Iteration 2, loss = 0.57069552\n",
      "Iteration 34, loss = 0.17221049\n",
      "Iteration 17, loss = 0.09319630\n",
      "Iteration 34, loss = 0.20331167\n",
      "Iteration 3, loss = 0.54896857\n",
      "Iteration 25, loss = 0.29130534\n",
      "Iteration 1, loss = 0.66700401\n",
      "Iteration 3, loss = 0.55570581\n",
      "Iteration 18, loss = 0.09192757\n",
      "Iteration 35, loss = 0.16876017\n",
      "Iteration 1, loss = 0.62879677\n",
      "Iteration 26, loss = 0.28232582\n",
      "Iteration 1, loss = 0.63748396\n",
      "Iteration 2, loss = 0.60768203\n",
      "Iteration 36, loss = 0.16557116\n",
      "Iteration 27, loss = 0.27392227\n",
      "Iteration 35, loss = 0.19894917\n",
      "Iteration 4, loss = 0.52906847\n",
      "Iteration 28, loss = 0.26593530\n",
      "Iteration 29, loss = 0.05981905\n",
      "Iteration 2, loss = 0.60978868\n",
      "Iteration 18, loss = 0.11323943\n",
      "Iteration 36, loss = 0.19480093\n",
      "Iteration 4, loss = 0.50939495\n",
      "Iteration 29, loss = 0.25860754\n",
      "Iteration 3, loss = 0.60503069\n",
      "Iteration 4, loss = 0.53892454\n",
      "Iteration 37, loss = 0.16251182\n",
      "Iteration 18, loss = 0.08756969\n",
      "Iteration 19, loss = 0.08674904\n",
      "Iteration 3, loss = 0.54667613\n",
      "Iteration 38, loss = 0.15967635\n",
      "Iteration 37, loss = 0.19091680\n",
      "Iteration 5, loss = 0.50870359\n",
      "Iteration 30, loss = 0.25168505\n",
      "Iteration 3, loss = 0.60508553\n",
      "Iteration 2, loss = 0.56954636\n",
      "Iteration 38, loss = 0.18722886\n",
      "Iteration 31, loss = 0.24528696\n",
      "Iteration 5, loss = 0.47932967\n",
      "Iteration 4, loss = 0.60519069\n",
      "Iteration 16, loss = 0.12545158\n",
      "Iteration 32, loss = 0.23919827\n",
      "Iteration 18, loss = 0.10165255\n",
      "Iteration 39, loss = 0.18365087\n",
      "Iteration 5, loss = 0.52229464\n",
      "Iteration 33, loss = 0.23344164\n",
      "Iteration 40, loss = 0.18031462\n",
      "Iteration 5, loss = 0.60517403\n",
      "Iteration 6, loss = 0.44770517\n",
      "Iteration 39, loss = 0.15689663\n",
      "Iteration 4, loss = 0.60545409\n",
      "Iteration 30, loss = 0.05815205\n",
      "Iteration 41, loss = 0.17715377\n",
      "Iteration 4, loss = 0.52390168\n",
      "Iteration 6, loss = 0.48660602\n",
      "Iteration 5, loss = 0.60545064\n",
      "Iteration 34, loss = 0.22815983\n",
      "Iteration 20, loss = 0.08226454\n",
      "Iteration 6, loss = 0.50583341\n",
      "Iteration 7, loss = 0.41508601\n",
      "Iteration 40, loss = 0.15434911\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 0.11628868\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), solver=adam; total time= 2.5min\n",
      "Iteration 19, loss = 0.08275424\n",
      "Iteration 35, loss = 0.22303050\n",
      "Iteration 6, loss = 0.60507765\n",
      "Iteration 36, loss = 0.21832528\n",
      "Iteration 42, loss = 0.17419946\n",
      "Iteration 7, loss = 0.46291810\n",
      "Iteration 3, loss = 0.54656575\n",
      "Iteration 19, loss = 0.10622880\n",
      "Iteration 7, loss = 0.48840727\n",
      "Iteration 37, loss = 0.21379533\n",
      "Iteration 1, loss = 0.70792873\n",
      "Iteration 43, loss = 0.17129715\n",
      "Iteration 38, loss = 0.20963909\n",
      "Iteration 39, loss = 0.20553447\n",
      "Iteration 31, loss = 0.05661915\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.43782908\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(150,), solver=adam; total time= 8.6min\n",
      "Iteration 6, loss = 0.60527479\n",
      "Iteration 5, loss = 0.49878207\n",
      "Iteration 2, loss = 0.61780245\n",
      "Iteration 7, loss = 0.60514017\n",
      "Iteration 4, loss = 0.52652179\n",
      "Iteration 40, loss = 0.20172772\n",
      "Iteration 20, loss = 0.07847226\n",
      "Iteration 8, loss = 0.46988554\n",
      "Iteration 44, loss = 0.16850799\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), solver=adam; total time= 2.9min\n",
      "Iteration 8, loss = 0.38318726\n",
      "Iteration 3, loss = 0.60550237\n",
      "Iteration 41, loss = 0.19805809\n",
      "Iteration 1, loss = 0.77588516\n",
      "Iteration 9, loss = 0.41204150\n",
      "Iteration 20, loss = 0.10028599\n",
      "Iteration 19, loss = 0.09574650\n",
      "Iteration 42, loss = 0.19463576\n",
      "Iteration 6, loss = 0.47123548\n",
      "Iteration 5, loss = 0.50723180\n",
      "Iteration 7, loss = 0.60524980\n",
      "Iteration 2, loss = 0.62469539\n",
      "Iteration 9, loss = 0.45050246\n",
      "Iteration 43, loss = 0.19130199\n",
      "Iteration 18, loss = 0.10855749\n",
      "Iteration 4, loss = 0.60548688\n",
      "Iteration 9, loss = 0.35315181\n",
      "Iteration 44, loss = 0.18816132\n",
      "Iteration 8, loss = 0.60516246\n",
      "Iteration 10, loss = 0.38638768\n",
      "Iteration 7, loss = 0.44163228\n",
      "Iteration 10, loss = 0.43056419\n",
      "Iteration 8, loss = 0.60529892\n",
      "Iteration 45, loss = 0.18516360\n",
      "Iteration 21, loss = 0.07837557\n",
      "Iteration 6, loss = 0.48848575\n",
      "Iteration 5, loss = 0.60535405\n",
      "Iteration 1, loss = 0.61813574\n",
      "Iteration 3, loss = 0.60506503\n",
      "Iteration 9, loss = 0.60505723\n",
      "Iteration 11, loss = 0.36175903\n",
      "Iteration 46, loss = 0.18230845\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50,), solver=adam; total time= 2.9min\n",
      "Iteration 21, loss = 0.09504426\n",
      "Iteration 10, loss = 0.32600631\n",
      "Iteration 9, loss = 0.60526393\n",
      "Iteration 21, loss = 0.07494347\n",
      "Iteration 8, loss = 0.41116816\n",
      "Iteration 19, loss = 0.10197524\n",
      "Iteration 11, loss = 0.40975266\n",
      "Iteration 2, loss = 0.60706100\n",
      "Iteration 6, loss = 0.60528508\n",
      "Iteration 20, loss = 0.09061574\n",
      "Iteration 11, loss = 0.30210319\n",
      "Iteration 9, loss = 0.38137329\n",
      "Iteration 10, loss = 0.60521827\n",
      "Iteration 3, loss = 0.60540853\n",
      "Iteration 4, loss = 0.60567240\n",
      "Iteration 10, loss = 0.60503179\n",
      "Iteration 7, loss = 0.60546601\n",
      "Iteration 1, loss = 0.64651323\n",
      "Iteration 12, loss = 0.33910551\n",
      "Iteration 22, loss = 0.09058782\n",
      "Iteration 22, loss = 0.07494678\n",
      "Iteration 12, loss = 0.38912482\n",
      "Iteration 8, loss = 0.60539974\n",
      "Iteration 12, loss = 0.28186186\n",
      "Iteration 5, loss = 0.60554842\n",
      "Iteration 7, loss = 0.46898133\n",
      "Iteration 21, loss = 0.08617508\n",
      "Iteration 11, loss = 0.60520719\n",
      "Iteration 9, loss = 0.60534204\n",
      "Iteration 4, loss = 0.60548115\n",
      "Iteration 22, loss = 0.07169650\n",
      "Iteration 10, loss = 0.35305857\n",
      "Iteration 11, loss = 0.60507708\n",
      "Iteration 13, loss = 0.36951782\n",
      "Iteration 2, loss = 0.56307327\n",
      "Iteration 13, loss = 0.26441475\n",
      "Iteration 6, loss = 0.60541568\n",
      "Iteration 20, loss = 0.09634188\n",
      "Iteration 10, loss = 0.60523770\n",
      "Iteration 12, loss = 0.60501463\n",
      "Iteration 14, loss = 0.35073311\n",
      "Iteration 23, loss = 0.07192420\n",
      "Iteration 7, loss = 0.60522550\n",
      "Iteration 22, loss = 0.08229762\n",
      "Iteration 14, loss = 0.24900813\n",
      "Iteration 5, loss = 0.60542271\n",
      "Iteration 12, loss = 0.60518890\n",
      "Iteration 23, loss = 0.08652786\n",
      "Iteration 13, loss = 0.31829582\n",
      "Iteration 13, loss = 0.60502665\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), solver=sgd; total time= 1.7min\n",
      "Iteration 11, loss = 0.32750575\n",
      "Iteration 8, loss = 0.60524641\n",
      "Iteration 11, loss = 0.60521452\n",
      "Iteration 3, loss = 0.53671315\n",
      "Iteration 6, loss = 0.60536842\n",
      "Iteration 12, loss = 0.60519823\n",
      "Iteration 7, loss = 0.60539203\n",
      "Iteration 15, loss = 0.33312533\n",
      "Iteration 23, loss = 0.07887165\n",
      "Iteration 8, loss = 0.44927197\n",
      "Iteration 14, loss = 0.29972774\n",
      "Iteration 21, loss = 0.09150220\n",
      "Iteration 15, loss = 0.23596732\n",
      "Iteration 13, loss = 0.60523667\n",
      "Iteration 23, loss = 0.06886760\n",
      "Iteration 13, loss = 0.60518681\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n",
      "Iteration 24, loss = 0.06929151\n",
      "Iteration 4, loss = 0.51371857\n",
      "Iteration 15, loss = 0.28323810\n",
      "Iteration 12, loss = 0.30489366\n",
      "Iteration 1, loss = 0.60792763\n",
      "Iteration 9, loss = 0.42823694\n",
      "Iteration 9, loss = 0.60523669\n",
      "Iteration 16, loss = 0.31700810\n",
      "Iteration 14, loss = 0.60518998\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), solver=sgd; total time= 1.4min\n",
      "Iteration 8, loss = 0.60545157\n",
      "Iteration 16, loss = 0.26883542\n",
      "Iteration 24, loss = 0.08301119\n",
      "Iteration 10, loss = 0.60519353\n",
      "Iteration 10, loss = 0.40763937\n",
      "Iteration 16, loss = 0.22474714\n",
      "Iteration 22, loss = 0.08723941\n",
      "Iteration 9, loss = 0.60538340\n",
      "Iteration 13, loss = 0.28546331\n",
      "Iteration 11, loss = 0.60519120\n",
      "Iteration 1, loss = 0.65319140\n",
      "Iteration 24, loss = 0.07582309\n",
      "Iteration 17, loss = 0.25596662\n",
      "Iteration 17, loss = 0.30228282\n",
      "Iteration 11, loss = 0.38704523\n",
      "Iteration 25, loss = 0.06682272\n",
      "Iteration 17, loss = 0.21489766\n",
      "Iteration 24, loss = 0.06634492\n",
      "Iteration 2, loss = 0.56437172\n",
      "Iteration 12, loss = 0.60515759\n",
      "Iteration 14, loss = 0.26858659\n",
      "Iteration 18, loss = 0.24467452\n",
      "Iteration 5, loss = 0.49213386\n",
      "Iteration 12, loss = 0.36725383\n",
      "Iteration 18, loss = 0.20608323\n",
      "Iteration 10, loss = 0.60536408\n",
      "Iteration 23, loss = 0.08347576\n",
      "Iteration 25, loss = 0.07985798\n",
      "Iteration 1, loss = 0.59312397\n",
      "Iteration 2, loss = 0.56646662\n",
      "Iteration 18, loss = 0.28878981\n",
      "Iteration 11, loss = 0.60530520\n",
      "Iteration 13, loss = 0.60520645\n",
      "Iteration 19, loss = 0.23458427\n",
      "Iteration 25, loss = 0.07305205\n",
      "Iteration 13, loss = 0.34836505\n",
      "Iteration 25, loss = 0.06406922\n",
      "Iteration 3, loss = 0.53623457\n",
      "Iteration 19, loss = 0.19827033\n",
      "Iteration 19, loss = 0.27677278\n",
      "Iteration 24, loss = 0.08016642\n",
      "Iteration 2, loss = 0.56156863\n",
      "Iteration 12, loss = 0.60550224\n",
      "Iteration 15, loss = 0.25392652\n",
      "Iteration 14, loss = 0.33073014\n",
      "Iteration 26, loss = 0.06463345\n",
      "Iteration 20, loss = 0.19129813\n",
      "Iteration 16, loss = 0.24127333\n",
      "Iteration 20, loss = 0.26572386\n",
      "Iteration 15, loss = 0.31445074\n",
      "Iteration 13, loss = 0.60529648\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), solver=sgd; total time= 1.8min\n",
      "Iteration 14, loss = 0.60518323\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), solver=sgd; total time= 1.9min\n",
      "Iteration 20, loss = 0.22558466\n",
      "Iteration 6, loss = 0.46932209\n",
      "Iteration 3, loss = 0.53286068\n",
      "Iteration 4, loss = 0.51133743\n",
      "Iteration 16, loss = 0.29967835\n",
      "Iteration 26, loss = 0.07059715\n",
      "Iteration 3, loss = 0.54063417\n",
      "Iteration 1, loss = 0.68507882\n",
      "Iteration 26, loss = 0.06199083\n",
      "Iteration 21, loss = 0.25575200\n",
      "Iteration 25, loss = 0.07712495\n",
      "Iteration 26, loss = 0.07700702\n",
      "Iteration 21, loss = 0.18503084\n",
      "Iteration 21, loss = 0.21743346\n",
      "Iteration 17, loss = 0.28632671\n",
      "Iteration 17, loss = 0.23010237\n",
      "Iteration 27, loss = 0.06262970\n",
      "Iteration 5, loss = 0.48584317\n",
      "Iteration 7, loss = 0.44660427\n",
      "Iteration 2, loss = 0.60856898\n",
      "Iteration 22, loss = 0.24661766\n",
      "Iteration 22, loss = 0.21017192\n",
      "Iteration 18, loss = 0.27404884\n",
      "Iteration 1, loss = 0.60405522\n",
      "Iteration 4, loss = 0.50404702\n",
      "Iteration 4, loss = 0.51867536\n",
      "Iteration 27, loss = 0.06009888\n",
      "Iteration 22, loss = 0.17931417\n",
      "Iteration 18, loss = 0.22036502\n",
      "Iteration 19, loss = 0.26319377\n",
      "Iteration 23, loss = 0.23841105\n",
      "Iteration 3, loss = 0.60710457\n",
      "Iteration 26, loss = 0.07445417\n",
      "Iteration 28, loss = 0.06080642\n",
      "Iteration 23, loss = 0.20351647\n",
      "Iteration 27, loss = 0.07445631\n",
      "Iteration 27, loss = 0.06832759\n",
      "Iteration 6, loss = 0.45810144\n",
      "Iteration 2, loss = 0.56538553\n",
      "Iteration 19, loss = 0.21179561\n",
      "Iteration 20, loss = 0.25312297\n",
      "Iteration 5, loss = 0.49787966\n",
      "Iteration 23, loss = 0.17420776\n",
      "Iteration 24, loss = 0.19772346\n",
      "Iteration 8, loss = 0.42380564\n",
      "Iteration 20, loss = 0.20409990\n",
      "Iteration 24, loss = 0.23082383\n",
      "Iteration 5, loss = 0.47223059\n",
      "Iteration 24, loss = 0.16929194\n",
      "Iteration 21, loss = 0.24411852\n",
      "Iteration 6, loss = 0.47608118\n",
      "Iteration 25, loss = 0.19188696\n",
      "Iteration 25, loss = 0.22390091\n",
      "Iteration 21, loss = 0.19716881\n",
      "Iteration 4, loss = 0.60688204\n",
      "Iteration 7, loss = 0.42980478\n",
      "Iteration 27, loss = 0.07201665\n",
      "Iteration 3, loss = 0.53835290\n",
      "Iteration 28, loss = 0.05837014\n",
      "Iteration 26, loss = 0.21752022\n",
      "Iteration 6, loss = 0.43857481\n",
      "Iteration 22, loss = 0.19106890\n",
      "Iteration 28, loss = 0.07208816\n",
      "Iteration 9, loss = 0.40027196\n",
      "Iteration 22, loss = 0.23597808\n",
      "Iteration 7, loss = 0.45377798\n",
      "Iteration 28, loss = 0.06630909\n",
      "Iteration 5, loss = 0.60606168\n",
      "Iteration 26, loss = 0.18686895\n",
      "Iteration 29, loss = 0.05912466\n",
      "Iteration 27, loss = 0.21168416\n",
      "Iteration 8, loss = 0.40106042\n",
      "Iteration 23, loss = 0.18510708\n",
      "Iteration 4, loss = 0.51312205\n",
      "Iteration 6, loss = 0.60601648\n",
      "Iteration 29, loss = 0.05678322\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.22823739\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(200,), solver=adam; total time= 9.1min\n",
      "Iteration 25, loss = 0.16496786\n",
      "Iteration 10, loss = 0.37718359\n",
      "Iteration 8, loss = 0.43058219\n",
      "Iteration 28, loss = 0.20614089\n",
      "Iteration 28, loss = 0.06982308\n",
      "Iteration 7, loss = 0.40426027\n",
      "Iteration 27, loss = 0.18208871\n",
      "Iteration 24, loss = 0.17998611\n",
      "Iteration 1, loss = 0.67736915\n",
      "Iteration 7, loss = 0.60606540\n",
      "Iteration 30, loss = 0.05757408\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.22154983\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(200,), solver=adam; total time= 9.5min\n",
      "Iteration 9, loss = 0.37306748\n",
      "Iteration 25, loss = 0.17510911\n",
      "Iteration 29, loss = 0.06992957\n",
      "Iteration 29, loss = 0.06438647\n",
      "Iteration 26, loss = 0.16090075\n",
      "Iteration 29, loss = 0.20107167\n",
      "Iteration 8, loss = 0.37040801\n",
      "Iteration 11, loss = 0.35588701\n",
      "Iteration 25, loss = 0.21489359\n",
      "Iteration 9, loss = 0.40733121\n",
      "Iteration 28, loss = 0.17775769\n",
      "Iteration 2, loss = 0.60868995\n",
      "Iteration 1, loss = 0.69818985\n",
      "Iteration 27, loss = 0.15707137\n",
      "Iteration 29, loss = 0.06775333\n",
      "Iteration 30, loss = 0.19632425\n",
      "Iteration 5, loss = 0.48706630\n",
      "Iteration 8, loss = 0.60600632\n",
      "Iteration 26, loss = 0.17067047\n",
      "Iteration 2, loss = 0.60742796\n",
      "Iteration 28, loss = 0.15362319Iteration 26, loss = 0.20914784\n",
      "\n",
      "Iteration 29, loss = 0.17377426\n",
      "Iteration 9, loss = 0.33956154\n",
      "Iteration 31, loss = 0.19188214\n",
      "Iteration 10, loss = 0.34722413\n",
      "Iteration 30, loss = 0.06794948\n",
      "Iteration 30, loss = 0.06266914\n",
      "Iteration 3, loss = 0.60701826\n",
      "Iteration 29, loss = 0.15024506\n",
      "Iteration 6, loss = 0.45804369\n",
      "Iteration 30, loss = 0.16981276\n",
      "Iteration 30, loss = 0.06588856\n",
      "Iteration 10, loss = 0.38421637\n",
      "Iteration 9, loss = 0.60595815\n",
      "Iteration 10, loss = 0.31240058\n",
      "Iteration 3, loss = 0.60799039\n",
      "Iteration 27, loss = 0.16656502\n",
      "Iteration 4, loss = 0.60641871\n",
      "Iteration 12, loss = 0.33598144\n",
      "Iteration 30, loss = 0.14717256\n",
      "Iteration 27, loss = 0.20370804\n",
      "Iteration 10, loss = 0.60592579\n",
      "Iteration 31, loss = 0.16624297\n",
      "Iteration 4, loss = 0.60629003\n",
      "Iteration 28, loss = 0.16269302\n",
      "Iteration 31, loss = 0.14425591\n",
      "Iteration 11, loss = 0.28931279\n",
      "Iteration 32, loss = 0.18778057\n",
      "Iteration 11, loss = 0.32434478\n",
      "Iteration 13, loss = 0.31793386\n",
      "Iteration 5, loss = 0.60634434\n",
      "Iteration 32, loss = 0.16282402\n",
      "Iteration 11, loss = 0.60591717\n",
      "Iteration 29, loss = 0.15913636\n",
      "Iteration 33, loss = 0.18387785\n",
      "Iteration 28, loss = 0.19875057\n",
      "Iteration 31, loss = 0.06414744\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(200,), solver=adam; total time= 9.7min\n",
      "Iteration 31, loss = 0.06613107\n",
      "Iteration 5, loss = 0.60610715\n",
      "Iteration 7, loss = 0.42815807\n",
      "Iteration 6, loss = 0.60631845\n",
      "Iteration 33, loss = 0.15968857\n",
      "Iteration 12, loss = 0.60592742\n",
      "Iteration 11, loss = 0.36210245\n",
      "Iteration 32, loss = 0.14145837\n",
      "Iteration 34, loss = 0.18015332\n",
      "Iteration 29, loss = 0.19403481\n",
      "Iteration 12, loss = 0.30331975\n",
      "Iteration 31, loss = 0.06105465\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(200,), solver=adam; total time=10.4min\n",
      "Iteration 14, loss = 0.30147392\n",
      "Iteration 7, loss = 0.60627305\n",
      "Iteration 30, loss = 0.15577055\n",
      "Iteration 13, loss = 0.60599165\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), solver=sgd; total time= 2.1min\n",
      "Iteration 1, loss = 0.61495081\n",
      "Iteration 34, loss = 0.15670459\n",
      "Iteration 12, loss = 0.26981862\n",
      "Iteration 6, loss = 0.60647379\n",
      "Iteration 35, loss = 0.17668222\n",
      "Iteration 31, loss = 0.15260045\n",
      "Iteration 32, loss = 0.06444861\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.001, hidden_layer_sizes=(200,), solver=adam; total time=10.5min\n",
      "Iteration 2, loss = 0.60710885\n",
      "Iteration 1, loss = 0.69221374\n",
      "Iteration 8, loss = 0.60625446\n",
      "Iteration 30, loss = 0.18954316\n",
      "Iteration 13, loss = 0.28541448\n",
      "Iteration 33, loss = 0.13880737\n",
      "Iteration 12, loss = 0.34171883\n",
      "Iteration 36, loss = 0.17337436\n",
      "Iteration 7, loss = 0.60613029\n",
      "Iteration 3, loss = 0.60631125\n",
      "Iteration 32, loss = 0.14962721\n",
      "Iteration 31, loss = 0.18543491\n",
      "Iteration 35, loss = 0.15384630\n",
      "Iteration 14, loss = 0.26947236\n",
      "Iteration 34, loss = 0.13641967\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), solver=adam; total time= 5.7min\n",
      "Iteration 1, loss = 0.63591411\n",
      "Iteration 2, loss = 0.60954254\n",
      "Iteration 8, loss = 0.60612652\n",
      "Iteration 8, loss = 0.39779089\n",
      "Iteration 13, loss = 0.25324901\n",
      "Iteration 4, loss = 0.60628771\n",
      "Iteration 37, loss = 0.17026862\n",
      "Iteration 9, loss = 0.60625703\n",
      "Iteration 32, loss = 0.18156467\n",
      "Iteration 13, loss = 0.32316151\n",
      "Iteration 3, loss = 0.60757410\n",
      "Iteration 15, loss = 0.28707003\n",
      "Iteration 9, loss = 0.60614466\n",
      "Iteration 10, loss = 0.60619653\n",
      "Iteration 5, loss = 0.60619570\n",
      "Iteration 33, loss = 0.17798422\n",
      "Iteration 36, loss = 0.15108526\n",
      "Iteration 1, loss = 0.60042976\n",
      "Iteration 15, loss = 0.25573956\n",
      "Iteration 1, loss = 0.66794623\n",
      "Iteration 33, loss = 0.14687769\n",
      "Iteration 14, loss = 0.30568799\n",
      "Iteration 2, loss = 0.56493132\n",
      "Iteration 38, loss = 0.16730187\n",
      "Iteration 14, loss = 0.23936437\n",
      "Iteration 4, loss = 0.60652716\n",
      "Iteration 37, loss = 0.14855948\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), solver=adam; total time= 6.0min\n",
      "Iteration 34, loss = 0.17446533\n",
      "Iteration 10, loss = 0.60609213\n",
      "Iteration 34, loss = 0.14415302\n",
      "Iteration 6, loss = 0.60615242\n",
      "Iteration 11, loss = 0.60616353\n",
      "Iteration 9, loss = 0.36916347\n",
      "Iteration 15, loss = 0.29061190\n",
      "Iteration 39, loss = 0.16447898\n",
      "Iteration 16, loss = 0.24383365\n",
      "Iteration 3, loss = 0.53265154\n",
      "Iteration 35, loss = 0.17123519\n",
      "Iteration 16, loss = 0.27371874\n",
      "Iteration 12, loss = 0.60618109\n",
      "Iteration 7, loss = 0.60616447\n",
      "Iteration 2, loss = 0.56533394\n",
      "Iteration 15, loss = 0.22711892\n",
      "Iteration 10, loss = 0.34212209\n",
      "Iteration 1, loss = 0.60206939\n",
      "Iteration 35, loss = 0.14177466\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), solver=adam; total time= 6.1min\n",
      "Iteration 40, loss = 0.16186228\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), solver=adam; total time= 6.2min\n",
      "Iteration 11, loss = 0.60618548\n",
      "Iteration 2, loss = 0.55687447\n",
      "Iteration 36, loss = 0.16813583\n",
      "Iteration 13, loss = 0.60631290\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), solver=sgd; total time= 2.0min\n",
      "Iteration 16, loss = 0.27720613\n",
      "Iteration 8, loss = 0.60619829\n",
      "Iteration 5, loss = 0.60621087\n",
      "Iteration 11, loss = 0.31863857\n",
      "Iteration 17, loss = 0.26183521\n",
      "Iteration 37, loss = 0.16515280\n",
      "Iteration 12, loss = 0.60626450\n",
      "Iteration 16, loss = 0.21645805\n",
      "Iteration 1, loss = 0.63246220\n",
      "Iteration 17, loss = 0.23301502\n",
      "Iteration 2, loss = 0.55861930\n",
      "Iteration 6, loss = 0.60622875\n",
      "Iteration 3, loss = 0.52711485\n",
      "Iteration 17, loss = 0.26511745\n",
      "Iteration 9, loss = 0.60621358\n",
      "Iteration 1, loss = 0.62490998\n",
      "Iteration 3, loss = 0.53756563\n",
      "Iteration 4, loss = 0.50844237\n",
      "Iteration 38, loss = 0.16232341\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65247476\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100,), solver=adam; total time= 6.1min\n",
      "Iteration 17, loss = 0.20751388\n",
      "Iteration 18, loss = 0.25131518\n",
      "Iteration 7, loss = 0.60624043\n",
      "Iteration 13, loss = 0.60604681\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), solver=sgd; total time= 2.3min\n",
      "Iteration 2, loss = 0.60794148\n",
      "Iteration 10, loss = 0.60607093\n",
      "Iteration 12, loss = 0.29746673\n",
      "Iteration 3, loss = 0.52858610\n",
      "Iteration 18, loss = 0.25432807\n",
      "Iteration 5, loss = 0.48495295\n",
      "Iteration 18, loss = 0.22355981\n",
      "Iteration 1, loss = 0.65577296\n",
      "Iteration 4, loss = 0.51449541\n",
      "Iteration 11, loss = 0.60610851\n",
      "Iteration 18, loss = 0.19927236\n",
      "Iteration 4, loss = 0.49899321\n",
      "Iteration 2, loss = 0.60811123\n",
      "Iteration 8, loss = 0.60612705\n",
      "Iteration 1, loss = 0.65259352\n",
      "Iteration 2, loss = 0.56146708\n",
      "Iteration 19, loss = 0.24461246\n",
      "Iteration 4, loss = 0.50134811\n",
      "Iteration 3, loss = 0.60747765\n",
      "Iteration 13, loss = 0.27969110\n",
      "Iteration 19, loss = 0.19222624\n",
      "Iteration 19, loss = 0.24182751\n",
      "Iteration 6, loss = 0.46047472\n",
      "Iteration 12, loss = 0.60612191\n",
      "Iteration 2, loss = 0.60852480\n",
      "Iteration 3, loss = 0.60817677\n",
      "Iteration 19, loss = 0.21517505\n",
      "Iteration 20, loss = 0.23593238\n",
      "Iteration 14, loss = 0.26403916\n",
      "Iteration 9, loss = 0.60612660\n",
      "Iteration 5, loss = 0.49272033\n",
      "Iteration 3, loss = 0.53234128\n",
      "Iteration 5, loss = 0.47155485\n",
      "Iteration 2, loss = 0.60836442\n",
      "Iteration 4, loss = 0.60727600\n",
      "Iteration 13, loss = 0.60610037\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), solver=sgd; total time= 2.1min\n",
      "Iteration 20, loss = 0.18555362\n",
      "Iteration 10, loss = 0.60612805\n",
      "Iteration 20, loss = 0.23318373\n",
      "Iteration 3, loss = 0.60808374\n",
      "Iteration 20, loss = 0.20764217\n",
      "Iteration 15, loss = 0.25062263\n",
      "Iteration 4, loss = 0.60715758\n",
      "Iteration 7, loss = 0.43509203\n",
      "Iteration 6, loss = 0.47115250\n",
      "Iteration 5, loss = 0.46808650\n",
      "Iteration 21, loss = 0.22787088\n",
      "Iteration 21, loss = 0.17982623\n",
      "Iteration 3, loss = 0.60790094\n",
      "Iteration 11, loss = 0.60619736\n",
      "Iteration 5, loss = 0.60693130\n",
      "Iteration 21, loss = 0.20093027\n",
      "Iteration 16, loss = 0.23888545\n",
      "Iteration 4, loss = 0.60711755\n",
      "Iteration 21, loss = 0.22566392\n",
      "Iteration 1, loss = 0.65984449\n",
      "Iteration 22, loss = 0.22074100\n",
      "Iteration 6, loss = 0.43963568\n",
      "Iteration 22, loss = 0.17433771\n",
      "Iteration 4, loss = 0.60724550\n",
      "Iteration 5, loss = 0.60714108\n",
      "Iteration 12, loss = 0.60613046\n",
      "Iteration 5, loss = 0.60734058\n",
      "Iteration 23, loss = 0.21418652\n",
      "Iteration 4, loss = 0.50671684\n",
      "Iteration 22, loss = 0.21842908\n",
      "Iteration 22, loss = 0.19470244\n",
      "Iteration 8, loss = 0.40980755\n",
      "Iteration 5, loss = 0.60715829\n",
      "Iteration 17, loss = 0.22858599\n",
      "Iteration 6, loss = 0.43634027\n",
      "Iteration 2, loss = 0.60921365\n",
      "Iteration 7, loss = 0.44831188\n",
      "Iteration 13, loss = 0.60613013\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), solver=sgd; total time= 2.5min\n",
      "Iteration 6, loss = 0.60690926\n",
      "Iteration 1, loss = 0.61039445\n",
      "Iteration 6, loss = 0.60702427\n",
      "Iteration 23, loss = 0.21208324\n",
      "Iteration 6, loss = 0.60708452\n",
      "Iteration 24, loss = 0.20809702\n",
      "Iteration 18, loss = 0.21938980\n",
      "Iteration 3, loss = 0.60803459\n",
      "Iteration 5, loss = 0.48138443\n",
      "Iteration 7, loss = 0.40835739\n",
      "Iteration 23, loss = 0.18889703\n",
      "Iteration 8, loss = 0.42534252\n",
      "Iteration 2, loss = 0.47320832\n",
      "Iteration 9, loss = 0.38464772\n",
      "Iteration 23, loss = 0.16960023\n",
      "Iteration 6, loss = 0.60717984\n",
      "Iteration 7, loss = 0.60710023\n",
      "Iteration 25, loss = 0.20247086\n",
      "Iteration 24, loss = 0.20603810\n",
      "Iteration 3, loss = 0.34609096\n",
      "Iteration 4, loss = 0.24843413\n",
      "Iteration 7, loss = 0.60716049\n",
      "Iteration 24, loss = 0.18380705\n",
      "Iteration 19, loss = 0.21116752\n",
      "Iteration 10, loss = 0.36083553\n",
      "Iteration 4, loss = 0.60700708\n",
      "Iteration 7, loss = 0.60716585\n",
      "Iteration 5, loss = 0.17955185\n",
      "Iteration 8, loss = 0.60708155\n",
      "Iteration 6, loss = 0.45550546\n",
      "Iteration 24, loss = 0.16502702\n",
      "Iteration 25, loss = 0.20055614\n",
      "Iteration 6, loss = 0.13250433\n",
      "Iteration 7, loss = 0.40380219\n",
      "Iteration 9, loss = 0.40224029\n",
      "Iteration 7, loss = 0.60701135\n",
      "Iteration 8, loss = 0.37660286\n",
      "Iteration 8, loss = 0.60714723\n",
      "Iteration 25, loss = 0.17899926\n",
      "Iteration 26, loss = 0.19735951\n",
      "Iteration 7, loss = 0.10064703\n",
      "Iteration 9, loss = 0.60699182\n",
      "Iteration 5, loss = 0.60728223\n",
      "Iteration 8, loss = 0.07835446\n",
      "Iteration 8, loss = 0.60714373\n",
      "Iteration 9, loss = 0.60713981\n",
      "Iteration 26, loss = 0.17452783\n",
      "Iteration 6, loss = 0.60716723\n",
      "Iteration 9, loss = 0.06250046\n",
      "Iteration 8, loss = 0.60723862\n",
      "Iteration 20, loss = 0.20405450\n",
      "Iteration 10, loss = 0.38006825\n",
      "Iteration 10, loss = 0.60716208\n",
      "Iteration 10, loss = 0.05094534\n",
      "Iteration 8, loss = 0.37234238\n",
      "Iteration 27, loss = 0.19249136\n",
      "Iteration 26, loss = 0.19543528\n",
      "Iteration 11, loss = 0.33871931\n",
      "Iteration 25, loss = 0.16085526\n",
      "Iteration 11, loss = 0.04232590\n",
      "Iteration 10, loss = 0.60712333\n",
      "Iteration 9, loss = 0.60685801\n",
      "Iteration 7, loss = 0.42832602\n",
      "Iteration 9, loss = 0.34778038\n",
      "Iteration 7, loss = 0.60713155\n",
      "Iteration 27, loss = 0.17039798\n",
      "Iteration 12, loss = 0.03562285\n",
      "Iteration 11, loss = 0.60698908\n",
      "Iteration 21, loss = 0.19731394\n",
      "Iteration 26, loss = 0.15709944\n",
      "Iteration 27, loss = 0.19072836\n",
      "Iteration 13, loss = 0.03048223\n",
      "Iteration 12, loss = 0.31867012\n",
      "Iteration 9, loss = 0.60708812\n",
      "Iteration 14, loss = 0.02630357\n",
      "Iteration 11, loss = 0.35889913\n",
      "Iteration 15, loss = 0.02300821\n",
      "Iteration 28, loss = 0.18805618\n",
      "Iteration 16, loss = 0.02023707\n",
      "Iteration 10, loss = 0.60681341\n",
      "Iteration 22, loss = 0.19132620\n",
      "Iteration 11, loss = 0.60711950\n",
      "Iteration 17, loss = 0.01802917\n",
      "Iteration 9, loss = 0.34328013\n",
      "Iteration 28, loss = 0.18636986\n",
      "Iteration 10, loss = 0.60725696\n",
      "Iteration 28, loss = 0.16650803\n",
      "Iteration 10, loss = 0.32215800\n",
      "Iteration 8, loss = 0.60700249\n",
      "Iteration 8, loss = 0.40145730\n",
      "Iteration 12, loss = 0.60698387\n",
      "Iteration 18, loss = 0.01609526\n",
      "Iteration 27, loss = 0.15340854\n",
      "Iteration 19, loss = 0.01451467\n",
      "Iteration 11, loss = 0.60690079\n",
      "Iteration 12, loss = 0.33898238\n",
      "Iteration 11, loss = 0.60704503\n",
      "Iteration 12, loss = 0.60711036\n",
      "Iteration 29, loss = 0.18224333\n",
      "Iteration 13, loss = 0.30074372\n",
      "Iteration 10, loss = 0.31740653\n",
      "Iteration 20, loss = 0.01313843\n",
      "Iteration 23, loss = 0.18579295\n",
      "Iteration 11, loss = 0.29981008\n",
      "Iteration 29, loss = 0.18382936\n",
      "Iteration 12, loss = 0.60689496\n",
      "Iteration 21, loss = 0.01196616\n",
      "Iteration 9, loss = 0.60702511\n",
      "Iteration 28, loss = 0.14996758\n",
      "Iteration 12, loss = 0.60703055\n",
      "Iteration 13, loss = 0.60702489\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(200,), solver=sgd; total time= 2.7min\n",
      "Iteration 13, loss = 0.60703010\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(200,), solver=sgd; total time= 3.1min\n",
      "Iteration 9, loss = 0.37502447\n",
      "Iteration 22, loss = 0.01096235\n",
      "Iteration 13, loss = 0.32128331\n",
      "Iteration 29, loss = 0.16291678\n",
      "Iteration 1, loss = 0.65895368\n",
      "Iteration 1, loss = 0.60009766\n",
      "Iteration 2, loss = 0.55671735\n",
      "Iteration 23, loss = 0.01008692\n",
      "Iteration 30, loss = 0.17991846\n",
      "Iteration 30, loss = 0.17852010\n",
      "Iteration 3, loss = 0.43929869\n",
      "Iteration 13, loss = 0.60679508\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(200,), solver=sgd; total time= 3.3min\n",
      "Iteration 13, loss = 0.60702461\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 24, loss = 0.00931599\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), solver=adam; total time= 1.8min\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(200,), solver=sgd; total time= 3.0min\n",
      "Iteration 2, loss = 0.46576770\n",
      "Iteration 29, loss = 0.14683013\n",
      "Iteration 14, loss = 0.28492070\n",
      "Iteration 1, loss = 0.63102754\n",
      "Iteration 12, loss = 0.28092082\n",
      "Iteration 24, loss = 0.18083795\n",
      "Iteration 3, loss = 0.34227632\n",
      "Iteration 11, loss = 0.29505949\n",
      "Iteration 1, loss = 0.74712314\n",
      "Iteration 4, loss = 0.32984024\n",
      "Iteration 14, loss = 0.30479938\n",
      "Iteration 10, loss = 0.60704613\n",
      "Iteration 4, loss = 0.24617293\n",
      "Iteration 31, loss = 0.17618920\n",
      "Iteration 1, loss = 0.62399398\n",
      "Iteration 5, loss = 0.24175941\n",
      "Iteration 5, loss = 0.17755369\n",
      "Iteration 10, loss = 0.35064121Iteration 2, loss = 0.49783050\n",
      "\n",
      "Iteration 2, loss = 0.73073270\n",
      "Iteration 6, loss = 0.17708436\n",
      "Iteration 31, loss = 0.17473714\n",
      "Iteration 6, loss = 0.13082024\n",
      "Iteration 30, loss = 0.15956629\n",
      "Iteration 7, loss = 0.13199496\n",
      "Iteration 3, loss = 0.36839211\n",
      "Iteration 7, loss = 0.09875781\n",
      "Iteration 13, loss = 0.26458315\n",
      "Iteration 8, loss = 0.10082194\n",
      "Iteration 3, loss = 0.71378808\n",
      "Iteration 8, loss = 0.07674112\n",
      "Iteration 2, loss = 0.48629550\n",
      "Iteration 25, loss = 0.17599059\n",
      "Iteration 9, loss = 0.06094705\n",
      "Iteration 9, loss = 0.07900407\n",
      "Iteration 11, loss = 0.60699309\n",
      "Iteration 4, loss = 0.26553718\n",
      "Iteration 3, loss = 0.36002719\n",
      "Iteration 4, loss = 0.69830702\n",
      "Iteration 30, loss = 0.14385994\n",
      "Iteration 10, loss = 0.04957071\n",
      "Iteration 15, loss = 0.29041543\n",
      "Iteration 11, loss = 0.04101045\n",
      "Iteration 4, loss = 0.26035355\n",
      "Iteration 32, loss = 0.17128506\n",
      "Iteration 12, loss = 0.27607675\n",
      "Iteration 32, loss = 0.17266849\n",
      "Iteration 15, loss = 0.27093652\n",
      "Iteration 12, loss = 0.03449103\n",
      "Iteration 5, loss = 0.68475999\n",
      "Iteration 5, loss = 0.19175586\n",
      "Iteration 5, loss = 0.18867114\n",
      "Iteration 11, loss = 0.32884898\n",
      "Iteration 12, loss = 0.60707825\n",
      "Iteration 10, loss = 0.06345126\n",
      "Iteration 26, loss = 0.17163205\n",
      "Iteration 13, loss = 0.02938625\n",
      "Iteration 6, loss = 0.13920301\n",
      "Iteration 31, loss = 0.15638971\n",
      "Iteration 14, loss = 0.02539206\n",
      "Iteration 14, loss = 0.25053359\n",
      "Iteration 7, loss = 0.10541092\n",
      "Iteration 6, loss = 0.67285510\n",
      "Iteration 6, loss = 0.14108169\n",
      "Iteration 8, loss = 0.08184404\n",
      "Iteration 11, loss = 0.05198778\n",
      "Iteration 15, loss = 0.02212757\n",
      "Iteration 12, loss = 0.04335263\n",
      "Iteration 9, loss = 0.06514979\n",
      "Iteration 33, loss = 0.16817634\n",
      "Iteration 16, loss = 0.25878293\n",
      "Iteration 7, loss = 0.66231753\n",
      "Iteration 7, loss = 0.10660785\n",
      "Iteration 13, loss = 0.03669197\n",
      "Iteration 10, loss = 0.05297834\n",
      "Iteration 13, loss = 0.60729074\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(200,), solver=sgd; total time= 3.0min\n",
      "Iteration 14, loss = 0.03150429\n",
      "Iteration 33, loss = 0.16950293\n",
      "Iteration 16, loss = 0.01947750\n",
      "Iteration 8, loss = 0.08282844\n",
      "Iteration 27, loss = 0.16776145\n",
      "Iteration 8, loss = 0.65353579\n",
      "Iteration 1, loss = 0.67103932\n",
      "Iteration 11, loss = 0.04390169\n",
      "Iteration 15, loss = 0.02736684\n",
      "Iteration 2, loss = 0.66448452\n",
      "Iteration 9, loss = 0.06586755\n",
      "Iteration 31, loss = 0.14100777\n",
      "Iteration 12, loss = 0.03690750\n",
      "Iteration 16, loss = 0.02397222\n",
      "Iteration 9, loss = 0.64552856\n",
      "Iteration 16, loss = 0.27699207\n",
      "Iteration 10, loss = 0.05358335\n",
      "Iteration 3, loss = 0.65727226\n",
      "Iteration 32, loss = 0.15319726\n",
      "Iteration 17, loss = 0.01729651\n",
      "Iteration 13, loss = 0.25982256\n",
      "Iteration 4, loss = 0.65046416\n",
      "Iteration 17, loss = 0.02122964\n",
      "Iteration 10, loss = 0.63879081\n",
      "Iteration 12, loss = 0.30924266\n",
      "Iteration 13, loss = 0.03147763\n",
      "Iteration 5, loss = 0.64447437\n",
      "Iteration 11, loss = 0.63275922\n",
      "Iteration 18, loss = 0.01546864\n",
      "Iteration 18, loss = 0.01894776\n",
      "Iteration 17, loss = 0.24772543\n",
      "Iteration 34, loss = 0.16501650\n",
      "Iteration 6, loss = 0.63883092\n",
      "Iteration 11, loss = 0.04427142\n",
      "Iteration 7, loss = 0.63403168\n",
      "Iteration 12, loss = 0.62757107\n",
      "Iteration 19, loss = 0.01391386\n",
      "Iteration 12, loss = 0.03730918\n",
      "Iteration 8, loss = 0.62932143\n",
      "Iteration 19, loss = 0.01701091\n",
      "Iteration 14, loss = 0.02719685\n",
      "Iteration 13, loss = 0.62289058\n",
      "Iteration 20, loss = 0.01259451\n",
      "Iteration 13, loss = 0.03182246\n",
      "Iteration 28, loss = 0.16390707\n",
      "Iteration 15, loss = 0.23845591\n",
      "Iteration 34, loss = 0.16630982\n",
      "Iteration 9, loss = 0.62535694\n",
      "Iteration 14, loss = 0.61886696\n",
      "Iteration 14, loss = 0.02747052\n",
      "Iteration 35, loss = 0.16212248\n",
      "Iteration 33, loss = 0.15044095\n",
      "Iteration 15, loss = 0.02368762\n",
      "Iteration 32, loss = 0.13846966\n",
      "Iteration 21, loss = 0.01146763\n",
      "Iteration 10, loss = 0.62189065\n",
      "Iteration 15, loss = 0.02395509\n",
      "Iteration 20, loss = 0.01538414\n",
      "Iteration 15, loss = 0.61530125\n",
      "Iteration 11, loss = 0.61868254\n",
      "Iteration 16, loss = 0.02087724\n",
      "Iteration 22, loss = 0.01049229\n",
      "Iteration 16, loss = 0.02109994\n",
      "Iteration 17, loss = 0.01851732\n",
      "Iteration 12, loss = 0.61599618\n",
      "Iteration 21, loss = 0.01398630\n",
      "Iteration 18, loss = 0.23802652\n",
      "Iteration 16, loss = 0.61217562\n",
      "Iteration 35, loss = 0.16335459\n",
      "Iteration 29, loss = 0.16029679\n",
      "Iteration 17, loss = 0.01871694\n",
      "Iteration 14, loss = 0.24583336\n",
      "Iteration 18, loss = 0.01656822\n",
      "Iteration 17, loss = 0.60949214\n",
      "Iteration 13, loss = 0.61354953\n",
      "Iteration 17, loss = 0.26557111\n",
      "Iteration 23, loss = 0.00964478\n",
      "Iteration 18, loss = 0.01672962\n",
      "Iteration 18, loss = 0.60723817\n",
      "Iteration 22, loss = 0.01280207\n",
      "Iteration 33, loss = 0.13594357\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), solver=adam; total time= 8.3min\n",
      "Iteration 14, loss = 0.61142857\n",
      "Iteration 19, loss = 0.01490914\n",
      "Iteration 19, loss = 0.60518286\n",
      "Iteration 13, loss = 0.29163108\n",
      "Iteration 15, loss = 0.60950850\n",
      "Iteration 19, loss = 0.01505614\n",
      "Iteration 1, loss = 0.65488707\n",
      "Iteration 20, loss = 0.60339687\n",
      "Iteration 34, loss = 0.14791936\n",
      "Iteration 20, loss = 0.01350079\n",
      "Iteration 23, loss = 0.01174722\n",
      "Iteration 21, loss = 0.60192855\n",
      "Iteration 2, loss = 0.64895614\n",
      "Iteration 20, loss = 0.01364124\n",
      "Iteration 36, loss = 0.15940317\n",
      "Iteration 24, loss = 0.00891301\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), solver=adam; total time= 1.5min\n",
      "Iteration 16, loss = 0.60780845\n",
      "Iteration 21, loss = 0.01241671\n",
      "Iteration 22, loss = 0.60058965\n",
      "Iteration 1, loss = 0.71003337\n",
      "Iteration 21, loss = 0.01228951\n",
      "Iteration 3, loss = 0.64241894\n",
      "Iteration 30, loss = 0.15708031\n",
      "Iteration 22, loss = 0.01136168\n",
      "Iteration 17, loss = 0.60635660\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), solver=sgd; total time=  42.0s\n",
      "Iteration 2, loss = 0.69927738\n",
      "Iteration 22, loss = 0.01125002\n",
      "Iteration 4, loss = 0.63657175\n",
      "Iteration 1, loss = 0.64757131\n",
      "Iteration 23, loss = 0.01045064\n",
      "Iteration 24, loss = 0.01084458\n",
      "Iteration 18, loss = 0.25527412\n",
      "Iteration 3, loss = 0.68688455\n",
      "Iteration 2, loss = 0.64267221\n",
      "Iteration 23, loss = 0.01034305\n",
      "Iteration 5, loss = 0.63137661\n",
      "Iteration 24, loss = 0.00964565\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), solver=adam; total time= 1.4min\n",
      "Iteration 36, loss = 0.16044668\n",
      "Iteration 23, loss = 0.59935808\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.67475578\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), solver=sgd; total time= 1.5min\n",
      "Iteration 25, loss = 0.01005484\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), solver=adam; total time= 1.6min\n",
      "Iteration 24, loss = 0.00954364\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), solver=adam; total time= 1.5min\n",
      "Iteration 5, loss = 0.66356804\n",
      "Iteration 16, loss = 0.22787595\n",
      "Iteration 35, loss = 0.14516103\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), solver=adam; total time= 8.8min\n",
      "Iteration 6, loss = 0.65354883\n",
      "Iteration 3, loss = 0.63752916\n",
      "Iteration 6, loss = 0.62662371\n",
      "Iteration 1, loss = 0.61028091\n",
      "Iteration 31, loss = 0.15401540\n",
      "Iteration 7, loss = 0.64472171\n",
      "Iteration 19, loss = 0.22909759\n",
      "Iteration 4, loss = 0.63305712\n",
      "Iteration 1, loss = 0.61629991\n",
      "Iteration 8, loss = 0.63731305\n",
      "Iteration 7, loss = 0.62267370\n",
      "Iteration 15, loss = 0.23375136\n",
      "Iteration 9, loss = 0.63083166\n",
      "Iteration 5, loss = 0.62910997\n",
      "Iteration 37, loss = 0.15774118\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), solver=adam; total time= 8.6min\n",
      "Iteration 6, loss = 0.62575979\n",
      "Iteration 10, loss = 0.62536657\n",
      "Iteration 8, loss = 0.61910170\n",
      "Iteration 1, loss = 0.63543128\n",
      "Iteration 11, loss = 0.62076135\n",
      "Iteration 19, loss = 0.24586068\n",
      "Iteration 7, loss = 0.62266711\n",
      "Iteration 9, loss = 0.61595680\n",
      "Iteration 12, loss = 0.61684509\n",
      "Iteration 1, loss = 0.61775275\n",
      "Iteration 37, loss = 0.15672512\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), solver=adam; total time= 9.6min\n",
      "Iteration 10, loss = 0.61326251\n",
      "Iteration 13, loss = 0.61338921\n",
      "Iteration 32, loss = 0.15100833\n",
      "Iteration 8, loss = 0.61996168\n",
      "Iteration 11, loss = 0.61097342\n",
      "Iteration 14, loss = 0.27626219\n",
      "Iteration 17, loss = 0.21857786\n",
      "Iteration 1, loss = 0.64121167\n",
      "Iteration 1, loss = 0.69866825\n",
      "Iteration 2, loss = 0.43809444\n",
      "Iteration 12, loss = 0.60882119\n",
      "Iteration 2, loss = 0.42261353\n",
      "Iteration 13, loss = 0.60702220\n",
      "Iteration 14, loss = 0.61057096\n",
      "Iteration 2, loss = 0.43155843\n",
      "Iteration 14, loss = 0.60538552\n",
      "Iteration 9, loss = 0.61742670\n",
      "Iteration 2, loss = 0.68855199\n",
      "Iteration 15, loss = 0.60814611\n",
      "Iteration 2, loss = 0.49067970\n",
      "Iteration 15, loss = 0.60400857\n",
      "Iteration 20, loss = 0.22133261\n",
      "Iteration 16, loss = 0.60600459\n",
      "Iteration 10, loss = 0.61520298\n",
      "Iteration 1, loss = 0.71430773\n",
      "Iteration 16, loss = 0.60273766\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), solver=sgd; total time=  40.3s\n",
      "Iteration 17, loss = 0.60425370\n",
      "Iteration 2, loss = 0.42539287\n",
      "Iteration 3, loss = 0.27319344\n",
      "Iteration 11, loss = 0.61327710\n",
      "Iteration 3, loss = 0.27476513\n",
      "Iteration 16, loss = 0.22342947\n",
      "Iteration 3, loss = 0.67733694\n",
      "Iteration 2, loss = 0.70188146\n",
      "Iteration 12, loss = 0.61154380\n",
      "Iteration 1, loss = 0.70360691\n",
      "Iteration 3, loss = 0.27050396\n",
      "Iteration 18, loss = 0.60257616\n",
      "Iteration 33, loss = 0.14827003\n",
      "Iteration 4, loss = 0.17088749\n",
      "Iteration 3, loss = 0.33545918\n",
      "Iteration 18, loss = 0.21037690\n",
      "Iteration 19, loss = 0.60122153\n",
      "Iteration 20, loss = 0.23738561\n",
      "Iteration 13, loss = 0.61002805\n",
      "Iteration 20, loss = 0.59992678\n",
      "Iteration 21, loss = 0.21433504\n",
      "Iteration 4, loss = 0.16996670\n",
      "Iteration 21, loss = 0.59880203\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), solver=sgd; total time=  47.3s\n",
      "Iteration 2, loss = 0.69332217\n",
      "Iteration 5, loss = 0.11095614\n",
      "Iteration 3, loss = 0.68888006\n",
      "Iteration 4, loss = 0.17170490\n",
      "Iteration 4, loss = 0.66746704\n",
      "Iteration 14, loss = 0.60857503\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(50,), solver=sgd; total time=  47.3s\n",
      "Iteration 15, loss = 0.26279606\n",
      "Iteration 3, loss = 0.26881726\n",
      "Iteration 4, loss = 0.21611730\n",
      "Iteration 4, loss = 0.67724819\n",
      "Iteration 34, loss = 0.14581287\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(150,), solver=adam; total time= 8.6min\n",
      "Iteration 5, loss = 0.65874865\n",
      "Iteration 5, loss = 0.11042939\n",
      "Iteration 1, loss = 0.71382967\n",
      "Iteration 3, loss = 0.68202090\n",
      "Iteration 1, loss = 0.72930000\n",
      "Iteration 22, loss = 0.20787021\n",
      "Iteration 5, loss = 0.11260342\n",
      "Iteration 6, loss = 0.65067078\n",
      "Iteration 5, loss = 0.13940257\n",
      "Iteration 4, loss = 0.16890695\n",
      "Iteration 17, loss = 0.21393979\n",
      "Iteration 21, loss = 0.22946637\n",
      "Iteration 6, loss = 0.07647357\n",
      "Iteration 6, loss = 0.07618348\n",
      "Iteration 5, loss = 0.66689163\n",
      "Iteration 1, loss = 0.63443505\n",
      "Iteration 5, loss = 0.11033663\n",
      "Iteration 4, loss = 0.67131093\n",
      "Iteration 7, loss = 0.05535716\n",
      "Iteration 6, loss = 0.07789684\n",
      "Iteration 6, loss = 0.65791788\n",
      "Iteration 2, loss = 0.71525008\n",
      "Iteration 7, loss = 0.05558913\n",
      "Iteration 2, loss = 0.70049950\n",
      "Iteration 19, loss = 0.20276297\n",
      "Iteration 7, loss = 0.64382758\n",
      "Iteration 16, loss = 0.25101589\n",
      "Iteration 6, loss = 0.07625472\n",
      "Iteration 7, loss = 0.64983243\n",
      "Iteration 3, loss = 0.68644329\n",
      "Iteration 8, loss = 0.04226449\n",
      "Iteration 8, loss = 0.63763034\n",
      "Iteration 2, loss = 0.42589685\n",
      "Iteration 3, loss = 0.70007614\n",
      "Iteration 6, loss = 0.09368377\n",
      "Iteration 5, loss = 0.66140620\n",
      "Iteration 8, loss = 0.04205072\n",
      "Iteration 9, loss = 0.63214992\n",
      "Iteration 8, loss = 0.64287114\n",
      "Iteration 7, loss = 0.05550370\n",
      "Iteration 4, loss = 0.68603612\n",
      "Iteration 23, loss = 0.20172502\n",
      "Iteration 4, loss = 0.67398700\n",
      "Iteration 17, loss = 0.24065833\n",
      "Iteration 9, loss = 0.03323613\n",
      "Iteration 7, loss = 0.06649359\n",
      "Iteration 9, loss = 0.63683777\n",
      "Iteration 6, loss = 0.65270467\n",
      "Iteration 5, loss = 0.67360098\n",
      "Iteration 20, loss = 0.19619978\n",
      "Iteration 10, loss = 0.62737903\n",
      "Iteration 8, loss = 0.04206874\n",
      "Iteration 3, loss = 0.24826618\n",
      "Iteration 22, loss = 0.22258916\n",
      "Iteration 6, loss = 0.66215246\n",
      "Iteration 7, loss = 0.05683521\n",
      "Iteration 11, loss = 0.62329058\n",
      "Iteration 18, loss = 0.20587601\n",
      "Iteration 9, loss = 0.03309886\n",
      "Iteration 10, loss = 0.63134179\n",
      "Iteration 7, loss = 0.65242218\n",
      "Iteration 24, loss = 0.19633379\n",
      "Iteration 8, loss = 0.04928953\n",
      "Iteration 5, loss = 0.66306650\n",
      "Iteration 18, loss = 0.23126486\n",
      "Iteration 4, loss = 0.14106776\n",
      "Iteration 12, loss = 0.61958159\n",
      "Iteration 8, loss = 0.64406592\n",
      "Iteration 8, loss = 0.04298692\n",
      "Iteration 7, loss = 0.64494595\n",
      "Iteration 11, loss = 0.62667657\n",
      "Iteration 10, loss = 0.02679011\n",
      "Iteration 10, loss = 0.02686250\n",
      "Iteration 9, loss = 0.03817355\n",
      "Iteration 6, loss = 0.65349766\n",
      "Iteration 13, loss = 0.61650837\n",
      "Iteration 8, loss = 0.63835031\n",
      "Iteration 9, loss = 0.03310684\n",
      "Iteration 21, loss = 0.19015979\n",
      "Iteration 25, loss = 0.19130158\n",
      "Iteration 23, loss = 0.21615202\n",
      "Iteration 7, loss = 0.64487066\n",
      "Iteration 19, loss = 0.22290385\n",
      "Iteration 14, loss = 0.61372891\n",
      "Iteration 19, loss = 0.19856461\n",
      "Iteration 9, loss = 0.63258870\n",
      "Iteration 9, loss = 0.03382472\n",
      "Iteration 10, loss = 0.02673853\n",
      "Iteration 11, loss = 0.02210551\n",
      "Iteration 5, loss = 0.08575513\n",
      "Iteration 9, loss = 0.63680683\n",
      "Iteration 10, loss = 0.03043333\n",
      "Iteration 8, loss = 0.63754252\n",
      "Iteration 11, loss = 0.02218314\n",
      "Iteration 12, loss = 0.62261592\n",
      "Iteration 10, loss = 0.62769922\n",
      "Iteration 26, loss = 0.18666588\n",
      "Iteration 12, loss = 0.01859880\n",
      "Iteration 15, loss = 0.61129040\n",
      "Iteration 10, loss = 0.02727561\n",
      "Iteration 10, loss = 0.63058357\n",
      "Iteration 9, loss = 0.63101058\n",
      "Iteration 11, loss = 0.02210243\n",
      "Iteration 22, loss = 0.18451319\n",
      "Iteration 12, loss = 0.01866978\n",
      "Iteration 13, loss = 0.61896438\n",
      "Iteration 20, loss = 0.21533894\n",
      "Iteration 24, loss = 0.21018125\n",
      "Iteration 11, loss = 0.02488890\n",
      "Iteration 10, loss = 0.62533875\n",
      "Iteration 11, loss = 0.62336402\n",
      "Iteration 11, loss = 0.62535238\n",
      "Iteration 13, loss = 0.01598146\n",
      "Iteration 14, loss = 0.61588159\n",
      "Iteration 13, loss = 0.01589922\n",
      "Iteration 11, loss = 0.62074961\n",
      "Iteration 12, loss = 0.02077173\n",
      "Iteration 12, loss = 0.61964088\n",
      "Iteration 12, loss = 0.62085389\n",
      "Iteration 12, loss = 0.01854972\n",
      "Iteration 16, loss = 0.60922477\n",
      "Iteration 20, loss = 0.19194104\n",
      "Iteration 11, loss = 0.02251973\n",
      "Iteration 14, loss = 0.01383838\n",
      "Iteration 12, loss = 0.61674342\n",
      "Iteration 15, loss = 0.61312189\n",
      "Iteration 13, loss = 0.61695563\n",
      "Iteration 6, loss = 0.05677653\n",
      "Iteration 14, loss = 0.01379529\n",
      "Iteration 27, loss = 0.18213710\n",
      "Iteration 13, loss = 0.01589692\n",
      "Iteration 25, loss = 0.20476129\n",
      "Iteration 21, loss = 0.20846236\n",
      "Iteration 16, loss = 0.61079927\n",
      "Iteration 13, loss = 0.61636936\n",
      "Iteration 17, loss = 0.60732765\n",
      "Iteration 15, loss = 0.01212159\n",
      "Iteration 14, loss = 0.61362757\n",
      "Iteration 23, loss = 0.17950910\n",
      "Iteration 13, loss = 0.01764416\n",
      "Iteration 17, loss = 0.60868740\n",
      "Iteration 18, loss = 0.60567598\n",
      "Iteration 14, loss = 0.61346200\n",
      "Iteration 13, loss = 0.61343239\n",
      "Iteration 16, loss = 0.01071499\n",
      "Iteration 14, loss = 0.01373569\n",
      "Iteration 21, loss = 0.18616191\n",
      "Iteration 14, loss = 0.01518210\n",
      "Iteration 15, loss = 0.01207164\n",
      "Iteration 28, loss = 0.17815039\n",
      "Iteration 12, loss = 0.01894625\n",
      "Iteration 7, loss = 0.04043834\n",
      "Iteration 18, loss = 0.60682102\n",
      "Iteration 15, loss = 0.61082478\n",
      "Iteration 15, loss = 0.61093353\n",
      "Iteration 19, loss = 0.60420342\n",
      "Iteration 17, loss = 0.00959408\n",
      "Iteration 15, loss = 0.01325796\n",
      "Iteration 26, loss = 0.19969595\n",
      "Iteration 22, loss = 0.20234058\n",
      "Iteration 14, loss = 0.61051297\n",
      "Iteration 13, loss = 0.01617862\n",
      "Iteration 16, loss = 0.60832259\n",
      "Iteration 15, loss = 0.01204193\n",
      "Iteration 16, loss = 0.01069198\n",
      "Iteration 19, loss = 0.60518174\n",
      "Iteration 24, loss = 0.17491987\n",
      "Iteration 15, loss = 0.60807336\n",
      "Iteration 17, loss = 0.60633588\n",
      "Iteration 16, loss = 0.01168563\n",
      "Iteration 8, loss = 0.03039490\n",
      "Iteration 16, loss = 0.60875352\n",
      "Iteration 18, loss = 0.00860819\n",
      "Iteration 20, loss = 0.60292052\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), solver=sgd; total time= 2.5min\n",
      "Iteration 20, loss = 0.60375390\n",
      "Iteration 17, loss = 0.00954970\n",
      "Iteration 16, loss = 0.01065620\n",
      "Iteration 18, loss = 0.60438450\n",
      "Iteration 22, loss = 0.18040191\n",
      "Iteration 27, loss = 0.19480960\n",
      "Iteration 17, loss = 0.01040845\n",
      "Iteration 23, loss = 0.19655139\n",
      "Iteration 16, loss = 0.60597543\n",
      "Iteration 21, loss = 0.60248858\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), solver=sgd; total time= 2.5min\n",
      "Iteration 17, loss = 0.60683943\n",
      "Iteration 18, loss = 0.00859336\n",
      "Iteration 25, loss = 0.17076626\n",
      "Iteration 17, loss = 0.60414790\n",
      "Iteration 9, loss = 0.02376597\n",
      "Iteration 18, loss = 0.60516647\n",
      "Iteration 17, loss = 0.00952272\n",
      "Iteration 29, loss = 0.17443007\n",
      "Iteration 14, loss = 0.01401521\n",
      "Iteration 19, loss = 0.60278579\n",
      "Iteration 1, loss = 0.63477852\n",
      "Iteration 19, loss = 0.60369340\n",
      "Iteration 18, loss = 0.60253384\n",
      "Iteration 18, loss = 0.00935212\n",
      "Iteration 23, loss = 0.17546696\n",
      "Iteration 19, loss = 0.00780763\n",
      "Iteration 10, loss = 0.01918161\n",
      "Iteration 18, loss = 0.00856511\n",
      "Iteration 24, loss = 0.19134729\n",
      "Iteration 19, loss = 0.60110270\n",
      "Iteration 20, loss = 0.60240170\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), solver=sgd; total time= 2.6min\n",
      "Iteration 2, loss = 0.41785907\n",
      "Iteration 20, loss = 0.60146951\n",
      "Iteration 19, loss = 0.00779812\n",
      "Iteration 19, loss = 0.00779582\n",
      "Iteration 20, loss = 0.00712168\n",
      "Iteration 1, loss = 0.62242005\n",
      "Iteration 21, loss = 0.60019542\n",
      "Iteration 20, loss = 0.59993811\n",
      "Iteration 30, loss = 0.17077365\n",
      "Iteration 11, loss = 0.01588544\n",
      "Iteration 20, loss = 0.00712179\n",
      "Iteration 25, loss = 0.18659675\n",
      "Iteration 21, loss = 0.00653526\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), solver=adam; total time= 3.2min\n",
      "Iteration 22, loss = 0.59913345\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), solver=sgd; total time= 2.6min\n",
      "Iteration 19, loss = 0.00846125\n",
      "Iteration 28, loss = 0.19046044\n",
      "Iteration 15, loss = 0.01228003\n",
      "Iteration 1, loss = 0.61287156\n",
      "Iteration 2, loss = 0.40828907\n",
      "Iteration 20, loss = 0.00771062\n",
      "Iteration 12, loss = 0.01339079\n",
      "Iteration 20, loss = 0.00709406\n",
      "Iteration 21, loss = 0.00652864\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), solver=adam; total time= 3.3min\n",
      "Iteration 1, loss = 0.61126954\n",
      "Iteration 3, loss = 0.24142622\n",
      "Iteration 21, loss = 0.59882865\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), solver=sgd; total time= 2.7min\n",
      "Iteration 1, loss = 0.73270528\n",
      "Iteration 16, loss = 0.01086673\n",
      "Iteration 26, loss = 0.18192132\n",
      "Iteration 21, loss = 0.00652009\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.00706941\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), solver=adam; total time= 3.5min\n",
      "Iteration 2, loss = 0.39797875\n",
      "Iteration 1, loss = 0.67150928\n",
      "Iteration 31, loss = 0.16727837\n",
      "Iteration 26, loss = 0.16660918\n",
      "Iteration 17, loss = 0.00971186\n",
      "Iteration 1, loss = 0.67869069\n",
      "Iteration 3, loss = 0.23645626\n",
      "Iteration 13, loss = 0.01149682\n",
      "Iteration 22, loss = 0.00651845\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), solver=adam; total time= 3.6min\n",
      "Iteration 2, loss = 0.71858637\n",
      "Iteration 24, loss = 0.17082504\n",
      "Iteration 2, loss = 0.39332275\n",
      "Iteration 29, loss = 0.18632412\n",
      "Iteration 18, loss = 0.00874038\n",
      "Iteration 1, loss = 0.71114919\n",
      "Iteration 2, loss = 0.67009454\n",
      "Iteration 2, loss = 0.66390945\n",
      "Iteration 4, loss = 0.13694998\n",
      "Iteration 32, loss = 0.16402373\n",
      "Iteration 27, loss = 0.16274192\n",
      "Iteration 3, loss = 0.22894527\n",
      "Iteration 14, loss = 0.00998277\n",
      "Iteration 1, loss = 0.67220114\n",
      "Iteration 27, loss = 0.17766422\n",
      "Iteration 3, loss = 0.70333427\n",
      "Iteration 2, loss = 0.69959381\n",
      "Iteration 3, loss = 0.66086074\n",
      "Iteration 3, loss = 0.22673018\n",
      "Iteration 19, loss = 0.00794306\n",
      "Iteration 3, loss = 0.65564385\n",
      "Iteration 4, loss = 0.13429522\n",
      "Iteration 4, loss = 0.65289816\n",
      "Iteration 30, loss = 0.18242907\n",
      "Iteration 20, loss = 0.00723714\n",
      "Iteration 25, loss = 0.16654837\n",
      "Iteration 3, loss = 0.68722536\n",
      "Iteration 5, loss = 0.08290088\n",
      "Iteration 4, loss = 0.13072963\n",
      "Iteration 4, loss = 0.68999851\n",
      "Iteration 4, loss = 0.64831592\n",
      "Iteration 33, loss = 0.16118937\n",
      "Iteration 4, loss = 0.13079045\n",
      "Iteration 2, loss = 0.66413146\n",
      "Iteration 21, loss = 0.00664036\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 28, loss = 0.15918014\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(100,), solver=adam; total time= 4.1min\n",
      "Iteration 5, loss = 0.64564341\n",
      "Iteration 5, loss = 0.08139061\n",
      "Iteration 5, loss = 0.67785851\n",
      "Iteration 26, loss = 0.16258267\n",
      "Iteration 15, loss = 0.00880596\n",
      "Iteration 4, loss = 0.67625780\n",
      "Iteration 5, loss = 0.08007240\n",
      "Iteration 28, loss = 0.17364982\n",
      "Iteration 3, loss = 0.65567855\n",
      "Iteration 6, loss = 0.05482990\n",
      "Iteration 5, loss = 0.64183900\n",
      "Iteration 31, loss = 0.17879487\n",
      "Iteration 1, loss = 0.59029610\n",
      "Iteration 6, loss = 0.66747396\n",
      "Iteration 29, loss = 0.15585161\n",
      "Iteration 16, loss = 0.00783476\n",
      "Iteration 6, loss = 0.05367301\n",
      "Iteration 5, loss = 0.66630073\n",
      "Iteration 34, loss = 0.15814102\n",
      "Iteration 6, loss = 0.05319549\n",
      "Iteration 5, loss = 0.08096021\n",
      "Iteration 6, loss = 0.63958223\n",
      "Iteration 4, loss = 0.64776682\n",
      "Iteration 6, loss = 0.63625435\n",
      "Iteration 7, loss = 0.65816015\n",
      "Iteration 17, loss = 0.00701915\n",
      "Iteration 6, loss = 0.65759569\n",
      "Iteration 7, loss = 0.63391812\n",
      "Iteration 29, loss = 0.17011004\n",
      "Iteration 6, loss = 0.05391282\n",
      "Iteration 8, loss = 0.64997715\n",
      "Iteration 5, loss = 0.64117005\n",
      "Iteration 7, loss = 0.03786447\n",
      "Iteration 7, loss = 0.03786865\n",
      "Iteration 7, loss = 0.63128621\n",
      "Iteration 32, loss = 0.17528761\n",
      "Iteration 27, loss = 0.15894278\n",
      "Iteration 30, loss = 0.15277008\n",
      "Iteration 7, loss = 0.03876643\n",
      "Iteration 7, loss = 0.64990313\n",
      "Iteration 35, loss = 0.15535798\n",
      "Iteration 2, loss = 0.35362451\n",
      "Iteration 8, loss = 0.62926807\n",
      "Iteration 8, loss = 0.02853813\n",
      "Iteration 18, loss = 0.00636916\n",
      "Iteration 9, loss = 0.64270180\n",
      "Iteration 7, loss = 0.03850243\n",
      "Iteration 8, loss = 0.64301906\n",
      "Iteration 6, loss = 0.63532020\n",
      "Iteration 8, loss = 0.02822387\n",
      "Iteration 8, loss = 0.62696721\n",
      "Iteration 28, loss = 0.15540873\n",
      "Iteration 8, loss = 0.02905006\n",
      "Iteration 9, loss = 0.02236359\n",
      "Iteration 9, loss = 0.62502728\n",
      "Iteration 33, loss = 0.17189882\n",
      "Iteration 36, loss = 0.15275444\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(200,), solver=adam; total time=10.6min\n",
      "Iteration 7, loss = 0.63037411\n",
      "Iteration 9, loss = 0.02192935\n",
      "Iteration 9, loss = 0.02261490\n",
      "Iteration 9, loss = 0.62313885\n",
      "Iteration 10, loss = 0.63640988\n",
      "Iteration 19, loss = 0.00580957\n",
      "Iteration 9, loss = 0.63708049\n",
      "Iteration 8, loss = 0.02891730\n",
      "Iteration 31, loss = 0.14987386\n",
      "Iteration 3, loss = 0.19008895\n",
      "Iteration 30, loss = 0.16660525\n",
      "Iteration 10, loss = 0.01819779\n",
      "Iteration 10, loss = 0.62129499\n",
      "Iteration 8, loss = 0.62581991\n",
      "Iteration 29, loss = 0.15232047\n",
      "Iteration 10, loss = 0.01805803\n",
      "Iteration 11, loss = 0.63080164\n",
      "Iteration 10, loss = 0.01762348\n",
      "Iteration 10, loss = 0.63180288\n",
      "Iteration 1, loss = 0.60947273\n",
      "Iteration 10, loss = 0.61972794\n",
      "Iteration 11, loss = 0.61808788\n",
      "Iteration 9, loss = 0.02258237\n",
      "Iteration 34, loss = 0.16882895\n",
      "Iteration 9, loss = 0.62194923\n",
      "Iteration 12, loss = 0.62600296\n",
      "Iteration 11, loss = 0.62719514\n",
      "Iteration 11, loss = 0.61675392\n",
      "Iteration 12, loss = 0.61518233\n",
      "Iteration 20, loss = 0.00533792\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 4, loss = 0.10380333\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), solver=adam; total time= 4.6min\n",
      "Iteration 11, loss = 0.01492931\n",
      "Iteration 11, loss = 0.01450188\n",
      "Iteration 11, loss = 0.01500085\n",
      "Iteration 13, loss = 0.62168979\n",
      "Iteration 12, loss = 0.62307527\n",
      "Iteration 2, loss = 0.37023466\n",
      "Iteration 12, loss = 0.61419362\n",
      "Iteration 10, loss = 0.01823118\n",
      "Iteration 31, loss = 0.16329702\n",
      "Iteration 30, loss = 0.14930818\n",
      "Iteration 14, loss = 0.61804737\n",
      "Iteration 12, loss = 0.01219073\n",
      "Iteration 12, loss = 0.01262087\n",
      "Iteration 5, loss = 0.06205459\n",
      "Iteration 32, loss = 0.14714326\n",
      "Iteration 13, loss = 0.61969945\n",
      "Iteration 10, loss = 0.61855152\n",
      "Iteration 13, loss = 0.61280489\n",
      "Iteration 13, loss = 0.61190452\n",
      "Iteration 1, loss = 0.62634386\n",
      "Iteration 12, loss = 0.01259153\n",
      "Iteration 14, loss = 0.61653667\n",
      "Iteration 15, loss = 0.61485675\n",
      "Iteration 13, loss = 0.01045999\n",
      "Iteration 13, loss = 0.01083832\n",
      "Iteration 3, loss = 0.19378371\n",
      "Iteration 14, loss = 0.60988988\n",
      "Iteration 11, loss = 0.61554261\n",
      "Iteration 14, loss = 0.61054760\n",
      "Iteration 16, loss = 0.61202579\n",
      "Iteration 11, loss = 0.01506750\n",
      "Iteration 33, loss = 0.14438066\n",
      "Iteration 35, loss = 0.16585913\n",
      "Iteration 15, loss = 0.61380953\n",
      "Iteration 31, loss = 0.14630465\n",
      "Iteration 14, loss = 0.00908694\n",
      "Iteration 15, loss = 0.60806792\n",
      "Iteration 15, loss = 0.60867238\n",
      "Iteration 6, loss = 0.04034234\n",
      "Iteration 12, loss = 0.61292994\n",
      "Iteration 14, loss = 0.00940731\n",
      "Iteration 13, loss = 0.01079861\n",
      "Iteration 17, loss = 0.60959804\n",
      "Iteration 2, loss = 0.38449774\n",
      "Iteration 16, loss = 0.61150269\n",
      "Iteration 4, loss = 0.10336362\n",
      "Iteration 15, loss = 0.00800537\n",
      "Iteration 16, loss = 0.60692705\n",
      "Iteration 16, loss = 0.60652271\n",
      "Iteration 14, loss = 0.00940092\n",
      "Iteration 13, loss = 0.61062165\n",
      "Iteration 12, loss = 0.01268237\n",
      "Iteration 15, loss = 0.00828485\n",
      "Iteration 36, loss = 0.16299061\n",
      "Iteration 34, loss = 0.14186153\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(200,), solver=adam; total time=11.3min\n",
      "Iteration 17, loss = 0.60941042\n",
      "Iteration 32, loss = 0.16021791\n",
      "Iteration 18, loss = 0.60748150\n",
      "Iteration 17, loss = 0.60549517\n",
      "Iteration 32, loss = 0.14340727\n",
      "Iteration 15, loss = 0.00827852\n",
      "Iteration 7, loss = 0.02845532\n",
      "Iteration 18, loss = 0.60414269\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), solver=sgd; total time= 3.0min\n",
      "Iteration 19, loss = 0.60561238\n",
      "Iteration 14, loss = 0.60857569\n",
      "Iteration 16, loss = 0.00736871\n",
      "Iteration 3, loss = 0.20145852\n",
      "Iteration 17, loss = 0.60508428\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), solver=sgd; total time= 3.1min\n",
      "Iteration 16, loss = 0.00712158\n",
      "Iteration 18, loss = 0.60763761\n",
      "Iteration 16, loss = 0.00736941\n",
      "Iteration 13, loss = 0.01085770\n",
      "Iteration 5, loss = 0.06096914\n",
      "Iteration 37, loss = 0.16027807\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(200,), solver=adam; total time=12.1min\n",
      "Iteration 20, loss = 0.60398728\n",
      "Iteration 15, loss = 0.60686636\n",
      "Iteration 17, loss = 0.00640345\n",
      "Iteration 19, loss = 0.60591610\n",
      "Iteration 17, loss = 0.00662754\n",
      "Iteration 8, loss = 0.02122446\n",
      "Iteration 33, loss = 0.15739926\n",
      "Iteration 17, loss = 0.00661832\n",
      "Iteration 4, loss = 0.10563689\n",
      "Iteration 21, loss = 0.60247840\n",
      "Iteration 1, loss = 0.61092911\n",
      "Iteration 1, loss = 0.70045295\n",
      "Iteration 1, loss = 0.59079731\n",
      "Iteration 16, loss = 0.60526735\n",
      "Iteration 14, loss = 0.00945186\n",
      "Iteration 33, loss = 0.14075303\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(200,), solver=adam; total time=12.1min\n",
      "Iteration 20, loss = 0.60452830\n",
      "Iteration 18, loss = 0.00580523\n",
      "Iteration 1, loss = 0.65819272\n",
      "Iteration 18, loss = 0.00599710\n",
      "Iteration 6, loss = 0.03966102\n",
      "Iteration 18, loss = 0.00601086\n",
      "Iteration 17, loss = 0.60388125\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), solver=sgd; total time= 3.4min\n",
      "Iteration 2, loss = 0.68912363\n",
      "Iteration 22, loss = 0.60120349\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), solver=sgd; total time= 3.7min\n",
      "Iteration 21, loss = 0.60325552\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), solver=sgd; total time= 3.4min\n",
      "Iteration 9, loss = 0.01660825\n",
      "Iteration 15, loss = 0.00832913\n",
      "Iteration 2, loss = 0.36014528\n",
      "Iteration 19, loss = 0.00548728\n",
      "Iteration 2, loss = 0.65154559\n",
      "Iteration 19, loss = 0.00547142\n",
      "Iteration 1, loss = 0.69980299\n",
      "Iteration 1, loss = 0.65445136\n",
      "Iteration 2, loss = 0.34518307\n",
      "Iteration 1, loss = 0.66541542\n",
      "Iteration 19, loss = 0.00530180\n",
      "Iteration 3, loss = 0.67712804\n",
      "Iteration 5, loss = 0.06157024\n",
      "Iteration 2, loss = 0.50743783\n",
      "Iteration 34, loss = 0.15440229\n",
      "Iteration 7, loss = 0.02805509\n",
      "Iteration 3, loss = 0.37326850\n",
      "Iteration 1, loss = 0.67592327\n",
      "Iteration 3, loss = 0.64467876\n",
      "Iteration 4, loss = 0.27061387\n",
      "Iteration 20, loss = 0.00504661\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), solver=adam; total time= 4.2min\n",
      "Iteration 10, loss = 0.01342334\n",
      "Iteration 3, loss = 0.18119737\n",
      "Iteration 1, loss = 0.66365021\n",
      "Iteration 16, loss = 0.00740814\n",
      "Iteration 2, loss = 0.68893746\n",
      "Iteration 6, loss = 0.04003412\n",
      "Iteration 5, loss = 0.19782976\n",
      "Iteration 2, loss = 0.65831335\n",
      "Iteration 6, loss = 0.14768819\n",
      "Iteration 20, loss = 0.00487369\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.00503040\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), solver=adam; total time= 4.5min\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), solver=adam; total time= 4.7min\n",
      "Iteration 4, loss = 0.63834947\n",
      "Iteration 7, loss = 0.11336678\n",
      "Iteration 2, loss = 0.52252653\n",
      "Iteration 1, loss = 0.71072583\n",
      "Iteration 2, loss = 0.56344819\n",
      "Iteration 8, loss = 0.08941988\n",
      "Iteration 3, loss = 0.38829900\n",
      "Iteration 35, loss = 0.15177363\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=logistic, alpha=0.01, hidden_layer_sizes=(200,), solver=adam; total time=12.2min\n",
      "Iteration 2, loss = 0.66772770\n",
      "Iteration 4, loss = 0.66634460\n",
      "Iteration 1, loss = 0.61891551\n",
      "Iteration 4, loss = 0.28219410\n",
      "Iteration 3, loss = 0.41783495\n",
      "Iteration 9, loss = 0.07234971\n",
      "Iteration 3, loss = 0.65070579\n",
      "Iteration 1, loss = 0.60950587\n",
      "Iteration 8, loss = 0.02097224\n",
      "Iteration 10, loss = 0.05984158\n",
      "Iteration 3, loss = 0.18803590\n",
      "Iteration 2, loss = 0.49299054\n",
      "Iteration 3, loss = 0.67700399\n",
      "Iteration 5, loss = 0.20534155\n",
      "Iteration 4, loss = 0.29735853\n",
      "Iteration 2, loss = 0.47221474\n",
      "Iteration 11, loss = 0.05053478\n",
      "Iteration 17, loss = 0.00666140\n",
      "Iteration 11, loss = 0.01113356\n",
      "Iteration 3, loss = 0.36984674\n",
      "Iteration 6, loss = 0.15228138\n",
      "Iteration 7, loss = 0.02837820\n",
      "Iteration 12, loss = 0.04346763\n",
      "Iteration 3, loss = 0.34532699\n",
      "Iteration 5, loss = 0.21129139\n",
      "Iteration 4, loss = 0.27010588\n",
      "Iteration 4, loss = 0.09895149\n",
      "Iteration 13, loss = 0.03785170\n",
      "Iteration 4, loss = 0.24936768\n",
      "Iteration 5, loss = 0.63286370\n",
      "Iteration 6, loss = 0.15305721\n",
      "Iteration 3, loss = 0.65895440\n",
      "Iteration 4, loss = 0.64362745\n",
      "Iteration 5, loss = 0.18206692\n",
      "Iteration 7, loss = 0.11616550\n",
      "Iteration 14, loss = 0.03347646\n",
      "Iteration 7, loss = 0.11474909\n",
      "Iteration 5, loss = 0.19665844\n",
      "Iteration 5, loss = 0.65673418\n",
      "Iteration 6, loss = 0.13566491\n",
      "Iteration 8, loss = 0.08912841\n",
      "Iteration 4, loss = 0.10090168\n",
      "Iteration 9, loss = 0.07125025\n",
      "Iteration 8, loss = 0.09110739\n",
      "Iteration 15, loss = 0.02994509\n",
      "Iteration 4, loss = 0.66639523\n",
      "Iteration 6, loss = 0.14559464\n",
      "Iteration 18, loss = 0.00603916\n",
      "Iteration 7, loss = 0.10398450\n",
      "Iteration 5, loss = 0.63751658\n",
      "Iteration 16, loss = 0.02706390\n",
      "Iteration 8, loss = 0.02126444\n",
      "Iteration 9, loss = 0.01642966\n",
      "Iteration 4, loss = 0.65055934\n",
      "Iteration 17, loss = 0.02465828\n",
      "Iteration 7, loss = 0.11063509\n",
      "Iteration 6, loss = 0.62819384\n",
      "Iteration 10, loss = 0.05864739\n",
      "Iteration 18, loss = 0.02266126\n",
      "Iteration 9, loss = 0.07337440\n",
      "Iteration 5, loss = 0.05929795\n",
      "Iteration 8, loss = 0.08638112\n",
      "Iteration 11, loss = 0.04930992\n",
      "Iteration 6, loss = 0.64854363\n",
      "Iteration 8, loss = 0.08203378\n",
      "Iteration 9, loss = 0.06946802\n",
      "Iteration 12, loss = 0.00945200\n",
      "Iteration 12, loss = 0.04226519\n",
      "Iteration 19, loss = 0.02099292\n",
      "Iteration 10, loss = 0.06062181\n",
      "Iteration 6, loss = 0.63213301\n",
      "Iteration 10, loss = 0.05717624\n",
      "Iteration 5, loss = 0.05977046\n",
      "Iteration 13, loss = 0.03678594\n",
      "Iteration 7, loss = 0.62413156\n",
      "Iteration 9, loss = 0.01671400\n",
      "Iteration 11, loss = 0.05102878\n",
      "Iteration 10, loss = 0.01326600\n",
      "Iteration 20, loss = 0.01954977\n",
      "Iteration 5, loss = 0.65709960\n",
      "Iteration 9, loss = 0.06645049\n",
      "Iteration 12, loss = 0.04383368\n",
      "Iteration 14, loss = 0.03247432\n",
      "Iteration 21, loss = 0.01830991\n",
      "Iteration 13, loss = 0.03815265\n",
      "Iteration 11, loss = 0.04805530\n",
      "Iteration 10, loss = 0.05501547\n",
      "Iteration 5, loss = 0.64332965\n",
      "Iteration 19, loss = 0.00551419\n",
      "Iteration 14, loss = 0.03371595\n",
      "Iteration 6, loss = 0.03898763\n",
      "Iteration 11, loss = 0.04649487\n",
      "Iteration 22, loss = 0.01725071\n",
      "Iteration 12, loss = 0.04118836\n",
      "Iteration 15, loss = 0.03010013\n",
      "Iteration 15, loss = 0.02903337\n",
      "Iteration 8, loss = 0.62045967\n",
      "Iteration 6, loss = 0.03925161\n",
      "Iteration 23, loss = 0.01631920\n",
      "Iteration 12, loss = 0.03998469\n",
      "Iteration 13, loss = 0.00812974\n",
      "Iteration 24, loss = 0.01550655\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), solver=adam; total time= 1.4min\n",
      "Iteration 13, loss = 0.03588270\n",
      "Iteration 11, loss = 0.01099971\n",
      "Iteration 10, loss = 0.01355637\n",
      "Iteration 16, loss = 0.02718715\n",
      "Iteration 7, loss = 0.62740820\n",
      "Iteration 13, loss = 0.03490492\n",
      "Iteration 16, loss = 0.02620824\n",
      "Iteration 1, loss = 0.63473206\n",
      "Iteration 14, loss = 0.03172882\n",
      "Iteration 7, loss = 0.64108920\n",
      "Iteration 6, loss = 0.63687734\n",
      "Iteration 14, loss = 0.03090637\n",
      "Iteration 2, loss = 0.63148387\n",
      "Iteration 6, loss = 0.64882829\n",
      "Iteration 15, loss = 0.02830670\n",
      "Iteration 17, loss = 0.02477708\n",
      "Iteration 3, loss = 0.62791340\n",
      "Iteration 15, loss = 0.02768036\n",
      "Iteration 9, loss = 0.61735505\n",
      "Iteration 17, loss = 0.02390782\n",
      "Iteration 4, loss = 0.62457710\n",
      "Iteration 16, loss = 0.02560881\n",
      "Iteration 7, loss = 0.02767676\n",
      "Iteration 17, loss = 0.02335841\n",
      "Iteration 16, loss = 0.02502164\n",
      "Iteration 20, loss = 0.00507035\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(150,), solver=adam; total time= 5.3min\n",
      "Iteration 5, loss = 0.62158979\n",
      "Iteration 1, loss = 0.67480182\n",
      "Iteration 18, loss = 0.02200107\n",
      "Iteration 6, loss = 0.61897650\n",
      "Iteration 2, loss = 0.66704338\n",
      "Iteration 7, loss = 0.64152980\n",
      "Iteration 7, loss = 0.61664741\n",
      "Iteration 18, loss = 0.02273724\n",
      "Iteration 8, loss = 0.62336963\n",
      "Iteration 19, loss = 0.02032835\n",
      "Iteration 17, loss = 0.02283302\n",
      "Iteration 8, loss = 0.61452230\n",
      "Iteration 14, loss = 0.00713473\n",
      "Iteration 8, loss = 0.63486317\n",
      "Iteration 12, loss = 0.00932042\n",
      "Iteration 20, loss = 0.01895548\n",
      "Iteration 3, loss = 0.65895780\n",
      "Iteration 18, loss = 0.02146542\n",
      "Iteration 9, loss = 0.61266805\n",
      "Iteration 18, loss = 0.02101522\n",
      "Iteration 21, loss = 0.01775274\n",
      "Iteration 10, loss = 0.61102831\n",
      "Iteration 11, loss = 0.01127598\n",
      "Iteration 19, loss = 0.01947574\n",
      "Iteration 22, loss = 0.01673509\n",
      "Iteration 19, loss = 0.01989554\n",
      "Iteration 11, loss = 0.60954175\n",
      "Iteration 7, loss = 0.02790651\n",
      "Iteration 19, loss = 0.02101434\n",
      "Iteration 20, loss = 0.01814402\n",
      "Iteration 12, loss = 0.60819226\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), solver=sgd; total time=  25.3s\n",
      "Iteration 23, loss = 0.01582447\n",
      "Iteration 4, loss = 0.65169967\n",
      "Iteration 10, loss = 0.61452982\n",
      "Iteration 8, loss = 0.02077073\n",
      "Iteration 9, loss = 0.61970499\n",
      "Iteration 1, loss = 0.74535775\n",
      "Iteration 21, loss = 0.01701253\n",
      "Iteration 20, loss = 0.01855481\n",
      "Iteration 5, loss = 0.64519651\n",
      "Iteration 9, loss = 0.62921198\n",
      "Iteration 8, loss = 0.63529379\n",
      "Iteration 2, loss = 0.73168795\n",
      "Iteration 7, loss = 0.63131246\n",
      "Iteration 6, loss = 0.63936526\n",
      "Iteration 21, loss = 0.01741573\n",
      "Iteration 22, loss = 0.01602233\n",
      "Iteration 3, loss = 0.71732801\n",
      "Iteration 7, loss = 0.63413268\n",
      "Iteration 22, loss = 0.01641242\n",
      "Iteration 4, loss = 0.70394515\n",
      "Iteration 23, loss = 0.01516795\n",
      "Iteration 20, loss = 0.01957094\n",
      "Iteration 24, loss = 0.01503484\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), solver=adam; total time= 1.6min\n",
      "Iteration 8, loss = 0.62957812\n",
      "Iteration 15, loss = 0.00631926\n",
      "Iteration 5, loss = 0.69166634\n",
      "Iteration 24, loss = 0.01440510\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), solver=adam; total time= 1.5min\n",
      "Iteration 6, loss = 0.68057474\n",
      "Iteration 1, loss = 0.71898661\n",
      "Iteration 10, loss = 0.61661387\n",
      "Iteration 23, loss = 0.01555803\n",
      "Iteration 21, loss = 0.01832273\n",
      "Iteration 9, loss = 0.62561852\n",
      "Iteration 2, loss = 0.70759690\n",
      "Iteration 7, loss = 0.67053812\n",
      "Iteration 1, loss = 0.64975680\n",
      "Iteration 12, loss = 0.00956082\n",
      "Iteration 24, loss = 0.01479529\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), solver=adam; total time= 1.7min\n",
      "Iteration 8, loss = 0.66147039\n",
      "Iteration 3, loss = 0.69460237\n",
      "Iteration 10, loss = 0.62204104\n",
      "Iteration 9, loss = 0.65338259\n",
      "Iteration 11, loss = 0.61900102\n",
      "Iteration 8, loss = 0.02093389\n",
      "Iteration 13, loss = 0.00804812\n",
      "Iteration 10, loss = 0.64622589\n",
      "Iteration 2, loss = 0.64517672\n",
      "Iteration 22, loss = 0.01723426\n",
      "Iteration 10, loss = 0.62446045\n",
      "Iteration 11, loss = 0.61208486\n",
      "Iteration 12, loss = 0.61620598\n",
      "Iteration 11, loss = 0.63993264\n",
      "Iteration 4, loss = 0.68227762\n",
      "Iteration 8, loss = 0.62650370\n",
      "Iteration 9, loss = 0.01616307\n",
      "Iteration 13, loss = 0.61386555\n",
      "Iteration 12, loss = 0.63448717\n",
      "Iteration 5, loss = 0.67115693\n",
      "Iteration 1, loss = 0.67181053\n",
      "Iteration 6, loss = 0.66119320\n",
      "Iteration 13, loss = 0.62962162\n",
      "Iteration 3, loss = 0.64053439\n",
      "Iteration 23, loss = 0.01630194\n",
      "Iteration 14, loss = 0.61171287\n",
      "Iteration 16, loss = 0.00567240\n",
      "Iteration 7, loss = 0.65254093\n",
      "Iteration 24, loss = 0.01547114\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), solver=adam; total time= 2.0min\n",
      "Iteration 4, loss = 0.63614094\n",
      "Iteration 8, loss = 0.64504190\n",
      "Iteration 9, loss = 0.62966285\n",
      "Iteration 15, loss = 0.60987327\n",
      "Iteration 5, loss = 0.63242490\n",
      "Iteration 14, loss = 0.62533172\n",
      "Iteration 13, loss = 0.00827990\n",
      "Iteration 11, loss = 0.61390269\n",
      "Iteration 9, loss = 0.63840501\n",
      "Iteration 11, loss = 0.62032088\n",
      "Iteration 10, loss = 0.63245838\n",
      "Iteration 6, loss = 0.62898584\n",
      "Iteration 16, loss = 0.60820623\n",
      "Iteration 10, loss = 0.01304074\n",
      "Iteration 12, loss = 0.60987204\n",
      "Iteration 9, loss = 0.62222940\n",
      "Iteration 1, loss = 0.62337360\n",
      "Iteration 9, loss = 0.01644400\n",
      "Iteration 11, loss = 0.62744407\n",
      "Iteration 15, loss = 0.62165284\n",
      "Iteration 14, loss = 0.00704221\n",
      "Iteration 17, loss = 0.60674621\n",
      "Iteration 17, loss = 0.00512694\n",
      "Iteration 10, loss = 0.62499272\n",
      "Iteration 7, loss = 0.62579270\n",
      "Iteration 18, loss = 0.60544344\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), solver=sgd; total time=  54.6s\n",
      "Iteration 12, loss = 0.61149755\n",
      "Iteration 2, loss = 0.42699237\n",
      "Iteration 2, loss = 0.49915161\n",
      "Iteration 16, loss = 0.61839961\n",
      "Iteration 12, loss = 0.62297036\n",
      "Iteration 8, loss = 0.62298848\n",
      "Iteration 13, loss = 0.61912206\n",
      "Iteration 10, loss = 0.61855467\n",
      "Iteration 14, loss = 0.61575214\n",
      "Iteration 17, loss = 0.61552912\n",
      "Iteration 15, loss = 0.61293761\n",
      "Iteration 18, loss = 0.61298714\n",
      "Iteration 1, loss = 0.63038567\n",
      "Iteration 18, loss = 0.00468988\n",
      "Iteration 9, loss = 0.62046455\n",
      "Iteration 16, loss = 0.61036982\n",
      "Iteration 19, loss = 0.61077926\n",
      "Iteration 10, loss = 0.01325795\n",
      "Iteration 14, loss = 0.00725033\n",
      "Iteration 3, loss = 0.33318077\n",
      "Iteration 10, loss = 0.61822253\n",
      "Iteration 20, loss = 0.60881298\n",
      "Iteration 17, loss = 0.60814297\n",
      "Iteration 13, loss = 0.60803629\n",
      "Iteration 11, loss = 0.62084366\n",
      "Iteration 21, loss = 0.60706904\n",
      "Iteration 2, loss = 0.43196352\n",
      "Iteration 22, loss = 0.60548826\n",
      "Iteration 3, loss = 0.27019819\n",
      "Iteration 13, loss = 0.60938173\n",
      "Iteration 23, loss = 0.60411001\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), solver=sgd; total time=  56.9s\n",
      "Iteration 18, loss = 0.60617194\n",
      "Iteration 12, loss = 0.61670112\n",
      "Iteration 11, loss = 0.61617907\n",
      "Iteration 15, loss = 0.00623573\n",
      "Iteration 14, loss = 0.60639189\n",
      "Iteration 3, loss = 0.27404212\n",
      "Iteration 19, loss = 0.60449295\n",
      "Iteration 11, loss = 0.01075759\n",
      "Iteration 12, loss = 0.61436342\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), solver=sgd; total time=  52.3s\n",
      "Iteration 4, loss = 0.21206839\n",
      "Iteration 19, loss = 0.00430530\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 11, loss = 0.61531607\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(200,), solver=adam; total time= 5.8min\n",
      "Iteration 20, loss = 0.60299433\n",
      "Iteration 11, loss = 0.01101180\n",
      "Iteration 4, loss = 0.16936268\n",
      "Iteration 4, loss = 0.17312939\n",
      "Iteration 1, loss = 0.60748027\n",
      "Iteration 1, loss = 0.74668260\n",
      "Iteration 12, loss = 0.61707466\n",
      "Iteration 21, loss = 0.60163923\n",
      "Iteration 14, loss = 0.60750431\n",
      "Iteration 12, loss = 0.00909623\n",
      "Iteration 15, loss = 0.00643137\n",
      "Iteration 2, loss = 0.73018093\n",
      "Iteration 1, loss = 0.62287434\n",
      "Iteration 5, loss = 0.11127484\n",
      "Iteration 5, loss = 0.11398284\n",
      "Iteration 13, loss = 0.61371940\n",
      "Iteration 22, loss = 0.60051019\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(50,), solver=sgd; total time= 1.2min\n",
      "Iteration 12, loss = 0.61255435\n",
      "Iteration 2, loss = 0.43293294\n",
      "Iteration 16, loss = 0.00559677\n",
      "Iteration 5, loss = 0.13804856\n",
      "Iteration 3, loss = 0.71301607\n",
      "Iteration 2, loss = 0.42309096\n",
      "Iteration 15, loss = 0.60489365\n",
      "Iteration 6, loss = 0.07792213\n",
      "Iteration 3, loss = 0.27892760\n",
      "Iteration 4, loss = 0.69737551\n",
      "Iteration 14, loss = 0.61097159\n",
      "Iteration 13, loss = 0.61400913\n",
      "Iteration 6, loss = 0.07952680\n",
      "Iteration 3, loss = 0.27082259\n",
      "Iteration 1, loss = 0.65169110\n",
      "Iteration 5, loss = 0.68383388\n",
      "Iteration 15, loss = 0.60587764\n",
      "Iteration 16, loss = 0.00577244\n",
      "Iteration 13, loss = 0.00779950\n",
      "Iteration 12, loss = 0.00934789\n",
      "Iteration 6, loss = 0.09496024\n",
      "Iteration 7, loss = 0.05787169\n",
      "Iteration 2, loss = 0.64595360\n",
      "Iteration 6, loss = 0.67171362\n",
      "Iteration 7, loss = 0.05906730\n",
      "Iteration 15, loss = 0.60857841\n",
      "Iteration 13, loss = 0.61015296\n",
      "Iteration 4, loss = 0.17330468\n",
      "Iteration 4, loss = 0.17458286\n",
      "Iteration 14, loss = 0.61130525\n",
      "Iteration 16, loss = 0.60439816\n",
      "Iteration 8, loss = 0.04596959\n",
      "Iteration 7, loss = 0.06962298\n",
      "Iteration 17, loss = 0.00522964\n",
      "Iteration 16, loss = 0.60350136\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 3, loss = 0.63991310\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(200,), solver=sgd; total time= 4.0min\n",
      "Iteration 8, loss = 0.04516635\n",
      "Iteration 17, loss = 0.00505943\n",
      "Iteration 5, loss = 0.11570692\n",
      "Iteration 7, loss = 0.66123947\n",
      "Iteration 4, loss = 0.63451772\n",
      "Iteration 9, loss = 0.03721271\n",
      "Iteration 8, loss = 0.05371114\n",
      "Iteration 1, loss = 0.72784155\n",
      "Iteration 5, loss = 0.11432855\n",
      "Iteration 14, loss = 0.60806143\n",
      "Iteration 17, loss = 0.60308499\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(200,), solver=sgd; total time= 3.8min\n",
      "Iteration 16, loss = 0.60663274\n",
      "Iteration 13, loss = 0.00805804\n",
      "Iteration 8, loss = 0.65195433\n",
      "Iteration 14, loss = 0.00682026\n",
      "Iteration 9, loss = 0.03660657\n",
      "Iteration 2, loss = 0.71474191\n",
      "Iteration 5, loss = 0.62977676\n",
      "Iteration 9, loss = 0.04310134\n",
      "Iteration 18, loss = 0.00462028\n",
      "Iteration 9, loss = 0.64403982\n",
      "Iteration 6, loss = 0.07967175\n",
      "Iteration 10, loss = 0.03105072\n",
      "Iteration 1, loss = 0.68303260\n",
      "Iteration 17, loss = 0.60478620\n",
      "Iteration 3, loss = 0.70140755\n",
      "Iteration 15, loss = 0.60890556\n",
      "Iteration 15, loss = 0.60621040\n",
      "Iteration 6, loss = 0.08179585\n",
      "Iteration 6, loss = 0.62559949\n",
      "Iteration 10, loss = 0.03064313\n",
      "Iteration 7, loss = 0.05913861\n",
      "Iteration 2, loss = 0.67441625\n",
      "Iteration 10, loss = 0.63694132\n",
      "Iteration 4, loss = 0.68911447\n",
      "Iteration 18, loss = 0.00477845\n",
      "Iteration 11, loss = 0.02658529\n",
      "Iteration 10, loss = 0.03574830\n",
      "Iteration 11, loss = 0.02619547\n",
      "Iteration 16, loss = 0.60692045\n",
      "Iteration 7, loss = 0.06088040\n",
      "Iteration 7, loss = 0.62184734\n",
      "Iteration 3, loss = 0.66518074\n",
      "Iteration 16, loss = 0.60460146\n",
      "Iteration 14, loss = 0.00704507\n",
      "Iteration 11, loss = 0.03044969\n",
      "Iteration 11, loss = 0.63084405\n",
      "Iteration 18, loss = 0.60315251\n",
      "Iteration 4, loss = 0.65656884\n",
      "Iteration 5, loss = 0.67836135\n",
      "Iteration 12, loss = 0.02325081\n",
      "Iteration 8, loss = 0.04760525\n",
      "Iteration 15, loss = 0.00602769\n",
      "Iteration 8, loss = 0.04609558\n",
      "Iteration 12, loss = 0.02296385\n",
      "Iteration 8, loss = 0.61861998\n",
      "Iteration 12, loss = 0.02649184\n",
      "Iteration 12, loss = 0.62562496\n",
      "Iteration 19, loss = 0.00425083\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(200,), solver=adam; total time= 6.2min\n",
      "Iteration 17, loss = 0.60321956\n",
      "Iteration 6, loss = 0.66858231\n",
      "Iteration 13, loss = 0.62118880\n",
      "Iteration 9, loss = 0.03851028\n",
      "Iteration 13, loss = 0.02041324\n",
      "Iteration 17, loss = 0.60521944\n",
      "Iteration 13, loss = 0.02065286\n",
      "Iteration 9, loss = 0.03715664\n",
      "Iteration 9, loss = 0.61554148\n",
      "Iteration 5, loss = 0.64907019\n",
      "Iteration 19, loss = 0.60181721\n",
      "Iteration 14, loss = 0.61717245\n",
      "Iteration 10, loss = 0.03216977\n",
      "Iteration 7, loss = 0.65970384\n",
      "Iteration 19, loss = 0.00439194\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(200,), solver=adam; total time= 5.9min\n",
      "Iteration 14, loss = 0.01863487\n",
      "Iteration 18, loss = 0.60192764\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(200,), solver=sgd; total time= 4.6min\n",
      "Iteration 15, loss = 0.61393811\n",
      "Iteration 14, loss = 0.01844303\n",
      "Iteration 1, loss = 0.70842748\n",
      "Iteration 16, loss = 0.00539368\n",
      "Iteration 6, loss = 0.64239430\n",
      "Iteration 11, loss = 0.02748565\n",
      "Iteration 18, loss = 0.60364327\n",
      "Iteration 13, loss = 0.02343962\n",
      "Iteration 15, loss = 0.01697747\n",
      "Iteration 15, loss = 0.00624775\n",
      "Iteration 7, loss = 0.63633438\n",
      "Iteration 15, loss = 0.01682899\n",
      "Iteration 1, loss = 0.63272107\n",
      "Iteration 10, loss = 0.03086934\n",
      "Iteration 16, loss = 0.61103505\n",
      "Iteration 8, loss = 0.65179735\n",
      "Iteration 12, loss = 0.02403915\n",
      "Iteration 10, loss = 0.61302142\n",
      "Iteration 9, loss = 0.64461939\n",
      "Iteration 16, loss = 0.01566290\n",
      "Iteration 14, loss = 0.02106695\n",
      "Iteration 8, loss = 0.63110072\n",
      "Iteration 20, loss = 0.60057131\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(200,), solver=sgd; total time= 5.2min\n",
      "Iteration 16, loss = 0.01550362\n",
      "Iteration 13, loss = 0.02133277\n",
      "Iteration 11, loss = 0.02629473\n",
      "Iteration 2, loss = 0.69617802\n",
      "Iteration 17, loss = 0.60853372\n",
      "Iteration 17, loss = 0.01457255\n",
      "Iteration 9, loss = 0.62654117\n",
      "Iteration 2, loss = 0.39443885\n",
      "Iteration 17, loss = 0.00487785\n",
      "Iteration 16, loss = 0.00559455\n",
      "Iteration 10, loss = 0.63838458\n",
      "Iteration 17, loss = 0.01443321\n",
      "Iteration 11, loss = 0.61074708\n",
      "Iteration 15, loss = 0.01916037\n",
      "Iteration 18, loss = 0.60639355\n",
      "Iteration 1, loss = 0.58924532\n",
      "Iteration 3, loss = 0.68278992\n",
      "Iteration 12, loss = 0.60865821\n",
      "Iteration 18, loss = 0.01352790\n",
      "Iteration 12, loss = 0.02294450\n",
      "Iteration 14, loss = 0.01923281\n",
      "Iteration 19, loss = 0.60457499\n",
      "Iteration 4, loss = 0.67067491\n",
      "Iteration 11, loss = 0.63281745\n",
      "Iteration 1, loss = 0.59230326\n",
      "Iteration 16, loss = 0.01759034\n",
      "Iteration 3, loss = 0.22537812\n",
      "Iteration 18, loss = 0.01364769\n",
      "Iteration 19, loss = 0.60232402\n",
      "Iteration 13, loss = 0.60687349\n",
      "Iteration 10, loss = 0.62240801\n",
      "Iteration 20, loss = 0.60289374\n",
      "Iteration 17, loss = 0.00505460\n",
      "Iteration 15, loss = 0.01752786\n",
      "Iteration 19, loss = 0.01275813\n",
      "Iteration 18, loss = 0.00444908\n",
      "Iteration 2, loss = 0.36113192\n",
      "Iteration 5, loss = 0.66001848\n",
      "Iteration 19, loss = 0.01287853\n",
      "Iteration 21, loss = 0.60148693\n",
      "Iteration 13, loss = 0.02034417\n",
      "Iteration 17, loss = 0.01631562\n",
      "Iteration 12, loss = 0.62790933\n",
      "Iteration 11, loss = 0.61888027\n",
      "Iteration 16, loss = 0.01615237\n",
      "Iteration 14, loss = 0.60529034\n",
      "Iteration 20, loss = 0.01210712\n",
      "Iteration 4, loss = 0.13194620\n",
      "Iteration 14, loss = 0.01826755\n",
      "Iteration 13, loss = 0.62365069\n",
      "Iteration 6, loss = 0.65069132\n",
      "Iteration 18, loss = 0.00460914\n",
      "Iteration 12, loss = 0.61576637\n",
      "Iteration 2, loss = 0.35127696\n",
      "Iteration 3, loss = 0.20865178\n",
      "Iteration 22, loss = 0.60023627\n",
      "Iteration 18, loss = 0.01522090\n",
      "Iteration 21, loss = 0.01152029\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), solver=adam; total time= 3.0min\n",
      "Iteration 15, loss = 0.60388060\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), solver=sgd; total time= 2.2min\n",
      "Iteration 20, loss = 0.01219660\n",
      "Iteration 14, loss = 0.61992705\n",
      "Iteration 7, loss = 0.64264439\n",
      "Iteration 23, loss = 0.59914172\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), solver=sgd; total time= 2.4min\n",
      "Iteration 17, loss = 0.01499565\n",
      "Iteration 15, loss = 0.01666888\n",
      "Iteration 20, loss = 0.60116700\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(200,), solver=sgd; total time= 5.4min\n",
      "Iteration 19, loss = 0.01431628\n",
      "Iteration 13, loss = 0.61300610\n",
      "Iteration 15, loss = 0.61663787\n",
      "Iteration 19, loss = 0.00423523\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(200,), solver=adam; total time= 6.0min\n",
      "Iteration 3, loss = 0.19771780\n",
      "Iteration 1, loss = 0.70793040\n",
      "Iteration 4, loss = 0.12457923\n",
      "Iteration 14, loss = 0.61074959\n",
      "Iteration 8, loss = 0.63547505\n",
      "Iteration 18, loss = 0.01403938\n",
      "Iteration 16, loss = 0.61374700\n",
      "Iteration 16, loss = 0.01535205\n",
      "Iteration 5, loss = 0.08366543\n",
      "Iteration 19, loss = 0.00408616\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.0001, hidden_layer_sizes=(200,), solver=adam; total time= 6.2min\n",
      "Iteration 21, loss = 0.01162161\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), solver=adam; total time= 3.0min\n",
      "Iteration 1, loss = 0.69151757\n",
      "Iteration 1, loss = 0.61937645\n",
      "Iteration 1, loss = 0.72688949\n",
      "Iteration 15, loss = 0.60868109\n",
      "Iteration 1, loss = 0.58239072\n",
      "Iteration 19, loss = 0.01322938\n",
      "Iteration 2, loss = 0.69583377\n",
      "Iteration 9, loss = 0.62947138\n",
      "Iteration 20, loss = 0.01352205\n",
      "Iteration 17, loss = 0.01426820\n",
      "Iteration 1, loss = 0.67332186\n",
      "Iteration 2, loss = 0.71401513\n",
      "Iteration 2, loss = 0.68159433\n",
      "Iteration 1, loss = 0.72699701\n",
      "Iteration 5, loss = 0.08021950\n",
      "Iteration 16, loss = 0.60685601\n",
      "Iteration 6, loss = 0.05778002\n",
      "Iteration 17, loss = 0.61127849\n",
      "Iteration 2, loss = 0.36536107\n",
      "Iteration 4, loss = 0.11669538\n",
      "Iteration 2, loss = 0.66629399\n",
      "Iteration 17, loss = 0.60530410\n",
      "Iteration 3, loss = 0.68290571\n",
      "Iteration 10, loss = 0.62425949\n",
      "Iteration 20, loss = 0.01252641\n",
      "Iteration 18, loss = 0.60898932\n",
      "Iteration 18, loss = 0.01335529\n",
      "Iteration 2, loss = 0.39883015\n",
      "Iteration 3, loss = 0.67084403\n",
      "Iteration 3, loss = 0.70018136\n",
      "Iteration 11, loss = 0.61982709\n",
      "Iteration 19, loss = 0.60715771\n",
      "Iteration 21, loss = 0.01286132\n",
      "Iteration 18, loss = 0.60387963\n",
      "Iteration 21, loss = 0.01191549\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), solver=adam; total time= 3.1min\n",
      "Iteration 3, loss = 0.21041070\n",
      "Iteration 20, loss = 0.60541376\n",
      "Iteration 6, loss = 0.05593124\n",
      "Iteration 2, loss = 0.71316190\n",
      "Iteration 4, loss = 0.66152638\n",
      "Iteration 5, loss = 0.07478045\n",
      "Iteration 4, loss = 0.67134978\n",
      "Iteration 4, loss = 0.68743785\n",
      "Iteration 12, loss = 0.61602222\n",
      "Iteration 3, loss = 0.22921030\n",
      "Iteration 7, loss = 0.04301687\n",
      "Iteration 3, loss = 0.65861528\n",
      "Iteration 19, loss = 0.01258252\n",
      "Iteration 19, loss = 0.60270379\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), solver=sgd; total time= 2.4min\n",
      "Iteration 13, loss = 0.61283486\n",
      "Iteration 21, loss = 0.60398579\n",
      "Iteration 5, loss = 0.66135254\n",
      "Iteration 5, loss = 0.65340551\n",
      "Iteration 5, loss = 0.67612729\n",
      "Iteration 22, loss = 0.60264618\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), solver=sgd; total time= 2.6min\n",
      "Iteration 7, loss = 0.04166104\n",
      "Iteration 3, loss = 0.69840506\n",
      "Iteration 6, loss = 0.65257991\n",
      "Iteration 4, loss = 0.12390556\n",
      "Iteration 1, loss = 0.58193069\n",
      "Iteration 4, loss = 0.13298527\n",
      "Iteration 14, loss = 0.61003893\n",
      "Iteration 6, loss = 0.05191118\n",
      "Iteration 20, loss = 0.01193393\n",
      "Iteration 4, loss = 0.65182459\n",
      "Iteration 22, loss = 0.01226147\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), solver=adam; total time= 4.2min\n",
      "Iteration 6, loss = 0.66581969\n",
      "Iteration 4, loss = 0.68530203\n",
      "Iteration 7, loss = 0.64496993\n",
      "Iteration 6, loss = 0.64598653\n",
      "Iteration 15, loss = 0.60771091\n",
      "Iteration 5, loss = 0.08351927\n",
      "Iteration 8, loss = 0.03275834\n",
      "Iteration 1, loss = 0.58860601\n",
      "Iteration 5, loss = 0.07862702\n",
      "Iteration 16, loss = 0.60563837\n",
      "Iteration 8, loss = 0.03382770\n",
      "Iteration 21, loss = 0.01136304\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), solver=adam; total time= 3.7min\n",
      "Iteration 7, loss = 0.63952039\n",
      "Iteration 7, loss = 0.65695504\n",
      "Iteration 5, loss = 0.64546491\n",
      "Iteration 8, loss = 0.63817711\n",
      "Iteration 7, loss = 0.03879738\n",
      "Iteration 1, loss = 0.59865358\n",
      "Iteration 2, loss = 0.34821219\n",
      "Iteration 17, loss = 0.60381951\n",
      "Iteration 6, loss = 0.05758881\n",
      "Iteration 9, loss = 0.02699139\n",
      "Iteration 5, loss = 0.67322585\n",
      "Iteration 8, loss = 0.63391632\n",
      "Iteration 1, loss = 0.61259512\n",
      "Iteration 6, loss = 0.63988903\n",
      "Iteration 6, loss = 0.05423379\n",
      "Iteration 18, loss = 0.60226180\n",
      "Iteration 9, loss = 0.02776984\n",
      "Iteration 7, loss = 0.04259652\n",
      "Iteration 9, loss = 0.63250587\n",
      "Iteration 6, loss = 0.66253880\n",
      "Iteration 7, loss = 0.63479686\n",
      "Iteration 10, loss = 0.02288769\n",
      "Iteration 8, loss = 0.03062323\n",
      "Iteration 3, loss = 0.18846476\n",
      "Iteration 8, loss = 0.64891748\n",
      "Iteration 9, loss = 0.62898363\n",
      "Iteration 19, loss = 0.60090422\n",
      "Iteration 2, loss = 0.34070450\n",
      "Iteration 2, loss = 0.36340170\n",
      "Iteration 1, loss = 0.60313771\n",
      "Iteration 2, loss = 0.38503387\n",
      "Iteration 9, loss = 0.64197457\n",
      "Iteration 7, loss = 0.65312851\n",
      "Iteration 8, loss = 0.63026680\n",
      "Iteration 10, loss = 0.62463832\n",
      "Iteration 8, loss = 0.03356099\n",
      "Iteration 10, loss = 0.62727230\n",
      "Iteration 7, loss = 0.04040188\n",
      "Iteration 20, loss = 0.59975250\n",
      "Iteration 9, loss = 0.02520303\n",
      "Iteration 11, loss = 0.01996762\n",
      "Iteration 10, loss = 0.02352651\n",
      "Iteration 11, loss = 0.62080462\n",
      "Iteration 3, loss = 0.17901257\n",
      "Iteration 9, loss = 0.02756894\n",
      "Iteration 8, loss = 0.64480319\n",
      "Iteration 3, loss = 0.19234045\n",
      "Iteration 3, loss = 0.20653180\n",
      "Iteration 4, loss = 0.10539704\n",
      "Iteration 9, loss = 0.62624541\n",
      "Iteration 8, loss = 0.03179206\n",
      "Iteration 2, loss = 0.36684869\n",
      "Iteration 11, loss = 0.62288719\n",
      "Iteration 11, loss = 0.02048130\n",
      "Iteration 21, loss = 0.59868741\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(100,), solver=sgd; total time= 3.1min\n",
      "Iteration 12, loss = 0.61756071\n",
      "Iteration 10, loss = 0.02337906\n",
      "Iteration 10, loss = 0.02135908\n",
      "Iteration 10, loss = 0.63597531\n",
      "Iteration 9, loss = 0.63776756\n",
      "Iteration 4, loss = 0.09924912\n",
      "Iteration 4, loss = 0.11158161\n",
      "Iteration 12, loss = 0.01778916\n",
      "Iteration 10, loss = 0.62254002\n",
      "Iteration 12, loss = 0.01820775\n",
      "Iteration 5, loss = 0.06509522\n",
      "Iteration 12, loss = 0.61899453\n",
      "Iteration 11, loss = 0.63049557\n",
      "Iteration 11, loss = 0.02033999\n",
      "Iteration 10, loss = 0.63143349\n",
      "Iteration 11, loss = 0.01861434\n",
      "Iteration 13, loss = 0.61464177\n",
      "Iteration 9, loss = 0.02611790\n",
      "Iteration 13, loss = 0.01606869\n",
      "Iteration 11, loss = 0.62614557\n",
      "Iteration 12, loss = 0.62605214\n",
      "Iteration 11, loss = 0.61945230\n",
      "Iteration 1, loss = 0.70696675\n",
      "Iteration 13, loss = 0.61563062\n",
      "Iteration 5, loss = 0.06150131\n",
      "Iteration 3, loss = 0.19617477\n",
      "Iteration 12, loss = 0.01655097\n",
      "Iteration 13, loss = 0.01643762\n",
      "Iteration 12, loss = 0.62151014\n",
      "Iteration 6, loss = 0.04482234\n",
      "Iteration 13, loss = 0.62179180\n",
      "Iteration 5, loss = 0.06753906\n",
      "Iteration 14, loss = 0.01472544\n",
      "Iteration 14, loss = 0.61273683\n",
      "Iteration 12, loss = 0.61659981\n",
      "Iteration 2, loss = 0.69521453\n",
      "Iteration 14, loss = 0.61212662\n",
      "Iteration 13, loss = 0.01496481\n",
      "Iteration 4, loss = 0.10507554\n",
      "Iteration 4, loss = 0.10824738\n",
      "Iteration 14, loss = 0.01501903\n",
      "Iteration 12, loss = 0.01807355\n",
      "Iteration 10, loss = 0.02218988\n",
      "Iteration 13, loss = 0.61754738\n",
      "Iteration 15, loss = 0.01363083\n",
      "Iteration 13, loss = 0.61417591\n",
      "Iteration 15, loss = 0.61017442\n",
      "Iteration 14, loss = 0.61830739\n",
      "Iteration 3, loss = 0.68212074\n",
      "Iteration 7, loss = 0.03373037\n",
      "Iteration 14, loss = 0.61414337\n",
      "Iteration 15, loss = 0.60984034\n",
      "Iteration 15, loss = 0.01389711\n",
      "Iteration 16, loss = 0.60796351\n",
      "Iteration 16, loss = 0.01275171Iteration 5, loss = 0.06609578\n",
      "\n",
      "Iteration 11, loss = 0.01936204\n",
      "Iteration 14, loss = 0.01371329\n",
      "Iteration 5, loss = 0.06459827\n",
      "Iteration 14, loss = 0.61203001\n",
      "Iteration 13, loss = 0.01630025\n",
      "Iteration 17, loss = 0.60605031\n",
      "Iteration 15, loss = 0.61113174\n",
      "Iteration 15, loss = 0.61525966\n",
      "Iteration 6, loss = 0.04590026\n",
      "Iteration 8, loss = 0.02685307\n",
      "Iteration 6, loss = 0.04249833\n",
      "Iteration 17, loss = 0.01200443\n",
      "Iteration 16, loss = 0.60784779\n",
      "Iteration 15, loss = 0.61010555\n",
      "Iteration 15, loss = 0.01269739\n",
      "Iteration 18, loss = 0.60439417\n",
      "Iteration 16, loss = 0.61257894\n",
      "Iteration 16, loss = 0.01296871\n",
      "Iteration 12, loss = 0.01720868\n",
      "Iteration 14, loss = 0.01492888\n",
      "Iteration 4, loss = 0.67028188\n",
      "Iteration 16, loss = 0.60866838\n",
      "Iteration 17, loss = 0.60618990\n",
      "Iteration 18, loss = 0.01137822\n",
      "Iteration 16, loss = 0.60841804\n",
      "Iteration 6, loss = 0.04450540\n",
      "Iteration 17, loss = 0.61021965\n",
      "Iteration 7, loss = 0.03200120\n",
      "Iteration 19, loss = 0.60289836\n",
      "Iteration 7, loss = 0.03406410\n",
      "Iteration 9, loss = 0.02239366\n",
      "Iteration 17, loss = 0.01219594\n",
      "Iteration 17, loss = 0.60650171\n",
      "Iteration 6, loss = 0.04521595\n",
      "Iteration 16, loss = 0.01187197\n",
      "Iteration 17, loss = 0.60695326\n",
      "Iteration 18, loss = 0.60458166\n",
      "Iteration 5, loss = 0.65963138\n",
      "Iteration 20, loss = 0.60163605\n",
      "Iteration 13, loss = 0.01554384\n",
      "Iteration 18, loss = 0.60451426\n",
      "Iteration 15, loss = 0.01380366\n",
      "Iteration 18, loss = 0.60809938\n",
      "Iteration 19, loss = 0.60325440\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), solver=sgd; total time= 3.3min\n",
      "Iteration 7, loss = 0.03346718\n",
      "Iteration 6, loss = 0.65054546\n",
      "Iteration 21, loss = 0.60040575\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), solver=sgd; total time= 3.4min\n",
      "Iteration 19, loss = 0.60287187\n",
      "Iteration 18, loss = 0.60563297\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), solver=sgd; total time= 3.3min\n",
      "Iteration 18, loss = 0.01154716\n",
      "Iteration 17, loss = 0.01117650\n",
      "Iteration 19, loss = 0.01084171\n",
      "Iteration 8, loss = 0.02690790\n",
      "Iteration 19, loss = 0.60638786\n",
      "Iteration 7, loss = 0.03359222\n",
      "Iteration 10, loss = 0.01927235\n",
      "Iteration 7, loss = 0.64236733\n",
      "Iteration 1, loss = 0.66777195\n",
      "Iteration 16, loss = 0.01287049\n",
      "Iteration 20, loss = 0.60134874\n",
      "Iteration 8, loss = 0.02545043\n",
      "Iteration 14, loss = 0.01423364\n",
      "Iteration 1, loss = 0.71513515\n",
      "Iteration 8, loss = 0.02674883\n",
      "Iteration 20, loss = 0.60468916\n",
      "Iteration 11, loss = 0.01704031\n",
      "Iteration 21, loss = 0.60017061\n",
      "Iteration 2, loss = 0.66098485\n",
      "Iteration 20, loss = 0.01037683\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), solver=adam; total time= 4.7min\n",
      "Iteration 19, loss = 0.01098911\n",
      "Iteration 9, loss = 0.02223611\n",
      "Iteration 21, loss = 0.60331270\n",
      "Iteration 1, loss = 0.72009354\n",
      "Iteration 8, loss = 0.02657385\n",
      "Iteration 8, loss = 0.63543569\n",
      "Iteration 17, loss = 0.01211174\n",
      "Iteration 18, loss = 0.01060027\n",
      "Iteration 9, loss = 0.02122438\n",
      "Iteration 2, loss = 0.70148908\n",
      "Iteration 3, loss = 0.65345337\n",
      "Iteration 22, loss = 0.59902361\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), solver=sgd; total time= 3.8min\n",
      "Iteration 1, loss = 0.67459867\n",
      "Iteration 2, loss = 0.70706542\n",
      "Iteration 20, loss = 0.01050256\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), solver=adam; total time= 5.0min\n",
      "Iteration 22, loss = 0.60208215\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), solver=sgd; total time= 3.9min\n",
      "Iteration 10, loss = 0.01896350\n",
      "Iteration 12, loss = 0.01532692\n",
      "Iteration 1, loss = 0.69380394\n",
      "Iteration 3, loss = 0.68701517\n",
      "Iteration 18, loss = 0.01145801\n",
      "Iteration 1, loss = 0.60632462\n",
      "Iteration 2, loss = 0.54893318\n",
      "Iteration 15, loss = 0.01317610\n",
      "Iteration 2, loss = 0.48698571\n",
      "Iteration 9, loss = 0.02234724\n",
      "Iteration 2, loss = 0.66722842\n",
      "Iteration 9, loss = 0.62961961\n",
      "Iteration 3, loss = 0.37154621\n",
      "Iteration 3, loss = 0.41166033\n",
      "Iteration 19, loss = 0.01009895\n",
      "Iteration 4, loss = 0.27795273\n",
      "Iteration 1, loss = 0.61308612\n",
      "Iteration 4, loss = 0.64656692\n",
      "Iteration 4, loss = 0.30430182\n",
      "Iteration 5, loss = 0.22834089\n",
      "Iteration 10, loss = 0.01827891\n",
      "Iteration 19, loss = 0.01091157\n",
      "Iteration 5, loss = 0.20966056\n",
      "Iteration 4, loss = 0.67427507\n",
      "Iteration 9, loss = 0.02201925\n",
      "Iteration 16, loss = 0.01231745\n",
      "Iteration 2, loss = 0.48110703\n",
      "Iteration 10, loss = 0.62450642\n",
      "Iteration 11, loss = 0.01660824\n",
      "Iteration 10, loss = 0.01924931\n",
      "Iteration 6, loss = 0.16287559\n",
      "Iteration 7, loss = 0.13110712\n",
      "Iteration 3, loss = 0.36520811\n",
      "Iteration 3, loss = 0.65930669\n",
      "Iteration 6, loss = 0.17667727\n",
      "Iteration 5, loss = 0.64053375\n",
      "Iteration 3, loss = 0.69309152\n",
      "Iteration 4, loss = 0.27610504\n",
      "Iteration 20, loss = 0.00967584\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), solver=adam; total time= 5.1min\n",
      "Iteration 11, loss = 0.62020478\n",
      "Iteration 8, loss = 0.10936061\n",
      "Iteration 5, loss = 0.21185599\n",
      "Iteration 13, loss = 0.01402857\n",
      "Iteration 17, loss = 0.01159430\n",
      "Iteration 1, loss = 0.63526494\n",
      "Iteration 5, loss = 0.66315001\n",
      "Iteration 11, loss = 0.01614719\n",
      "Iteration 6, loss = 0.16671790\n",
      "Iteration 9, loss = 0.09392008\n",
      "Iteration 7, loss = 0.14148942\n",
      "Iteration 11, loss = 0.01703857\n",
      "Iteration 20, loss = 0.01042976\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), solver=adam; total time= 4.6min\n",
      "Iteration 10, loss = 0.08274503\n",
      "Iteration 7, loss = 0.13512117\n",
      "Iteration 6, loss = 0.63502022\n",
      "Iteration 12, loss = 0.01486484\n",
      "Iteration 1, loss = 0.61234358\n",
      "Iteration 11, loss = 0.07443071\n",
      "Iteration 4, loss = 0.68033649\n",
      "Iteration 8, loss = 0.11248254\n",
      "Iteration 2, loss = 0.49346241\n",
      "Iteration 10, loss = 0.01881945\n",
      "Iteration 8, loss = 0.11741795\n",
      "Iteration 12, loss = 0.06786734\n",
      "Iteration 9, loss = 0.09583253\n",
      "Iteration 13, loss = 0.06272295\n",
      "Iteration 18, loss = 0.01097903\n",
      "Iteration 9, loss = 0.10043287\n",
      "Iteration 12, loss = 0.61633221\n",
      "Iteration 4, loss = 0.65250016\n",
      "Iteration 10, loss = 0.08808016\n",
      "Iteration 6, loss = 0.65344165\n",
      "Iteration 2, loss = 0.50182739\n",
      "Iteration 10, loss = 0.08389997\n",
      "Iteration 3, loss = 0.36875135\n",
      "Iteration 11, loss = 0.07877545\n",
      "Iteration 14, loss = 0.05856816\n",
      "Iteration 11, loss = 0.07507203\n",
      "Iteration 7, loss = 0.63032980\n",
      "Iteration 15, loss = 0.05512828\n",
      "Iteration 12, loss = 0.07171175\n",
      "Iteration 4, loss = 0.27483888\n",
      "Iteration 3, loss = 0.38892573\n",
      "Iteration 5, loss = 0.20876886\n",
      "Iteration 16, loss = 0.05224985\n",
      "Iteration 13, loss = 0.06604259\n",
      "Iteration 13, loss = 0.01352384\n",
      "Iteration 4, loss = 0.29467612\n",
      "Iteration 13, loss = 0.61311478\n",
      "Iteration 6, loss = 0.16350131\n",
      "Iteration 17, loss = 0.04977335\n",
      "Iteration 14, loss = 0.06150042\n",
      "Iteration 14, loss = 0.01296298\n",
      "Iteration 5, loss = 0.22387643\n",
      "Iteration 19, loss = 0.01045535\n",
      "Iteration 12, loss = 0.06826315\n",
      "Iteration 7, loss = 0.13257577\n",
      "Iteration 12, loss = 0.01532301\n",
      "Iteration 18, loss = 0.04764279\n",
      "Iteration 8, loss = 0.62599200\n",
      "Iteration 5, loss = 0.66865164\n",
      "Iteration 15, loss = 0.05773835\n",
      "Iteration 6, loss = 0.17375722\n",
      "Iteration 13, loss = 0.06299490\n",
      "Iteration 8, loss = 0.11110918\n",
      "Iteration 7, loss = 0.64492326\n",
      "Iteration 19, loss = 0.04576244\n",
      "Iteration 11, loss = 0.01661355\n",
      "Iteration 14, loss = 0.05867560\n",
      "Iteration 16, loss = 0.05456870\n",
      "Iteration 15, loss = 0.05517921\n",
      "Iteration 9, loss = 0.09589506\n",
      "Iteration 20, loss = 0.04411690\n",
      "Iteration 14, loss = 0.01245359\n",
      "Iteration 12, loss = 0.01456019\n",
      "Iteration 7, loss = 0.13954833\n",
      "Iteration 16, loss = 0.05221757\n",
      "Iteration 10, loss = 0.08454704\n",
      "Iteration 21, loss = 0.04264142\n",
      "Iteration 17, loss = 0.05186458\n",
      "Iteration 5, loss = 0.64638786\n",
      "Iteration 22, loss = 0.04129349\n",
      "Iteration 18, loss = 0.04956954\n",
      "Iteration 20, loss = 0.01001220\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(150,), solver=adam; total time= 5.2min\n",
      "Iteration 11, loss = 0.07608705\n",
      "Iteration 13, loss = 0.01401334\n",
      "Iteration 17, loss = 0.04974198\n",
      "Iteration 8, loss = 0.11608318\n",
      "Iteration 23, loss = 0.04010774\n",
      "Iteration 6, loss = 0.65869574\n",
      "Iteration 12, loss = 0.06947292\n",
      "Iteration 1, loss = 0.65778760\n",
      "Iteration 14, loss = 0.61040777\n",
      "Iteration 24, loss = 0.03900561\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), solver=adam; total time= 1.2min\n",
      "Iteration 13, loss = 0.06425764\n",
      "Iteration 9, loss = 0.62226812\n",
      "Iteration 2, loss = 0.65223718\n",
      "Iteration 1, loss = 0.64850325\n",
      "Iteration 19, loss = 0.04754185\n",
      "Iteration 8, loss = 0.63788527\n",
      "Iteration 9, loss = 0.09955736\n",
      "Iteration 18, loss = 0.04755446\n",
      "Iteration 14, loss = 0.05992751\n",
      "Iteration 2, loss = 0.64386677\n",
      "Iteration 15, loss = 0.01160190\n",
      "Iteration 15, loss = 0.05638626\n",
      "Iteration 3, loss = 0.63909576\n",
      "Iteration 15, loss = 0.01210817\n",
      "Iteration 16, loss = 0.05345751\n",
      "Iteration 7, loss = 0.65013201\n",
      "Iteration 3, loss = 0.64628801\n",
      "Iteration 12, loss = 0.01491715\n",
      "Iteration 4, loss = 0.63472038\n",
      "Iteration 10, loss = 0.08751550\n",
      "Iteration 5, loss = 0.63083005\n",
      "Iteration 4, loss = 0.64097051\n",
      "Iteration 17, loss = 0.05089099\n",
      "Iteration 6, loss = 0.64101459\n",
      "Iteration 20, loss = 0.04574873\n",
      "Iteration 19, loss = 0.04566058\n",
      "Iteration 15, loss = 0.60801193\n",
      "Iteration 5, loss = 0.63620124\n",
      "Iteration 6, loss = 0.62752425\n",
      "Iteration 20, loss = 0.04400004\n",
      "Iteration 14, loss = 0.01296030\n",
      "Iteration 6, loss = 0.63193344\n",
      "Iteration 18, loss = 0.04869379\n",
      "Iteration 21, loss = 0.04249893\n",
      "Iteration 13, loss = 0.01329860\n",
      "Iteration 19, loss = 0.04679476\n",
      "Iteration 7, loss = 0.62825563\n",
      "Iteration 10, loss = 0.61897594\n",
      "Iteration 22, loss = 0.04118199\n",
      "Iteration 11, loss = 0.07856661\n",
      "Iteration 7, loss = 0.62444291\n",
      "Iteration 8, loss = 0.62503918\n",
      "Iteration 21, loss = 0.04414646\n",
      "Iteration 20, loss = 0.04511588\n",
      "Iteration 23, loss = 0.03995404\n",
      "Iteration 16, loss = 0.60595283\n",
      "Iteration 9, loss = 0.62205776\n",
      "Iteration 21, loss = 0.04359237\n",
      "Iteration 7, loss = 0.63627893\n",
      "Iteration 10, loss = 0.61950974\n",
      "Iteration 8, loss = 0.62172581\n",
      "Iteration 12, loss = 0.07162611\n",
      "Iteration 11, loss = 0.61716270\n",
      "Iteration 9, loss = 0.61929518\n",
      "Iteration 12, loss = 0.61517673\n",
      "Iteration 13, loss = 0.01360979\n",
      "Iteration 16, loss = 0.01140831\n",
      "Iteration 13, loss = 0.06606422\n",
      "Iteration 24, loss = 0.03885188\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), solver=adam; total time= 1.6min\n",
      "Iteration 22, loss = 0.04225522\n",
      "Iteration 8, loss = 0.64266464\n",
      "Iteration 11, loss = 0.61622551\n",
      "Iteration 22, loss = 0.04272828\n",
      "Iteration 1, loss = 0.76772864\n",
      "Iteration 10, loss = 0.61717268\n",
      "Iteration 14, loss = 0.06161560\n",
      "Iteration 13, loss = 0.61331980\n",
      "Iteration 17, loss = 0.60410671\n",
      "Iteration 11, loss = 0.61511891\n",
      "Iteration 9, loss = 0.63162835\n",
      "Iteration 2, loss = 0.75123777\n",
      "Iteration 15, loss = 0.05794545\n",
      "Iteration 14, loss = 0.01232765\n",
      "Iteration 12, loss = 0.61342413\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), solver=sgd; total time=  33.3s\n",
      "Iteration 3, loss = 0.73366933\n",
      "Iteration 16, loss = 0.01088861\n",
      "Iteration 1, loss = 0.64909625\n",
      "Iteration 4, loss = 0.71730713\n",
      "Iteration 14, loss = 0.61166295Iteration 23, loss = 0.04098721\n",
      "\n",
      "Iteration 23, loss = 0.04142917\n",
      "Iteration 2, loss = 0.64441268\n",
      "Iteration 16, loss = 0.05486391\n",
      "Iteration 5, loss = 0.70257808\n",
      "Iteration 3, loss = 0.63946741\n",
      "Iteration 10, loss = 0.62632187\n",
      "Iteration 8, loss = 0.63199175\n",
      "Iteration 6, loss = 0.68909786\n",
      "Iteration 4, loss = 0.63489103\n",
      "Iteration 12, loss = 0.61363254\n",
      "Iteration 15, loss = 0.01210649\n",
      "Iteration 18, loss = 0.60262628\n",
      "Iteration 7, loss = 0.67722634\n",
      "Iteration 5, loss = 0.63084490\n",
      "Iteration 15, loss = 0.61020540\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), solver=sgd; total time=  46.5s\n",
      "Iteration 24, loss = 0.04026227\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), solver=adam; total time= 2.0min\n",
      "Iteration 24, loss = 0.03985817\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), solver=adam; total time= 1.5min\n",
      "Iteration 14, loss = 0.01256806\n",
      "Iteration 9, loss = 0.63612280\n",
      "Iteration 6, loss = 0.62728030\n",
      "Iteration 1, loss = 0.73021102\n",
      "Iteration 15, loss = 0.01153205\n",
      "Iteration 17, loss = 0.05222392\n",
      "Iteration 7, loss = 0.62418339\n",
      "Iteration 8, loss = 0.66665630\n",
      "Iteration 2, loss = 0.71462198\n",
      "Iteration 18, loss = 0.04996018\n",
      "Iteration 3, loss = 0.69859234\n",
      "Iteration 1, loss = 0.66194893\n",
      "Iteration 8, loss = 0.62137068\n",
      "Iteration 4, loss = 0.68425988\n",
      "Iteration 9, loss = 0.65741713\n",
      "Iteration 19, loss = 0.04795969\n",
      "Iteration 17, loss = 0.01080434\n",
      "Iteration 9, loss = 0.61893312\n",
      "Iteration 1, loss = 0.63571418\n",
      "Iteration 9, loss = 0.62810544\n",
      "Iteration 13, loss = 0.61145184\n",
      "Iteration 10, loss = 0.63043195\n",
      "Iteration 20, loss = 0.04619110\n",
      "Iteration 5, loss = 0.67174309\n",
      "Iteration 10, loss = 0.64934595\n",
      "Iteration 21, loss = 0.04463097\n",
      "Iteration 11, loss = 0.62169022\n",
      "Iteration 10, loss = 0.61678267\n",
      "Iteration 11, loss = 0.64242372\n",
      "Iteration 19, loss = 0.60122779\n",
      "Iteration 22, loss = 0.04320530\n",
      "Iteration 15, loss = 0.01173217\n",
      "Iteration 2, loss = 0.48806278\n",
      "Iteration 23, loss = 0.04190283\n",
      "Iteration 6, loss = 0.66083449\n",
      "Iteration 11, loss = 0.61488116\n",
      "Iteration 17, loss = 0.01030950\n",
      "Iteration 11, loss = 0.62565922\n",
      "Iteration 12, loss = 0.63624541\n",
      "Iteration 2, loss = 0.42649618\n",
      "Iteration 24, loss = 0.04074617\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), solver=adam; total time= 1.7min\n",
      "Iteration 16, loss = 0.01140580\n",
      "Iteration 12, loss = 0.61312926\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), solver=sgd; total time=  30.3s\n",
      "Iteration 13, loss = 0.63103849\n",
      "Iteration 16, loss = 0.01087430\n",
      "Iteration 14, loss = 0.62637236\n",
      "Iteration 7, loss = 0.65162584\n",
      "Iteration 15, loss = 0.62242657\n",
      "Iteration 10, loss = 0.62472848\n",
      "Iteration 3, loss = 0.28235050\n",
      "Iteration 3, loss = 0.32750994\n",
      "Iteration 16, loss = 0.61887259\n",
      "Iteration 17, loss = 0.61585755\n",
      "Iteration 1, loss = 0.63550566\n",
      "Iteration 12, loss = 0.61774709\n",
      "Iteration 8, loss = 0.64346445\n",
      "Iteration 18, loss = 0.01030411\n",
      "Iteration 20, loss = 0.60004346\n",
      "Iteration 12, loss = 0.62143637\n",
      "Iteration 18, loss = 0.61318889\n",
      "Iteration 14, loss = 0.60954791\n",
      "Iteration 9, loss = 0.63669131\n",
      "Iteration 16, loss = 0.01105023\n",
      "Iteration 10, loss = 0.63059861\n",
      "Iteration 17, loss = 0.01031823\n",
      "Iteration 2, loss = 0.45675610\n",
      "Iteration 4, loss = 0.19467078\n",
      "Iteration 11, loss = 0.62569855\n",
      "Iteration 1, loss = 0.61242977\n",
      "Iteration 12, loss = 0.62130328\n",
      "Iteration 11, loss = 0.62166154\n",
      "Iteration 19, loss = 0.61087306\n",
      "Iteration 4, loss = 0.21665329\n",
      "Iteration 13, loss = 0.61740204\n",
      "Iteration 18, loss = 0.00981178\n",
      "Iteration 2, loss = 0.43754449\n",
      "Iteration 5, loss = 0.14183054\n",
      "Iteration 13, loss = 0.61783816\n",
      "Iteration 21, loss = 0.59892014\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(200,), solver=sgd; total time= 4.7min\n",
      "Iteration 13, loss = 0.61447465\n",
      "Iteration 20, loss = 0.60882958\n",
      "Iteration 14, loss = 0.61432206\n",
      "Iteration 3, loss = 0.30281349\n",
      "Iteration 17, loss = 0.01080777\n",
      "Iteration 15, loss = 0.61150904\n",
      "Iteration 17, loss = 0.01046543\n",
      "Iteration 5, loss = 0.15077233\n",
      "Iteration 16, loss = 0.60913758\n",
      "Iteration 15, loss = 0.60790490\n",
      "Iteration 12, loss = 0.61894361\n",
      "Iteration 21, loss = 0.60709207\n",
      "Iteration 3, loss = 0.29040073\n",
      "Iteration 22, loss = 0.60551991\n",
      "Iteration 6, loss = 0.10965974\n",
      "Iteration 4, loss = 0.20075903\n",
      "Iteration 17, loss = 0.60706902\n",
      "Iteration 23, loss = 0.60421084\n",
      "Iteration 19, loss = 0.00987109\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(200,), solver=adam; total time= 6.2min\n",
      "Iteration 6, loss = 0.11332003\n",
      "Iteration 14, loss = 0.61154883\n",
      "Iteration 1, loss = 0.58749950\n",
      "Iteration 4, loss = 0.19456817\n",
      "Iteration 18, loss = 0.60532827\n",
      "Iteration 24, loss = 0.60291308\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), solver=sgd; total time= 1.3min\n",
      "Iteration 18, loss = 0.00984581\n",
      "Iteration 18, loss = 0.01030191\n",
      "Iteration 1, loss = 0.70022713\n",
      "Iteration 7, loss = 0.08928751\n",
      "Iteration 14, loss = 0.61467910\n",
      "Iteration 16, loss = 0.60633351\n",
      "Iteration 1, loss = 0.68731600\n",
      "Iteration 19, loss = 0.60384681\n",
      "Iteration 19, loss = 0.00938599\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(200,), solver=adam; total time= 6.0min\n",
      "Iteration 5, loss = 0.14154313\n",
      "Iteration 5, loss = 0.13942126\n",
      "Iteration 20, loss = 0.60246444\n",
      "Iteration 8, loss = 0.07605480\n",
      "Iteration 21, loss = 0.60130453\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(50,), solver=sgd; total time= 1.1min\n",
      "Iteration 1, loss = 0.74319212\n",
      "Iteration 7, loss = 0.09124513\n",
      "Iteration 2, loss = 0.69109988\n",
      "Iteration 15, loss = 0.60909131\n",
      "Iteration 19, loss = 0.00943440\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.61651369\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(200,), solver=adam; total time= 6.3min\n",
      "Iteration 2, loss = 0.41164066\n",
      "Iteration 15, loss = 0.61209153\n",
      "Iteration 6, loss = 0.10721900\n",
      "Iteration 2, loss = 0.67868309\n",
      "Iteration 2, loss = 0.72733302\n",
      "Iteration 9, loss = 0.06668371\n",
      "Iteration 18, loss = 0.00997425\n",
      "Iteration 3, loss = 0.68128035\n",
      "Iteration 1, loss = 0.67757254\n",
      "Iteration 6, loss = 0.10793378\n",
      "Iteration 7, loss = 0.08782133\n",
      "Iteration 3, loss = 0.27315312\n",
      "Iteration 17, loss = 0.60503805\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(200,), solver=sgd; total time= 3.8min\n",
      "Iteration 19, loss = 0.00985990\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.07716942\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(200,), solver=adam; total time= 6.5min\n",
      "Iteration 3, loss = 0.71107205\n",
      "Iteration 3, loss = 0.66929047\n",
      "Iteration 1, loss = 0.71278804\n",
      "Iteration 16, loss = 0.60965493\n",
      "Iteration 2, loss = 0.66962875\n",
      "Iteration 4, loss = 0.67183536\n",
      "Iteration 14, loss = 0.61432412\n",
      "Iteration 7, loss = 0.08785847\n",
      "Iteration 9, loss = 0.06761704\n",
      "Iteration 8, loss = 0.07516215\n",
      "Iteration 4, loss = 0.66065663\n",
      "Iteration 2, loss = 0.70125918\n",
      "Iteration 1, loss = 0.63441043\n",
      "Iteration 19, loss = 0.00954827\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(200,), solver=adam; total time= 6.3min\n",
      "Iteration 3, loss = 0.66101680\n",
      "Iteration 4, loss = 0.69647873\n",
      "Iteration 16, loss = 0.60693432\n",
      "Iteration 5, loss = 0.65313019\n",
      "Iteration 4, loss = 0.18554036\n",
      "Iteration 1, loss = 0.61336515\n",
      "Iteration 9, loss = 0.06646103\n",
      "Iteration 10, loss = 0.06011775\n",
      "Iteration 3, loss = 0.68857325\n",
      "Iteration 17, loss = 0.60766571\n",
      "Iteration 8, loss = 0.07503655\n",
      "Iteration 10, loss = 0.06077160\n",
      "Iteration 4, loss = 0.65350898\n",
      "Iteration 5, loss = 0.66350690\n",
      "Iteration 6, loss = 0.64636090\n",
      "Iteration 4, loss = 0.67713707\n",
      "Iteration 10, loss = 0.06007977\n",
      "Iteration 15, loss = 0.61238957\n",
      "Iteration 5, loss = 0.64684127\n",
      "Iteration 5, loss = 0.68367275\n",
      "Iteration 5, loss = 0.13388172\n",
      "Iteration 2, loss = 0.41207849\n",
      "Iteration 9, loss = 0.06619455\n",
      "Iteration 6, loss = 0.65567979\n",
      "Iteration 5, loss = 0.66686468\n",
      "Iteration 18, loss = 0.60583793\n",
      "Iteration 11, loss = 0.05567248\n",
      "Iteration 6, loss = 0.64104083\n",
      "Iteration 7, loss = 0.64056069\n",
      "Iteration 2, loss = 0.40625573\n",
      "Iteration 6, loss = 0.67205614\n",
      "Iteration 11, loss = 0.05515651\n",
      "Iteration 6, loss = 0.10356066\n",
      "Iteration 17, loss = 0.60502165\n",
      "Iteration 6, loss = 0.65774264\n",
      "Iteration 8, loss = 0.63523734\n",
      "Iteration 11, loss = 0.05531502\n",
      "Iteration 10, loss = 0.05976064\n",
      "Iteration 7, loss = 0.63592308\n",
      "Iteration 7, loss = 0.66200234\n",
      "Iteration 16, loss = 0.61062546\n",
      "Iteration 7, loss = 0.64862793\n",
      "Iteration 7, loss = 0.08524926\n",
      "Iteration 1, loss = 0.63126002\n",
      "Iteration 7, loss = 0.65001961\n",
      "Iteration 3, loss = 0.24947380\n",
      "Iteration 9, loss = 0.63080885\n",
      "Iteration 12, loss = 0.05160445\n",
      "Iteration 11, loss = 0.05496005\n",
      "Iteration 18, loss = 0.60339780\n",
      "Iteration 3, loss = 0.24844804\n",
      "Iteration 19, loss = 0.60433864\n",
      "Iteration 12, loss = 0.05122803\n",
      "Iteration 8, loss = 0.07306883\n",
      "Iteration 8, loss = 0.64313823\n",
      "Iteration 8, loss = 0.65314476\n",
      "Iteration 8, loss = 0.63158898\n",
      "Iteration 13, loss = 0.04837343\n",
      "Iteration 17, loss = 0.60908440\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(200,), solver=sgd; total time= 4.3min\n",
      "Iteration 4, loss = 0.15833950\n",
      "Iteration 8, loss = 0.64234755\n",
      "Iteration 12, loss = 0.05107739\n",
      "Iteration 10, loss = 0.62694214\n",
      "Iteration 9, loss = 0.64547586\n",
      "Iteration 12, loss = 0.05152177\n",
      "Iteration 9, loss = 0.06467072\n",
      "Iteration 9, loss = 0.63729650\n",
      "Iteration 20, loss = 0.60286120\n",
      "Iteration 9, loss = 0.63668159\n",
      "Iteration 2, loss = 0.39155724\n",
      "Iteration 13, loss = 0.04809445\n",
      "Iteration 10, loss = 0.63225101\n",
      "Iteration 4, loss = 0.15887474\n",
      "Iteration 10, loss = 0.05846449\n",
      "Iteration 14, loss = 0.04565289\n",
      "Iteration 10, loss = 0.63879898\n",
      "Iteration 19, loss = 0.60190659\n",
      "Iteration 11, loss = 0.62337247\n",
      "Iteration 11, loss = 0.62782504\n",
      "Iteration 11, loss = 0.63308016\n",
      "Iteration 9, loss = 0.62772070\n",
      "Iteration 10, loss = 0.63189360\n",
      "Iteration 14, loss = 0.04545714\n",
      "Iteration 5, loss = 0.11170449\n",
      "Iteration 11, loss = 0.05383721\n",
      "Iteration 13, loss = 0.04843613\n",
      "Iteration 13, loss = 0.04801136\n",
      "Iteration 15, loss = 0.04336191\n",
      "Iteration 12, loss = 0.62400752\n",
      "Iteration 10, loss = 0.62423508\n",
      "Iteration 5, loss = 0.11225464\n",
      "Iteration 3, loss = 0.23741492\n",
      "Iteration 12, loss = 0.62809642\n",
      "Iteration 21, loss = 0.60170782\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(200,), solver=sgd; total time= 4.9min\n",
      "Iteration 12, loss = 0.62048604\n",
      "Iteration 12, loss = 0.05017553\n",
      "Iteration 20, loss = 0.60059234\n",
      "Iteration 13, loss = 0.62392502\n",
      "Iteration 14, loss = 0.04542123\n",
      "Iteration 15, loss = 0.04329239\n",
      "Iteration 6, loss = 0.08705868\n",
      "Iteration 11, loss = 0.62129260\n",
      "Iteration 16, loss = 0.04140407\n",
      "Iteration 1, loss = 0.61580027\n",
      "Iteration 1, loss = 0.59754854\n",
      "Iteration 13, loss = 0.62067501\n",
      "Iteration 6, loss = 0.08746131\n",
      "Iteration 14, loss = 0.04583398\n",
      "Iteration 11, loss = 0.62771367\n",
      "Iteration 14, loss = 0.62023656\n",
      "Iteration 4, loss = 0.15463261\n",
      "Iteration 21, loss = 0.59952866\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.001, hidden_layer_sizes=(200,), solver=sgd; total time= 5.2min\n",
      "Iteration 13, loss = 0.61784707\n",
      "Iteration 14, loss = 0.61777463\n",
      "Iteration 15, loss = 0.04324370\n",
      "Iteration 16, loss = 0.04139023\n",
      "Iteration 12, loss = 0.61872065\n",
      "Iteration 13, loss = 0.04714027\n",
      "Iteration 15, loss = 0.61711683\n",
      "Iteration 17, loss = 0.03971416\n",
      "Iteration 13, loss = 0.61636073\n",
      "Iteration 7, loss = 0.07233001\n",
      "Iteration 14, loss = 0.04469512\n",
      "Iteration 15, loss = 0.61529505\n",
      "Iteration 12, loss = 0.62408714\n",
      "Iteration 15, loss = 0.04370054\n",
      "Iteration 16, loss = 0.61428743\n",
      "Iteration 2, loss = 0.39856470\n",
      "Iteration 1, loss = 0.70055868\n",
      "Iteration 17, loss = 0.03978421\n",
      "Iteration 14, loss = 0.61441232\n",
      "Iteration 5, loss = 0.11019407\n",
      "Iteration 16, loss = 0.61309631\n",
      "Iteration 17, loss = 0.61203344\n",
      "Iteration 16, loss = 0.04181311\n",
      "Iteration 14, loss = 0.61561323\n",
      "Iteration 2, loss = 0.39141528\n",
      "Iteration 15, loss = 0.04257366\n",
      "Iteration 7, loss = 0.07285417\n",
      "Iteration 13, loss = 0.62087612\n",
      "Iteration 16, loss = 0.04132379\n",
      "Iteration 18, loss = 0.03833565\n",
      "Iteration 18, loss = 0.60991526\n",
      "Iteration 18, loss = 0.03817186\n",
      "Iteration 17, loss = 0.61115962\n",
      "Iteration 17, loss = 0.04018270\n",
      "Iteration 2, loss = 0.69068232\n",
      "Iteration 16, loss = 0.04075972\n",
      "Iteration 15, loss = 0.61259773\n",
      "Iteration 8, loss = 0.06303083\n",
      "Iteration 15, loss = 0.61361736\n",
      "Iteration 14, loss = 0.61821384\n",
      "Iteration 3, loss = 0.24385180\n",
      "Iteration 18, loss = 0.60951006\n",
      "Iteration 8, loss = 0.06351827\n",
      "Iteration 19, loss = 0.03703069\n",
      "Iteration 6, loss = 0.08597218\n",
      "Iteration 17, loss = 0.03966260\n",
      "Iteration 3, loss = 0.23895410\n",
      "Iteration 19, loss = 0.60805961\n",
      "Iteration 16, loss = 0.61191166\n",
      "Iteration 17, loss = 0.03919063\n",
      "Iteration 15, loss = 0.61587842\n",
      "Iteration 16, loss = 0.61105134\n",
      "Iteration 19, loss = 0.60823992\n",
      "Iteration 18, loss = 0.03872778\n",
      "Iteration 9, loss = 0.05651657\n",
      "Iteration 20, loss = 0.60667734\n",
      "Iteration 17, loss = 0.61032496\n",
      "Iteration 18, loss = 0.03777037\n",
      "Iteration 4, loss = 0.15634672\n",
      "Iteration 20, loss = 0.03590523\n",
      "Iteration 9, loss = 0.05708313\n",
      "Iteration 18, loss = 0.03826275\n",
      "Iteration 4, loss = 0.15298517\n",
      "Iteration 17, loss = 0.60967285\n",
      "Iteration 16, loss = 0.61384322\n",
      "Iteration 3, loss = 0.67986839\n",
      "Iteration 21, loss = 0.60546059\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), solver=sgd; total time= 2.1min\n",
      "Iteration 19, loss = 0.03682906\n",
      "Iteration 7, loss = 0.07158425\n",
      "Iteration 18, loss = 0.60898048\n",
      "Iteration 19, loss = 0.03650799\n",
      "Iteration 20, loss = 0.60661968\n",
      "Iteration 10, loss = 0.05167615\n",
      "Iteration 18, loss = 0.60843306\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), solver=sgd; total time= 2.3min\n",
      "Iteration 21, loss = 0.03481855\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), solver=adam; total time= 3.4min\n",
      "Iteration 1, loss = 0.71579709\n",
      "Iteration 19, loss = 0.60777860\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.10841774\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), solver=sgd; total time= 2.5min\n",
      "Iteration 5, loss = 0.11041817\n",
      "Iteration 20, loss = 0.03538202\n",
      "Iteration 17, loss = 0.61193527\n",
      "Iteration 4, loss = 0.67082227\n",
      "Iteration 10, loss = 0.05231083\n",
      "Iteration 19, loss = 0.03744057\n",
      "Iteration 19, loss = 0.03695627\n",
      "Iteration 18, loss = 0.61039368\n",
      "Iteration 20, loss = 0.03561511\n",
      "Iteration 6, loss = 0.08521091\n",
      "Iteration 21, loss = 0.60526259\n",
      "Iteration 8, loss = 0.06222104\n",
      "Iteration 2, loss = 0.70364611\n",
      "Iteration 21, loss = 0.03432941\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), solver=adam; total time= 2.9min\n",
      "Iteration 5, loss = 0.66257792\n",
      "Iteration 20, loss = 0.03625167\n",
      "Iteration 19, loss = 0.60903904\n",
      "Iteration 1, loss = 0.70372921\n",
      "Iteration 1, loss = 0.73408721\n",
      "Iteration 22, loss = 0.60402080\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), solver=sgd; total time= 2.6min\n",
      "Iteration 1, loss = 0.67267337\n",
      "Iteration 11, loss = 0.04802971\n",
      "Iteration 20, loss = 0.60770107\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), solver=sgd; total time= 2.8min\n",
      "Iteration 11, loss = 0.04855979\n",
      "Iteration 9, loss = 0.05571367\n",
      "Iteration 21, loss = 0.03451374\n",
      "Iteration 6, loss = 0.08437518\n",
      "Iteration 7, loss = 0.07091903\n",
      "Iteration 2, loss = 0.69320473\n",
      "Iteration 6, loss = 0.65530092\n",
      "Iteration 20, loss = 0.03575708\n",
      "Iteration 2, loss = 0.72129449\n",
      "Iteration 21, loss = 0.03523211\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), solver=adam; total time= 3.5min\n",
      "Iteration 3, loss = 0.69035600\n",
      "Iteration 1, loss = 0.58794054\n",
      "Iteration 22, loss = 0.03349779\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), solver=adam; total time= 3.9min\n",
      "Iteration 12, loss = 0.04557816\n",
      "Iteration 1, loss = 0.59248871\n",
      "Iteration 12, loss = 0.04498042\n",
      "Iteration 2, loss = 0.66595680\n",
      "Iteration 3, loss = 0.70687559\n",
      "Iteration 21, loss = 0.03468734\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(100,), solver=adam; total time= 3.7min\n",
      "Iteration 7, loss = 0.07045408\n",
      "Iteration 4, loss = 0.67824830\n",
      "Iteration 10, loss = 0.05104807\n",
      "Iteration 3, loss = 0.68155661\n",
      "Iteration 7, loss = 0.64896332\n",
      "Iteration 8, loss = 0.06152053\n",
      "Iteration 3, loss = 0.65886482\n",
      "Iteration 4, loss = 0.69299750\n",
      "Iteration 1, loss = 0.61447259\n",
      "Iteration 13, loss = 0.04305702\n",
      "Iteration 1, loss = 0.57711467\n",
      "Iteration 1, loss = 0.69798817\n",
      "Iteration 2, loss = 0.36606979\n",
      "Iteration 5, loss = 0.66756547\n",
      "Iteration 4, loss = 0.67105316\n",
      "Iteration 1, loss = 0.59824611\n",
      "Iteration 13, loss = 0.04249528\n",
      "Iteration 8, loss = 0.06125071\n",
      "Iteration 4, loss = 0.65223551\n",
      "Iteration 2, loss = 0.36167740\n",
      "Iteration 11, loss = 0.04734227\n",
      "Iteration 8, loss = 0.64340669\n",
      "Iteration 9, loss = 0.05516064\n",
      "Iteration 6, loss = 0.65801917\n",
      "Iteration 14, loss = 0.04099519\n",
      "Iteration 2, loss = 0.68856684\n",
      "Iteration 5, loss = 0.68076511\n",
      "Iteration 9, loss = 0.63836625\n",
      "Iteration 3, loss = 0.20852432\n",
      "Iteration 5, loss = 0.66149800\n",
      "Iteration 5, loss = 0.64639289\n",
      "Iteration 9, loss = 0.05491113\n",
      "Iteration 14, loss = 0.04044500\n",
      "Iteration 10, loss = 0.05046172\n",
      "Iteration 2, loss = 0.35757330\n",
      "Iteration 12, loss = 0.04443636\n",
      "Iteration 2, loss = 0.34491699\n",
      "Iteration 6, loss = 0.66970145\n",
      "Iteration 6, loss = 0.64133023\n",
      "Iteration 7, loss = 0.64985625\n",
      "Iteration 3, loss = 0.67879401\n",
      "Iteration 15, loss = 0.03865356\n",
      "Iteration 15, loss = 0.03919947\n",
      "Iteration 10, loss = 0.63399793\n",
      "Iteration 10, loss = 0.05024406\n",
      "Iteration 2, loss = 0.37079947\n",
      "Iteration 3, loss = 0.20191759\n",
      "Iteration 13, loss = 0.04203102\n",
      "Iteration 8, loss = 0.64285657\n",
      "Iteration 3, loss = 0.20183023\n",
      "Iteration 11, loss = 0.63011101\n",
      "Iteration 6, loss = 0.65327100\n",
      "Iteration 11, loss = 0.04685443\n",
      "Iteration 16, loss = 0.03759313\n",
      "Iteration 4, loss = 0.12918351\n",
      "Iteration 7, loss = 0.63669854\n",
      "Iteration 7, loss = 0.66030309\n",
      "Iteration 9, loss = 0.63674104\n",
      "Iteration 11, loss = 0.04663013\n",
      "Iteration 4, loss = 0.67000454\n",
      "Iteration 16, loss = 0.03710041\n",
      "Iteration 3, loss = 0.21027739\n",
      "Iteration 3, loss = 0.19483411\n",
      "Iteration 14, loss = 0.03994071\n",
      "Iteration 17, loss = 0.03621207\n",
      "Iteration 8, loss = 0.63262352\n",
      "Iteration 10, loss = 0.63154455\n",
      "Iteration 4, loss = 0.12435614\n",
      "Iteration 5, loss = 0.66247757\n",
      "Iteration 7, loss = 0.64589289\n",
      "Iteration 12, loss = 0.04392533\n",
      "Iteration 12, loss = 0.62656186\n",
      "Iteration 11, loss = 0.62706989\n",
      "Iteration 8, loss = 0.65207817\n",
      "Iteration 12, loss = 0.04372758\n",
      "Iteration 5, loss = 0.09230031\n",
      "Iteration 4, loss = 0.12585603\n",
      "Iteration 8, loss = 0.63967725\n",
      "Iteration 17, loss = 0.03570324\n",
      "Iteration 9, loss = 0.62899990\n",
      "Iteration 15, loss = 0.03823544\n",
      "Iteration 13, loss = 0.62364060\n",
      "Iteration 5, loss = 0.08867497\n",
      "Iteration 13, loss = 0.04156609\n",
      "Iteration 18, loss = 0.03495381\n",
      "Iteration 9, loss = 0.64511853\n",
      "Iteration 6, loss = 0.65587451\n",
      "Iteration 4, loss = 0.12108491\n",
      "Iteration 4, loss = 0.13089935\n",
      "Iteration 9, loss = 0.63421846\n",
      "Iteration 13, loss = 0.04136378\n",
      "Iteration 12, loss = 0.62327244\n",
      "Iteration 10, loss = 0.62576345\n",
      "Iteration 14, loss = 0.62087811\n",
      "Iteration 6, loss = 0.07343434\n",
      "Iteration 18, loss = 0.03447844\n",
      "Iteration 5, loss = 0.09044938\n",
      "Iteration 19, loss = 0.03381828\n",
      "Iteration 7, loss = 0.65009526\n",
      "Iteration 16, loss = 0.03672821\n",
      "Iteration 15, loss = 0.61865561\n",
      "Iteration 14, loss = 0.03954100\n",
      "Iteration 13, loss = 0.61998095\n",
      "Iteration 11, loss = 0.62282470\n",
      "Iteration 10, loss = 0.63911969\n",
      "Iteration 5, loss = 0.08643430\n",
      "Iteration 14, loss = 0.03931130\n",
      "Iteration 10, loss = 0.62948378\n",
      "Iteration 16, loss = 0.61657728\n",
      "Iteration 8, loss = 0.64477127\n",
      "Iteration 19, loss = 0.03334367\n",
      "Iteration 20, loss = 0.03281145\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), solver=adam; total time= 4.3min\n",
      "Iteration 12, loss = 0.62036544\n",
      "Iteration 14, loss = 0.61726014\n",
      "Iteration 11, loss = 0.63394254\n",
      "Iteration 6, loss = 0.07086719\n",
      "Iteration 17, loss = 0.03533238\n",
      "Iteration 15, loss = 0.03779030\n",
      "Iteration 15, loss = 0.61476755\n",
      "Iteration 17, loss = 0.61484620\n",
      "Iteration 11, loss = 0.62548597\n",
      "Iteration 5, loss = 0.09369942\n",
      "Iteration 7, loss = 0.06230363\n",
      "Iteration 15, loss = 0.03758971\n",
      "Iteration 12, loss = 0.62952506\n",
      "Iteration 6, loss = 0.07201323\n",
      "Iteration 13, loss = 0.61814763\n",
      "Iteration 9, loss = 0.64008589\n",
      "Iteration 6, loss = 0.06897916\n",
      "Iteration 18, loss = 0.61326213\n",
      "Iteration 16, loss = 0.61269414\n",
      "Iteration 20, loss = 0.03235761\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), solver=adam; total time= 4.6min\n",
      "Iteration 18, loss = 0.03414100\n",
      "Iteration 1, loss = 0.67189277\n",
      "Iteration 12, loss = 0.62206422\n",
      "Iteration 14, loss = 0.61621038\n",
      "Iteration 13, loss = 0.62571637\n",
      "Iteration 16, loss = 0.03610553\n",
      "Iteration 16, loss = 0.03631305\n",
      "Iteration 17, loss = 0.61091558\n",
      "Iteration 6, loss = 0.07461007\n",
      "Iteration 13, loss = 0.61906580\n",
      "Iteration 8, loss = 0.05506488\n",
      "Iteration 2, loss = 0.66558620\n",
      "Iteration 10, loss = 0.63598674\n",
      "Iteration 15, loss = 0.61448019\n",
      "Iteration 19, loss = 0.61193178\n",
      "Iteration 1, loss = 0.71139376\n",
      "Iteration 7, loss = 0.05870959\n",
      "Iteration 7, loss = 0.06037583\n",
      "Iteration 14, loss = 0.62234899\n",
      "Iteration 19, loss = 0.03312030\n",
      "Iteration 16, loss = 0.61291986\n",
      "Iteration 14, loss = 0.61655741\n",
      "Iteration 7, loss = 0.06125978\n",
      "Iteration 18, loss = 0.60935852\n",
      "Iteration 11, loss = 0.63237127\n",
      "Iteration 3, loss = 0.65882834\n",
      "Iteration 8, loss = 0.05190014\n",
      "Iteration 20, loss = 0.03209108\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), solver=adam; total time= 4.9min\n",
      "Iteration 17, loss = 0.03496348\n",
      "Iteration 2, loss = 0.70022307\n",
      "Iteration 17, loss = 0.03475596\n",
      "Iteration 19, loss = 0.60806152\n",
      "Iteration 8, loss = 0.05355823\n",
      "Iteration 17, loss = 0.61162767\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), solver=sgd; total time= 3.0min\n",
      "Iteration 20, loss = 0.61064870\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), solver=sgd; total time= 3.8min\n",
      "Iteration 15, loss = 0.61433593\n",
      "Iteration 7, loss = 0.06328461\n",
      "Iteration 9, loss = 0.04990413\n",
      "Iteration 15, loss = 0.61950333\n",
      "Iteration 8, loss = 0.05405912\n",
      "Iteration 12, loss = 0.62910717\n",
      "Iteration 16, loss = 0.61249257\n",
      "Iteration 1, loss = 0.72363259\n",
      "Iteration 20, loss = 0.60681456\n",
      "Iteration 1, loss = 0.70463687\n",
      "Iteration 18, loss = 0.03375206\n",
      "Iteration 21, loss = 0.60572973\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.03355650\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), solver=sgd; total time= 3.3min\n",
      "Iteration 3, loss = 0.68832951\n",
      "Iteration 13, loss = 0.62618519\n",
      "Iteration 2, loss = 0.71058385\n",
      "Iteration 4, loss = 0.65268243\n",
      "Iteration 9, loss = 0.04861976\n",
      "Iteration 9, loss = 0.04906497\n",
      "Iteration 16, loss = 0.61706768\n",
      "Iteration 19, loss = 0.03268359\n",
      "Iteration 8, loss = 0.05584083\n",
      "Iteration 4, loss = 0.67791533\n",
      "Iteration 17, loss = 0.61083174\n",
      "Iteration 5, loss = 0.64728572\n",
      "Iteration 14, loss = 0.62360726\n",
      "Iteration 10, loss = 0.04522853\n",
      "Iteration 10, loss = 0.04503158\n",
      "Iteration 9, loss = 0.04717507\n",
      "Iteration 17, loss = 0.61486939\n",
      "Iteration 5, loss = 0.66881007\n",
      "Iteration 10, loss = 0.04606812\n",
      "Iteration 19, loss = 0.03253824\n",
      "Iteration 20, loss = 0.03166075\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), solver=adam; total time= 4.9min\n",
      "Iteration 15, loss = 0.62134096\n",
      "Iteration 18, loss = 0.61298816\n",
      "Iteration 2, loss = 0.69474094\n",
      "Iteration 6, loss = 0.64276087\n",
      "Iteration 18, loss = 0.60937438\n",
      "Iteration 6, loss = 0.66078197\n",
      "Iteration 20, loss = 0.03152972\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), solver=adam; total time= 4.6min\n",
      "Iteration 19, loss = 0.61131818\n",
      "Iteration 11, loss = 0.04212962\n",
      "Iteration 7, loss = 0.65376852\n",
      "Iteration 20, loss = 0.60989423\n",
      "Iteration 3, loss = 0.68386662\n",
      "Iteration 7, loss = 0.63860921\n",
      "Iteration 3, loss = 0.69700904\n",
      "Iteration 12, loss = 0.03969709\n",
      "Iteration 21, loss = 0.60862057\n",
      "Iteration 8, loss = 0.64747933\n",
      "Iteration 11, loss = 0.04305757\n",
      "Iteration 4, loss = 0.67420296\n",
      "Iteration 22, loss = 0.60751334\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), solver=sgd; total time= 3.9min\n",
      "Iteration 9, loss = 0.64204441\n",
      "Iteration 5, loss = 0.66569710\n",
      "Iteration 13, loss = 0.03769707\n",
      "Iteration 16, loss = 0.61935553\n",
      "Iteration 9, loss = 0.05064753\n",
      "Iteration 12, loss = 0.04054266\n",
      "Iteration 10, loss = 0.63717356\n",
      "Iteration 6, loss = 0.65804312\n",
      "Iteration 19, loss = 0.60815753\n",
      "Iteration 14, loss = 0.03598792\n",
      "Iteration 11, loss = 0.63300966\n",
      "Iteration 7, loss = 0.65139377\n",
      "Iteration 13, loss = 0.03846394\n",
      "Iteration 11, loss = 0.04226278\n",
      "Iteration 15, loss = 0.03449366\n",
      "Iteration 12, loss = 0.62936686\n",
      "Iteration 8, loss = 0.64539742\n",
      "Iteration 10, loss = 0.04360001\n",
      "Iteration 20, loss = 0.60703824\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(150,), solver=sgd; total time= 4.1min\n",
      "Iteration 14, loss = 0.03667491\n",
      "Iteration 4, loss = 0.68518896\n",
      "Iteration 13, loss = 0.62617404\n",
      "Iteration 9, loss = 0.64025907\n",
      "Iteration 8, loss = 0.63502226\n",
      "Iteration 12, loss = 0.03981075\n",
      "Iteration 14, loss = 0.62333834\n",
      "Iteration 10, loss = 0.63574368\n",
      "Iteration 10, loss = 0.04676601\n",
      "Iteration 15, loss = 0.03508654\n",
      "Iteration 17, loss = 0.61754499\n",
      "Iteration 15, loss = 0.62097612\n",
      "Iteration 11, loss = 0.63177719\n",
      "Iteration 11, loss = 0.04357715\n",
      "Iteration 16, loss = 0.03370822\n",
      "Iteration 16, loss = 0.61890286\n",
      "Iteration 12, loss = 0.62837568\n",
      "Iteration 5, loss = 0.67473349\n",
      "Iteration 13, loss = 0.03778235\n",
      "Iteration 17, loss = 0.03248736\n",
      "Iteration 12, loss = 0.04108607\n",
      "Iteration 13, loss = 0.62544070\n",
      "Iteration 11, loss = 0.04078244\n",
      "Iteration 17, loss = 0.61701546\n",
      "Iteration 14, loss = 0.62278003\n",
      "Iteration 9, loss = 0.63185326\n",
      "Iteration 18, loss = 0.03140908\n",
      "Iteration 13, loss = 0.03894995\n",
      "Iteration 16, loss = 0.03316223\n",
      "Iteration 18, loss = 0.61541043\n",
      "Iteration 15, loss = 0.62052711\n",
      "Iteration 14, loss = 0.03718238\n",
      "Iteration 19, loss = 0.03039726\n",
      "Iteration 18, loss = 0.61601069\n",
      "Iteration 6, loss = 0.66574189\n",
      "Iteration 16, loss = 0.61846893\n",
      "Iteration 19, loss = 0.61404243\n",
      "Iteration 15, loss = 0.03557301\n",
      "Iteration 20, loss = 0.02947333\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(200,), solver=adam; total time= 4.7min\n",
      "Iteration 17, loss = 0.61668607\n",
      "Iteration 20, loss = 0.61271296\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(200,), solver=sgd; total time= 2.4min\n",
      "Iteration 14, loss = 0.03604300\n",
      "Iteration 12, loss = 0.03847082\n",
      "Iteration 18, loss = 0.61513216\n",
      "Iteration 16, loss = 0.03418666\n",
      "Iteration 10, loss = 0.62899914\n",
      "Iteration 15, loss = 0.03450659\n",
      "Iteration 13, loss = 0.03653517\n",
      "Iteration 19, loss = 0.61372063\n",
      "Iteration 19, loss = 0.61454933\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(200,), solver=sgd; total time= 4.5min\n",
      "Iteration 7, loss = 0.65779891\n",
      "Iteration 17, loss = 0.03296863\n",
      "Iteration 17, loss = 0.03199604\n",
      "Iteration 16, loss = 0.03319568\n",
      "Iteration 20, loss = 0.61245534\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(200,), solver=sgd; total time= 2.1min\n",
      "Iteration 14, loss = 0.03490268\n",
      "Iteration 11, loss = 0.62649714\n",
      "Iteration 8, loss = 0.65098666\n",
      "Iteration 18, loss = 0.03183912\n",
      "Iteration 18, loss = 0.03089333\n",
      "Iteration 15, loss = 0.03341422\n",
      "Iteration 9, loss = 0.64488635\n",
      "Iteration 17, loss = 0.03204204\n",
      "Iteration 12, loss = 0.62426740\n",
      "Iteration 19, loss = 0.03080261\n",
      "Iteration 10, loss = 0.63975424\n",
      "Iteration 16, loss = 0.03217308\n",
      "Iteration 18, loss = 0.03096966\n",
      "Iteration 11, loss = 0.63501542\n",
      "Iteration 20, loss = 0.02999221\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(200,), solver=adam; total time= 5.0min\n",
      "Iteration 19, loss = 0.02995285\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(200,), solver=adam; total time= 5.2min\n",
      "Iteration 17, loss = 0.03107064\n",
      "Iteration 13, loss = 0.62231609\n",
      "Iteration 12, loss = 0.63118814\n",
      "Iteration 19, loss = 0.02997645\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(200,), solver=adam; total time= 5.0min\n",
      "Iteration 14, loss = 0.62050980\n",
      "Iteration 18, loss = 0.03003718\n",
      "Iteration 13, loss = 0.62769172\n",
      "Iteration 15, loss = 0.61894427\n",
      "Iteration 14, loss = 0.62464256\n",
      "Iteration 19, loss = 0.02909348\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(200,), solver=adam; total time= 5.3min\n",
      "Iteration 16, loss = 0.61752674\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(200,), solver=sgd; total time= 3.4min\n",
      "Iteration 15, loss = 0.62194594\n",
      "Iteration 16, loss = 0.61962813\n",
      "Iteration 17, loss = 0.61757871\n",
      "Iteration 18, loss = 0.61576449\n",
      "Iteration 19, loss = 0.61415611\n",
      "Iteration 20, loss = 0.61274116\n",
      "Iteration 21, loss = 0.61150941\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "[CV] END activation=relu, alpha=0.01, hidden_layer_sizes=(200,), solver=sgd; total time= 2.9min\n",
      "Iteration 1, loss = 0.59092744\n",
      "Iteration 2, loss = 0.30693590\n",
      "Iteration 3, loss = 0.15832387\n",
      "Iteration 4, loss = 0.09936871\n",
      "Iteration 5, loss = 0.07442710\n",
      "Iteration 6, loss = 0.06128656\n",
      "Iteration 7, loss = 0.05336664\n",
      "Iteration 8, loss = 0.04803583\n",
      "Iteration 9, loss = 0.04407544\n",
      "Iteration 10, loss = 0.04097085\n",
      "Iteration 11, loss = 0.03844476\n",
      "Iteration 12, loss = 0.03637572\n",
      "Iteration 13, loss = 0.03465410\n",
      "Iteration 14, loss = 0.03307949\n",
      "Iteration 15, loss = 0.03180734\n",
      "Iteration 16, loss = 0.03060740\n",
      "Iteration 17, loss = 0.02954696\n",
      "Iteration 18, loss = 0.02850016\n",
      "Iteration 19, loss = 0.02762940\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Best parameters found:  {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (200,), 'solver': 'adam'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9922705314009662,\n",
       " 0.9863481228668942,\n",
       " 0.9863481228668942,\n",
       " 0.9863481228668942)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,), (200,)],\n",
    "    'activation': ['logistic', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "clf = MLPClassifier(max_iter=100, tol=0.005, verbose=True)\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "best_clf = grid_search.best_estimator_\n",
    "mlp_y_pred = best_clf.predict(X_test_tfidf)\n",
    "\n",
    "# Calcolare le metriche di valutazione\n",
    "mlp_accuracy = accuracy_score(y_test, mlp_y_pred)\n",
    "mlp_precision = precision_score(y_test, mlp_y_pred)\n",
    "mlp_recall = recall_score(y_test, mlp_y_pred)\n",
    "mlp_f1 = f1_score(y_test, mlp_y_pred)\n",
    "\n",
    "classification_report(y_test, mlp_y_pred)\n",
    "\n",
    "mlp_accuracy, mlp_precision, mlp_recall, mlp_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f621ab6c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlp_conf_matrix = confusion_matrix(y_test, mlp_y_pred)\n",
    "sns.heatmap(mlp_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Ham\", \"Spam\"], yticklabels=[\"Ham\", \"Spam\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - MLPClassifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67595358",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.73188595\n",
      "Iteration 1, loss = 0.70649115\n",
      "Iteration 1, loss = 0.66524857\n",
      "Iteration 1, loss = 0.71370742\n",
      "Iteration 2, loss = 0.68636560\n",
      "Iteration 2, loss = 0.66481899\n",
      "Iteration 2, loss = 0.62791409\n",
      "Iteration 2, loss = 0.67678224\n",
      "Iteration 1, loss = 0.66561659\n",
      "Iteration 1, loss = 0.66930175\n",
      "Iteration 3, loss = 0.64270386\n",
      "Iteration 1, loss = 0.67253736\n",
      "Iteration 3, loss = 0.62576355\n",
      "Iteration 3, loss = 0.59389053\n",
      "Iteration 3, loss = 0.64018282\n",
      "Iteration 4, loss = 0.59789416\n",
      "Iteration 4, loss = 0.58613051\n",
      "Iteration 4, loss = 0.55827010\n",
      "Iteration 4, loss = 0.60136402\n",
      "Iteration 1, loss = 0.67213611\n",
      "Iteration 5, loss = 0.55389998\n",
      "Iteration 1, loss = 0.67418530\n",
      "Iteration 5, loss = 0.54599329\n",
      "Iteration 1, loss = 0.64912270\n",
      "Iteration 5, loss = 0.52152394\n",
      "Iteration 5, loss = 0.56031818\n",
      "Iteration 2, loss = 0.57346477\n",
      "Iteration 2, loss = 0.57536245\n",
      "Iteration 6, loss = 0.51082209\n",
      "Iteration 2, loss = 0.56915422\n",
      "Iteration 1, loss = 0.60765943\n",
      "Iteration 6, loss = 0.50585998\n",
      "Iteration 6, loss = 0.48444668\n",
      "Iteration 6, loss = 0.51890742\n",
      "Iteration 1, loss = 0.62284691\n",
      "Iteration 1, loss = 0.64336051\n",
      "Iteration 7, loss = 0.46973486\n",
      "Iteration 7, loss = 0.46620881\n",
      "Iteration 7, loss = 0.44744900\n",
      "Iteration 7, loss = 0.47846010\n",
      "Iteration 1, loss = 0.60410471\n",
      "Iteration 8, loss = 0.42816520\n",
      "Iteration 8, loss = 0.43086218\n",
      "Iteration 1, loss = 0.59818251\n",
      "Iteration 8, loss = 0.41155973\n",
      "Iteration 8, loss = 0.43894230\n",
      "Iteration 3, loss = 0.48172476\n",
      "Iteration 1, loss = 0.61964075\n",
      "Iteration 9, loss = 0.39181863\n",
      "Iteration 9, loss = 0.39360617\n",
      "Iteration 3, loss = 0.47470186\n",
      "Iteration 9, loss = 0.37674462\n",
      "Iteration 10, loss = 0.34351091\n",
      "Iteration 3, loss = 0.47076787\n",
      "Iteration 10, loss = 0.35870452\n",
      "Iteration 2, loss = 0.52648537\n",
      "Iteration 11, loss = 0.31252207\n",
      "Iteration 9, loss = 0.40070012\n",
      "Iteration 10, loss = 0.35739283\n",
      "Iteration 2, loss = 0.41795475\n",
      "Iteration 2, loss = 0.50304016\n",
      "Iteration 2, loss = 0.50259893\n",
      "Iteration 4, loss = 0.38316172\n",
      "Iteration 11, loss = 0.32636914\n",
      "Iteration 11, loss = 0.32501655\n",
      "Iteration 10, loss = 0.36472568\n",
      "Iteration 4, loss = 0.39227386\n",
      "Iteration 12, loss = 0.28347806\n",
      "Iteration 4, loss = 0.38012372\n",
      "Iteration 13, loss = 0.25647974\n",
      "Iteration 12, loss = 0.29635936\n",
      "Iteration 12, loss = 0.29505079\n",
      "Iteration 2, loss = 0.43166136\n",
      "Iteration 14, loss = 0.23203146\n",
      "Iteration 13, loss = 0.26742787\n",
      "Iteration 13, loss = 0.26888747\n",
      "Iteration 15, loss = 0.20991362\n",
      "Iteration 2, loss = 0.44427501\n",
      "Iteration 11, loss = 0.33098905\n",
      "Iteration 14, loss = 0.24385842\n",
      "Iteration 14, loss = 0.24203127\n",
      "Iteration 5, loss = 0.30215976\n",
      "Iteration 12, loss = 0.29951016\n",
      "Iteration 15, loss = 0.22128059\n",
      "Iteration 15, loss = 0.21921830\n",
      "Iteration 16, loss = 0.19015886\n",
      "Iteration 3, loss = 0.38653918\n",
      "Iteration 13, loss = 0.27074180\n",
      "Iteration 5, loss = 0.30494558\n",
      "Iteration 3, loss = 0.36337690\n",
      "Iteration 3, loss = 0.26903462\n",
      "Iteration 2, loss = 0.36811105\n",
      "Iteration 5, loss = 0.31327180\n",
      "Iteration 16, loss = 0.19850716\n",
      "Iteration 6, loss = 0.23835135\n",
      "Iteration 16, loss = 0.20096791\n",
      "Iteration 14, loss = 0.24455780\n",
      "Iteration 2, loss = 0.36425897\n",
      "Iteration 17, loss = 0.18303372\n",
      "Iteration 17, loss = 0.18008785\n",
      "Iteration 17, loss = 0.17249718\n",
      "Iteration 15, loss = 0.22090493\n",
      "Iteration 3, loss = 0.35750099\n",
      "Iteration 16, loss = 0.19976626\n",
      "Iteration 2, loss = 0.37230413\n",
      "Iteration 7, loss = 0.18861662\n",
      "Iteration 3, loss = 0.27723541\n",
      "Iteration 18, loss = 0.16685391\n",
      "Iteration 6, loss = 0.24743595\n",
      "Iteration 18, loss = 0.16380943\n",
      "Iteration 4, loss = 0.25421199\n",
      "Iteration 17, loss = 0.18093577\n",
      "Iteration 18, loss = 0.15693033\n",
      "Iteration 19, loss = 0.15274585\n",
      "Iteration 3, loss = 0.28057990\n",
      "Iteration 6, loss = 0.24209003\n",
      "Iteration 19, loss = 0.14942339\n",
      "Iteration 4, loss = 0.27403302\n",
      "Iteration 20, loss = 0.14019321\n",
      "Iteration 20, loss = 0.13677489\n",
      "Iteration 18, loss = 0.16436486\n",
      "Iteration 8, loss = 0.15173951\n",
      "Iteration 19, loss = 0.14306442\n",
      "Iteration 21, loss = 0.12920460\n",
      "Iteration 21, loss = 0.12553360\n",
      "Iteration 19, loss = 0.14971706\n",
      "Iteration 20, loss = 0.13104155\n",
      "Iteration 7, loss = 0.19354208\n",
      "Iteration 4, loss = 0.17475783\n",
      "Iteration 20, loss = 0.13693466\n",
      "Iteration 22, loss = 0.11923769\n",
      "Iteration 22, loss = 0.11579964\n",
      "Iteration 9, loss = 0.12446996\n",
      "Iteration 23, loss = 0.11067701\n",
      "Iteration 21, loss = 0.12049500\n",
      "Iteration 7, loss = 0.19581239\n",
      "Iteration 5, loss = 0.17979968\n",
      "Iteration 23, loss = 0.10728997\n",
      "Iteration 24, loss = 0.10306544\n",
      "Iteration 22, loss = 0.11109941\n",
      "Iteration 3, loss = 0.20587918\n",
      "Iteration 8, loss = 0.15721644\n",
      "Iteration 5, loss = 0.19513453\n",
      "Iteration 24, loss = 0.09972499\n",
      "Iteration 21, loss = 0.12565457\n",
      "Iteration 23, loss = 0.10292735\n",
      "Iteration 4, loss = 0.25058086\n",
      "Iteration 25, loss = 0.09637527\n",
      "Iteration 4, loss = 0.17864833\n",
      "Iteration 25, loss = 0.09302515\n",
      "Iteration 3, loss = 0.20851021\n",
      "Iteration 26, loss = 0.09035687\n",
      "Iteration 24, loss = 0.09577399\n",
      "Iteration 22, loss = 0.11578667\n",
      "Iteration 26, loss = 0.08721034\n",
      "Iteration 10, loss = 0.10475827\n",
      "Iteration 25, loss = 0.08948772\n",
      "Iteration 23, loss = 0.10719007\n",
      "Iteration 27, loss = 0.08214189\n",
      "Iteration 26, loss = 0.08388276\n",
      "Iteration 4, loss = 0.17880292\n",
      "Iteration 27, loss = 0.07905360\n",
      "Iteration 24, loss = 0.09964856\n",
      "Iteration 27, loss = 0.08502692\n",
      "Iteration 6, loss = 0.13272907\n",
      "Iteration 28, loss = 0.07766601\n",
      "Iteration 11, loss = 0.09025754\n",
      "Iteration 28, loss = 0.07466149\n",
      "Iteration 25, loss = 0.09297722\n",
      "Iteration 5, loss = 0.18044160\n",
      "Iteration 29, loss = 0.07353289\n",
      "Iteration 6, loss = 0.14408378\n",
      "Iteration 26, loss = 0.08720181\n",
      "Iteration 5, loss = 0.12230718\n",
      "Iteration 3, loss = 0.21004477\n",
      "Iteration 30, loss = 0.07005245\n",
      "Iteration 8, loss = 0.15726552\n",
      "Iteration 27, loss = 0.08200251\n",
      "Iteration 31, loss = 0.06678954\n",
      "Iteration 9, loss = 0.13000587\n",
      "Iteration 29, loss = 0.07086041\n",
      "Iteration 28, loss = 0.07753139\n",
      "Iteration 28, loss = 0.08038160\n",
      "Iteration 32, loss = 0.06395854\n",
      "Iteration 5, loss = 0.12383685\n",
      "Iteration 4, loss = 0.12637047\n",
      "Iteration 12, loss = 0.07952564\n",
      "Iteration 33, loss = 0.06138297\n",
      "Iteration 9, loss = 0.12879388\n",
      "Iteration 30, loss = 0.06749583\n",
      "Iteration 29, loss = 0.07349672\n",
      "Iteration 34, loss = 0.05907877\n",
      "Iteration 29, loss = 0.07626862\n",
      "Iteration 35, loss = 0.05695946\n",
      "Iteration 7, loss = 0.11181118\n",
      "Iteration 6, loss = 0.13560544\n",
      "Iteration 30, loss = 0.07254658\n",
      "Iteration 4, loss = 0.13010522\n",
      "Iteration 36, loss = 0.05507629\n",
      "Iteration 10, loss = 0.10991419\n",
      "Iteration 31, loss = 0.06438370\n",
      "Iteration 30, loss = 0.06992561\n",
      "Iteration 13, loss = 0.07135202\n",
      "Iteration 10, loss = 0.10815372\n",
      "Iteration 31, loss = 0.06912001\n",
      "Iteration 37, loss = 0.05331562\n",
      "Iteration 32, loss = 0.06620627\n",
      "Iteration 32, loss = 0.06161687\n",
      "Iteration 33, loss = 0.06352182\n",
      "Iteration 31, loss = 0.06677890\n",
      "Iteration 6, loss = 0.09285325\n",
      "Iteration 11, loss = 0.09464868\n",
      "Iteration 7, loss = 0.10341382\n",
      "Iteration 38, loss = 0.05175655\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 32, loss = 0.06392509\n",
      "Iteration 33, loss = 0.05920181\n",
      "Iteration 34, loss = 0.06109624\n",
      "Iteration 11, loss = 0.09314075\n",
      "Iteration 33, loss = 0.06137707\n",
      "Iteration 4, loss = 0.12983711\n",
      "Iteration 5, loss = 0.12303108\n",
      "Iteration 34, loss = 0.05698435\n",
      "Iteration 14, loss = 0.06499491\n",
      "Iteration 34, loss = 0.05907422\n",
      "Iteration 35, loss = 0.05888305\n",
      "Iteration 35, loss = 0.05495621\n",
      "Iteration 1, loss = 0.67507965\n",
      "Iteration 8, loss = 0.09151370\n",
      "Iteration 36, loss = 0.05682734\n",
      "Iteration 12, loss = 0.08352922\n",
      "Iteration 6, loss = 0.09350769\n",
      "Iteration 35, loss = 0.05696694\n",
      "Iteration 36, loss = 0.05316434\n",
      "Iteration 7, loss = 0.10737806\n",
      "Iteration 12, loss = 0.08190392\n",
      "Iteration 8, loss = 0.08493379\n",
      "Iteration 37, loss = 0.05503214\n",
      "Iteration 36, loss = 0.05506096\n",
      "Iteration 5, loss = 0.09334205\n",
      "Iteration 2, loss = 0.58433583\n",
      "Iteration 38, loss = 0.05332155\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 37, loss = 0.05337937\n",
      "Iteration 5, loss = 0.08998323\n",
      "Iteration 37, loss = 0.05144209\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 38, loss = 0.05179121\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 15, loss = 0.06011772\n",
      "Iteration 13, loss = 0.07471987\n",
      "Iteration 13, loss = 0.07344763\n",
      "Iteration 8, loss = 0.08876500\n",
      "Iteration 3, loss = 0.49150384\n",
      "Iteration 14, loss = 0.06801899\n",
      "Iteration 16, loss = 0.05608294\n",
      "Iteration 5, loss = 0.09185746\n",
      "Iteration 7, loss = 0.07634660\n",
      "Iteration 7, loss = 0.07607718\n",
      "Iteration 9, loss = 0.07266644\n",
      "Iteration 14, loss = 0.06701903\n",
      "Iteration 1, loss = 0.66069460\n",
      "Iteration 9, loss = 0.07788500\n",
      "Iteration 6, loss = 0.09319676\n",
      "Iteration 1, loss = 0.65736553\n",
      "Iteration 17, loss = 0.05280748\n",
      "Iteration 9, loss = 0.07642391\n",
      "Iteration 6, loss = 0.07152604\n",
      "Iteration 15, loss = 0.06245715\n",
      "Iteration 6, loss = 0.07418335Iteration 4, loss = 0.40183513\n",
      "\n",
      "Iteration 15, loss = 0.06187147\n",
      "Iteration 16, loss = 0.05820448\n",
      "Iteration 7, loss = 0.07608660\n",
      "Iteration 8, loss = 0.06527624\n",
      "Iteration 18, loss = 0.05004143\n",
      "Iteration 2, loss = 0.46085601\n",
      "Iteration 10, loss = 0.06832979\n",
      "Iteration 10, loss = 0.06747754\n",
      "Iteration 2, loss = 0.52642760\n",
      "Iteration 8, loss = 0.06553003\n",
      "Iteration 5, loss = 0.32111007\n",
      "Iteration 19, loss = 0.04759427\n",
      "Iteration 10, loss = 0.06407159\n",
      "Iteration 16, loss = 0.05783954\n",
      "Iteration 1, loss = 0.60556403\n",
      "Iteration 7, loss = 0.06285577\n",
      "Iteration 6, loss = 0.07256287\n",
      "Iteration 6, loss = 0.25325599\n",
      "Iteration 20, loss = 0.04554983\n",
      "Iteration 7, loss = 0.06066082\n",
      "Iteration 8, loss = 0.06546276\n",
      "Iteration 9, loss = 0.05790093\n",
      "Iteration 11, loss = 0.06100030\n",
      "Iteration 11, loss = 0.06156206\n",
      "Iteration 17, loss = 0.05449822\n",
      "Iteration 7, loss = 0.19992421\n",
      "Iteration 17, loss = 0.05438694\n",
      "Iteration 11, loss = 0.05789001\n",
      "Iteration 21, loss = 0.04373496\n",
      "Iteration 3, loss = 0.29544205\n",
      "Iteration 3, loss = 0.39615785\n",
      "Iteration 18, loss = 0.05151499\n",
      "Iteration 9, loss = 0.05820936\n",
      "Iteration 18, loss = 0.05148694\n",
      "Iteration 12, loss = 0.05299592\n",
      "Iteration 8, loss = 0.15958617\n",
      "Iteration 22, loss = 0.04212119\n",
      "Iteration 19, loss = 0.04888531\n",
      "Iteration 12, loss = 0.05598241\n",
      "Iteration 8, loss = 0.05555208\n",
      "Iteration 19, loss = 0.04909664\n",
      "Iteration 12, loss = 0.05629384\n",
      "Iteration 9, loss = 0.05821558\n",
      "Iteration 20, loss = 0.04658708\n",
      "Iteration 13, loss = 0.04928306\n",
      "Iteration 10, loss = 0.05261735\n",
      "Iteration 20, loss = 0.04700022\n",
      "Iteration 8, loss = 0.05371409\n",
      "Iteration 2, loss = 0.36484784\n",
      "Iteration 9, loss = 0.13002536\n",
      "Iteration 4, loss = 0.28430155\n",
      "Iteration 21, loss = 0.04514375\n",
      "Iteration 13, loss = 0.05195533\n",
      "Iteration 21, loss = 0.04466147\n",
      "Iteration 10, loss = 0.05288212\n",
      "Iteration 7, loss = 0.06140648\n",
      "Iteration 14, loss = 0.04626583\n",
      "Iteration 10, loss = 0.10864008\n",
      "Iteration 23, loss = 0.04070638\n",
      "Iteration 22, loss = 0.04293273\n",
      "Iteration 10, loss = 0.05293573\n",
      "Iteration 13, loss = 0.05214000\n",
      "Iteration 11, loss = 0.09316720\n",
      "Iteration 22, loss = 0.04349743\n",
      "Iteration 4, loss = 0.18925619\n",
      "Iteration 14, loss = 0.04862584\n",
      "Iteration 24, loss = 0.03943811\n",
      "Iteration 3, loss = 0.20887900\n",
      "Iteration 5, loss = 0.20223781\n",
      "Iteration 9, loss = 0.04867965\n",
      "Iteration 9, loss = 0.05032751\n",
      "Iteration 15, loss = 0.04374278\n",
      "Iteration 23, loss = 0.04205480\n",
      "Iteration 11, loss = 0.04884726\n",
      "Iteration 14, loss = 0.04874621\n",
      "Iteration 11, loss = 0.04862125\n",
      "Iteration 25, loss = 0.03827066\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 8, loss = 0.05422008\n",
      "Iteration 15, loss = 0.04583726\n",
      "Iteration 23, loss = 0.04138553\n",
      "Iteration 12, loss = 0.08173369\n",
      "Iteration 5, loss = 0.13010578\n",
      "Iteration 1, loss = 0.67291745\n",
      "Iteration 24, loss = 0.04069132\n",
      "Iteration 24, loss = 0.04002302\n",
      "Iteration 6, loss = 0.14862311\n",
      "Iteration 11, loss = 0.04890624\n",
      "Iteration 16, loss = 0.04154288\n",
      "Iteration 13, loss = 0.07318480\n",
      "Iteration 15, loss = 0.04586381\n",
      "Iteration 2, loss = 0.63602734\n",
      "Iteration 16, loss = 0.04344456\n",
      "Iteration 3, loss = 0.60242128\n",
      "Iteration 12, loss = 0.04536689\n",
      "Iteration 12, loss = 0.04564582\n",
      "Iteration 4, loss = 0.56715385\n",
      "Iteration 4, loss = 0.13047307\n",
      "Iteration 14, loss = 0.06656398\n",
      "Iteration 10, loss = 0.04491954\n",
      "Iteration 25, loss = 0.03949116\n",
      "Iteration 5, loss = 0.53088563\n",
      "Iteration 16, loss = 0.04340666\n",
      "Iteration 10, loss = 0.04632145\n",
      "Iteration 6, loss = 0.09810525\n",
      "Iteration 15, loss = 0.06138233\n",
      "Iteration 6, loss = 0.49356328\n",
      "Iteration 25, loss = 0.03879941\n",
      "Iteration 26, loss = 0.03837895\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 7, loss = 0.11451316\n",
      "Iteration 9, loss = 0.04911620\n",
      "Iteration 7, loss = 0.45772905\n",
      "Iteration 16, loss = 0.05717143\n",
      "Iteration 17, loss = 0.03972353\n",
      "Iteration 8, loss = 0.42208697\n",
      "Iteration 17, loss = 0.04134473\n",
      "Iteration 9, loss = 0.38761358\n",
      "Iteration 17, loss = 0.05374989\n",
      "Iteration 1, loss = 0.70745324\n",
      "Iteration 10, loss = 0.35529017\n",
      "Iteration 12, loss = 0.04572690\n",
      "Iteration 17, loss = 0.04146849\n",
      "Iteration 11, loss = 0.32415369\n",
      "Iteration 13, loss = 0.04276239\n",
      "Iteration 7, loss = 0.07983118\n",
      "Iteration 18, loss = 0.05084564\n",
      "Iteration 13, loss = 0.04303522\n",
      "Iteration 8, loss = 0.09292115\n",
      "Iteration 18, loss = 0.03966924\n",
      "Iteration 5, loss = 0.09327358\n",
      "Iteration 26, loss = 0.03769299\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 12, loss = 0.29512569\n",
      "Iteration 2, loss = 0.59439928\n",
      "Iteration 11, loss = 0.04199460\n",
      "Iteration 19, loss = 0.04838454\n",
      "Iteration 18, loss = 0.03810774\n",
      "Iteration 18, loss = 0.03971535\n",
      "Iteration 14, loss = 0.04084105\n",
      "Iteration 8, loss = 0.06839796\n",
      "Iteration 11, loss = 0.04322207\n",
      "Iteration 13, loss = 0.26816087\n",
      "Iteration 19, loss = 0.03813114\n",
      "Iteration 14, loss = 0.04053841\n",
      "Iteration 20, loss = 0.04625690\n",
      "Iteration 19, loss = 0.03656611\n",
      "Iteration 9, loss = 0.07900916\n",
      "Iteration 13, loss = 0.04309201\n",
      "Iteration 19, loss = 0.03814725\n",
      "Iteration 14, loss = 0.24331878\n",
      "Iteration 3, loss = 0.48444031\n",
      "Iteration 10, loss = 0.04533448\n",
      "Iteration 21, loss = 0.04440434\n",
      "Iteration 15, loss = 0.22058022\n",
      "Iteration 6, loss = 0.07407323\n",
      "Iteration 1, loss = 0.67744567\n",
      "Iteration 12, loss = 0.03958414\n",
      "Iteration 4, loss = 0.38479375\n",
      "Iteration 10, loss = 0.06914861\n",
      "Iteration 16, loss = 0.20032013\n",
      "Iteration 20, loss = 0.03669884\n",
      "Iteration 20, loss = 0.03676115\n",
      "Iteration 15, loss = 0.03900805\n",
      "Iteration 17, loss = 0.18203825\n",
      "Iteration 9, loss = 0.06061940\n",
      "Iteration 5, loss = 0.30154034\n",
      "Iteration 18, loss = 0.16562233\n",
      "Iteration 22, loss = 0.04273034\n",
      "Iteration 19, loss = 0.15130094\n",
      "Iteration 12, loss = 0.04066964\n",
      "Iteration 15, loss = 0.03869746\n",
      "Iteration 2, loss = 0.51438700\n",
      "Iteration 6, loss = 0.23564210\n",
      "Iteration 21, loss = 0.03551220\n",
      "Iteration 20, loss = 0.03539537\n",
      "Iteration 14, loss = 0.04089181\n",
      "Iteration 20, loss = 0.13858651\n",
      "Iteration 21, loss = 0.12739746\n",
      "Iteration 7, loss = 0.06308301\n",
      "Iteration 21, loss = 0.03544208\n",
      "Iteration 22, loss = 0.11743464\n",
      "Iteration 7, loss = 0.18521898\n",
      "Iteration 11, loss = 0.06204583\n",
      "Iteration 23, loss = 0.04128073\n",
      "Iteration 11, loss = 0.04231395\n",
      "Iteration 10, loss = 0.05507766\n",
      "Iteration 16, loss = 0.03737330\n",
      "Iteration 23, loss = 0.10871813\n",
      "Iteration 22, loss = 0.03435533\n",
      "Iteration 3, loss = 0.37002899\n",
      "Iteration 21, loss = 0.03424341\n",
      "Iteration 22, loss = 0.03438221\n",
      "Iteration 24, loss = 0.03995792\n",
      "Iteration 24, loss = 0.10112094\n",
      "Iteration 25, loss = 0.09435154\n",
      "Iteration 15, loss = 0.03906515\n",
      "Iteration 8, loss = 0.14859215\n",
      "Iteration 23, loss = 0.03329919\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.03756875\n",
      "Iteration 26, loss = 0.08843078\n",
      "Iteration 23, loss = 0.03339996\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.03856252\n",
      "Iteration 11, loss = 0.05079614\n",
      "Iteration 9, loss = 0.12189708\n",
      "Iteration 22, loss = 0.03318924\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 27, loss = 0.08319570\n",
      "Iteration 8, loss = 0.05563468\n",
      "Iteration 16, loss = 0.03706740\n",
      "Iteration 12, loss = 0.03986405\n",
      "Iteration 28, loss = 0.07853850\n",
      "Iteration 25, loss = 0.03874816\n",
      "Iteration 4, loss = 0.26088509\n",
      "Iteration 10, loss = 0.10266694\n",
      "Iteration 29, loss = 0.07444813\n",
      "Iteration 12, loss = 0.05668353\n",
      "Iteration 30, loss = 0.07081982\n",
      "Iteration 17, loss = 0.03595663\n",
      "Iteration 11, loss = 0.08839969\n",
      "Iteration 14, loss = 0.03585113\n",
      "Iteration 16, loss = 0.03742217\n",
      "Iteration 26, loss = 0.03764950\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 31, loss = 0.06752036\n",
      "Iteration 14, loss = 0.03675427\n",
      "Iteration 12, loss = 0.04741366\n",
      "Iteration 1, loss = 0.61511519\n",
      "Iteration 32, loss = 0.06462239\n",
      "Iteration 33, loss = 0.06197742\n",
      "Iteration 12, loss = 0.07813860\n",
      "Iteration 13, loss = 0.05237174\n",
      "Iteration 13, loss = 0.03776702\n",
      "Iteration 34, loss = 0.05963072\n",
      "Iteration 17, loss = 0.03561739\n",
      "Iteration 1, loss = 0.60749539\n",
      "Iteration 13, loss = 0.04464434\n",
      "Iteration 9, loss = 0.05051625\n",
      "Iteration 5, loss = 0.18702925\n",
      "Iteration 35, loss = 0.05742183\n",
      "Iteration 2, loss = 0.42832567\n",
      "Iteration 15, loss = 0.03433638\n",
      "Iteration 17, loss = 0.03598033\n",
      "Iteration 13, loss = 0.07023687\n",
      "Iteration 18, loss = 0.03471556\n",
      "Iteration 15, loss = 0.03521271\n",
      "Iteration 14, loss = 0.04896658\n",
      "Iteration 36, loss = 0.05550950\n",
      "Iteration 6, loss = 0.13977809\n",
      "Iteration 18, loss = 0.03432194\n",
      "Iteration 14, loss = 0.06408935\n",
      "Iteration 14, loss = 0.03593028\n",
      "Iteration 3, loss = 0.27491126\n",
      "Iteration 37, loss = 0.05372367\n",
      "Iteration 15, loss = 0.04606203\n",
      "Iteration 15, loss = 0.05927055\n",
      "Iteration 16, loss = 0.03300103\n",
      "Iteration 19, loss = 0.03358019\n",
      "Iteration 38, loss = 0.05207585\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 16, loss = 0.03386152\n",
      "Iteration 7, loss = 0.10926654\n",
      "Iteration 16, loss = 0.05534648\n",
      "Iteration 14, loss = 0.04234565\n",
      "Iteration 17, loss = 0.05210163\n",
      "Iteration 19, loss = 0.03317554\n",
      "Iteration 20, loss = 0.03256506\n",
      "Iteration 10, loss = 0.04661532\n",
      "Iteration 18, loss = 0.03473755\n",
      "Iteration 15, loss = 0.03443697\n",
      "Iteration 15, loss = 0.04029240\n",
      "Iteration 8, loss = 0.09002220\n",
      "Iteration 18, loss = 0.04931637\n",
      "Iteration 4, loss = 0.17756464\n",
      "Iteration 16, loss = 0.04365065\n",
      "Iteration 17, loss = 0.03180632\n",
      "Iteration 19, loss = 0.04700932\n",
      "Iteration 2, loss = 0.37137722\n",
      "Iteration 9, loss = 0.07691762\n",
      "Iteration 16, loss = 0.03861151\n",
      "Iteration 16, loss = 0.03301857\n",
      "Iteration 17, loss = 0.04164303\n",
      "Iteration 5, loss = 0.12314570\n",
      "Iteration 11, loss = 0.04357618\n",
      "Iteration 20, loss = 0.04494078\n",
      "Iteration 18, loss = 0.03974761\n",
      "Iteration 17, loss = 0.03265255\n",
      "Iteration 21, loss = 0.04311195\n",
      "Iteration 10, loss = 0.06771566\n",
      "Iteration 19, loss = 0.03359040\n",
      "Iteration 6, loss = 0.09323546\n",
      "Iteration 19, loss = 0.03814227\n",
      "Iteration 18, loss = 0.03070904\n",
      "Iteration 22, loss = 0.04156531\n",
      "Iteration 11, loss = 0.06096108\n",
      "Iteration 7, loss = 0.07596049\n",
      "Iteration 12, loss = 0.04101876\n",
      "Iteration 20, loss = 0.03676236\n",
      "Iteration 3, loss = 0.21061833\n",
      "Iteration 20, loss = 0.03212630\n",
      "Iteration 8, loss = 0.06518334\n",
      "Iteration 21, loss = 0.03548819\n",
      "Iteration 20, loss = 0.03257819\n",
      "Iteration 19, loss = 0.02979885\n",
      "Iteration 21, loss = 0.03164820\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.04011698\n",
      "Iteration 21, loss = 0.03118462\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 0.03183878\n",
      "Iteration 17, loss = 0.03708267\n",
      "Iteration 9, loss = 0.05784643\n",
      "Iteration 20, loss = 0.02890390\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.03894657\n",
      "Iteration 18, loss = 0.03575901\n",
      "Iteration 10, loss = 0.05259447\n",
      "Iteration 24, loss = 0.03885801\n",
      "Iteration 25, loss = 0.03768013\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 18, loss = 0.03074411\n",
      "Iteration 14, loss = 0.03712157\n",
      "Iteration 12, loss = 0.05575935\n",
      "Iteration 22, loss = 0.03434286\n",
      "Iteration 11, loss = 0.04861426\n",
      "Iteration 23, loss = 0.03326599\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 21, loss = 0.03164037\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 19, loss = 0.02978335\n",
      "Iteration 12, loss = 0.04544451\n",
      "Iteration 19, loss = 0.03452364\n",
      "Iteration 15, loss = 0.03554202\n",
      "Iteration 18, loss = 0.03155626\n",
      "Iteration 13, loss = 0.04287653\n",
      "Iteration 20, loss = 0.02883322\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 13, loss = 0.05163218\n",
      "Iteration 4, loss = 0.13010389\n",
      "Iteration 16, loss = 0.03422440\n",
      "Iteration 19, loss = 0.03050317\n",
      "Iteration 14, loss = 0.04069243\n",
      "Iteration 14, loss = 0.04830162\n",
      "Iteration 20, loss = 0.03343243\n",
      "Iteration 17, loss = 0.03295262\n",
      "Iteration 15, loss = 0.04553099\n",
      "Iteration 15, loss = 0.03886901\n",
      "Iteration 20, loss = 0.02963656\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 5, loss = 0.09243851\n",
      "Iteration 16, loss = 0.04314712\n",
      "Iteration 16, loss = 0.03724832\n",
      "Iteration 18, loss = 0.03183999\n",
      "Iteration 21, loss = 0.03247512\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 17, loss = 0.04120787\n",
      "Iteration 6, loss = 0.07325402\n",
      "Iteration 17, loss = 0.03582846\n",
      "Iteration 18, loss = 0.03948678\n",
      "Iteration 19, loss = 0.03089081\n",
      "Iteration 18, loss = 0.03455794\n",
      "Iteration 19, loss = 0.03785981\n",
      "Iteration 7, loss = 0.06188796\n",
      "Iteration 20, loss = 0.02995583\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 20, loss = 0.03649540\n",
      "Iteration 19, loss = 0.03346701\n",
      "Iteration 8, loss = 0.05458316\n",
      "Iteration 21, loss = 0.03525281\n",
      "Iteration 20, loss = 0.03243338\n",
      "Iteration 22, loss = 0.03411863\n",
      "Iteration 9, loss = 0.04940258\n",
      "Iteration 21, loss = 0.03149487\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 23, loss = 0.03319915\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n",
      "Iteration 10, loss = 0.04548632\n",
      "Iteration 11, loss = 0.04247633\n",
      "Iteration 12, loss = 0.03992667\n",
      "Iteration 13, loss = 0.03786759\n",
      "Iteration 14, loss = 0.03607197\n",
      "Iteration 15, loss = 0.03453710\n",
      "Iteration 16, loss = 0.03319272\n",
      "Iteration 17, loss = 0.03196645\n",
      "Iteration 18, loss = 0.03087349\n",
      "Iteration 19, loss = 0.02991876\n",
      "Iteration 20, loss = 0.02902821\n",
      "Training loss did not improve more than tol=0.005000 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sizes, train_scores, val_scores = learning_curve(best_clf, X_train_tfidf, y_train, cv=5, scoring='neg_log_loss', n_jobs=-1)\n",
    "\n",
    "# Calcolare la media e la deviazione standard delle perdite\n",
    "train_mean = -np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "val_mean = -np.mean(val_scores, axis=1)\n",
    "val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Tracciare le learning curves\n",
    "plt.figure()\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='r', label='Training Loss')\n",
    "plt.plot(train_sizes, val_mean, 'o-', color='g', label='Validation Loss')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='g')\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28171b1c",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>MLPClassifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.923671</td>\n",
       "      <td>0.992271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Precision</td>\n",
       "      <td>0.995370</td>\n",
       "      <td>0.986348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recall</td>\n",
       "      <td>0.733788</td>\n",
       "      <td>0.986348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F1-Score</td>\n",
       "      <td>0.844794</td>\n",
       "      <td>0.986348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Metric  Naive Bayes  MLPClassifier\n",
       "0   Accuracy     0.923671       0.992271\n",
       "1  Precision     0.995370       0.986348\n",
       "2     Recall     0.733788       0.986348\n",
       "3   F1-Score     0.844794       0.986348"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"],\n",
    "    \"Naive Bayes\": [nb_accuracy, nb_precision, nb_recall, nb_f1],\n",
    "    \"MLPClassifier\": [mlp_accuracy, mlp_precision, mlp_recall, mlp_f1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9b834fc",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pycaret.classification import *\n",
    "\n",
    "X_train_tfidf_df = pd.DataFrame(X_train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "X_test_tfidf_df = pd.DataFrame(X_test_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "X_train_tfidf_df['label'] = y_train.values\n",
    "X_test_tfidf_df['label'] = y_test.values\n",
    "data = pd.concat([X_train_tfidf_df, X_test_tfidf_df])\n",
    "train_indices = list(range(len(X_train)))\n",
    "\n",
    "\n",
    "# Setup l'esperimento con i dati concatenati\n",
    "s = setup(data=X_train_tfidf, target=y_train, session_id=42, train_size=len(X_train)/len(data))\n",
    "\n",
    "# Confronta i modelli\n",
    "best = compare_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
